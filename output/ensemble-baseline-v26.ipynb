{"cells":[{"metadata":{"papermill":{"duration":0.053794,"end_time":"2020-10-22T04:56:35.702249","exception":false,"start_time":"2020-10-22T04:56:35.648455","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Strategy\n\n- Preprocessing\n    - RankGauss\n    - PCA + Existing Features\n    - Variance Encoding\n- Model\n    - Normal Neural Network\n    - Split Neural Network\n    - ~~NODE (Neural Oblivious Decision Ensembles)~~\n    - TabNet\n    - Multi input ResNet\n    - ~~Kernel Ridge Regression - Platt Scaling ~~\n- Learning\n    - Pre-train with non-scored label\n    - Optimizer: AdamW with weight_decay\n    - Label smoothing\n- Prediction\n    - Ensemble above with weight optimization\n    - With clipping"},{"metadata":{"papermill":{"duration":0.051376,"end_time":"2020-10-22T04:56:35.804344","exception":false,"start_time":"2020-10-22T04:56:35.752968","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Library"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:35.909983Z","iopub.status.busy":"2020-10-22T04:56:35.909052Z","iopub.status.idle":"2020-10-22T04:56:35.912184Z","shell.execute_reply":"2020-10-22T04:56:35.911681Z"},"papermill":{"duration":0.05804,"end_time":"2020-10-22T04:56:35.912281","exception":false,"start_time":"2020-10-22T04:56:35.854241","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:36.017123Z","iopub.status.busy":"2020-10-22T04:56:36.016307Z","iopub.status.idle":"2020-10-22T04:56:37.028707Z","shell.execute_reply":"2020-10-22T04:56:37.027906Z"},"papermill":{"duration":1.06696,"end_time":"2020-10-22T04:56:37.028846","exception":false,"start_time":"2020-10-22T04:56:35.961886","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nsys.path.append(\"../input/autograd\")\nimport autograd.numpy as np\nfrom autograd import grad","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:37.145112Z","iopub.status.busy":"2020-10-22T04:56:37.144277Z","iopub.status.idle":"2020-10-22T04:56:43.613589Z","shell.execute_reply":"2020-10-22T04:56:43.612416Z"},"papermill":{"duration":6.531872,"end_time":"2020-10-22T04:56:43.61372","exception":false,"start_time":"2020-10-22T04:56:37.081848","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import datetime\nimport gc\nimport os\nimport random\nfrom collections import defaultdict\nfrom time import time\nfrom typing import Optional\n\n# import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow_addons as tfa\nimport tensorflow_probability as tfp\n\n# import optuna\nfrom scipy.optimize import fsolve, minimize\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:43.728516Z","iopub.status.busy":"2020-10-22T04:56:43.726987Z","iopub.status.idle":"2020-10-22T04:56:43.733156Z","shell.execute_reply":"2020-10-22T04:56:43.73234Z"},"papermill":{"duration":0.068084,"end_time":"2020-10-22T04:56:43.733291","exception":false,"start_time":"2020-10-22T04:56:43.665207","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"MIXED_PRECISION = False\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n\n    if tpu:\n        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_bfloat16\")\n    else:\n        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n    mixed_precision.set_policy(policy)\n    print(\"Mixed precision enabled\")\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print(\"Accelerated Linear Algebra enabled\")","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.051567,"end_time":"2020-10-22T04:56:43.839105","exception":false,"start_time":"2020-10-22T04:56:43.787538","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Functions"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:43.948466Z","iopub.status.busy":"2020-10-22T04:56:43.947264Z","iopub.status.idle":"2020-10-22T04:56:43.950289Z","shell.execute_reply":"2020-10-22T04:56:43.949823Z"},"papermill":{"duration":0.060237,"end_time":"2020-10-22T04:56:43.950401","exception":false,"start_time":"2020-10-22T04:56:43.890164","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def fix_seed(seed=2020):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n\nrandom_seed = 22\nfix_seed(random_seed)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:44.059Z","iopub.status.busy":"2020-10-22T04:56:44.058165Z","iopub.status.idle":"2020-10-22T04:56:44.062162Z","shell.execute_reply":"2020-10-22T04:56:44.061615Z"},"papermill":{"duration":0.060946,"end_time":"2020-10-22T04:56:44.062268","exception":false,"start_time":"2020-10-22T04:56:44.001322","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# https://www.kaggle.com/c/lish-moa/discussion/189857#1043953\n\n# Prediction Clipping Thresholds\np_min = 0.001\np_max = 0.999\n\n# Evaluation Metric with clipping and no label smoothing\ndef logloss(y_true, y_pred):\n    # y_pred = tf.clip_by_value(y_pred, p_min, p_max)\n    return -K.mean(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:44.201171Z","iopub.status.busy":"2020-10-22T04:56:44.199511Z","iopub.status.idle":"2020-10-22T04:56:44.201837Z","shell.execute_reply":"2020-10-22T04:56:44.202308Z"},"papermill":{"duration":0.072428,"end_time":"2020-10-22T04:56:44.202441","exception":false,"start_time":"2020-10-22T04:56:44.130013","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# [Fast Numpy Log Loss] https://www.kaggle.com/gogo827jz/optimise-blending-weights-4-5x-faster-log-loss\ndef metric(y_true, y_pred):\n    loss = 0\n    y_pred_clip = np.clip(y_pred, 1e-7, 1 - 1e-7)\n    for i in range(y_pred.shape[1]):\n        loss += -np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n    return loss / y_pred.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:44.316106Z","iopub.status.busy":"2020-10-22T04:56:44.314085Z","iopub.status.idle":"2020-10-22T04:56:44.316832Z","shell.execute_reply":"2020-10-22T04:56:44.317337Z"},"papermill":{"duration":0.061461,"end_time":"2020-10-22T04:56:44.317473","exception":false,"start_time":"2020-10-22T04:56:44.256012","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def blend(size, weights, oof):\n    blend_ = np.zeros(size)\n    for i, key in enumerate(oof.keys()):\n        blend_ += weights[i] * oof[key].values\n    return blend_","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.050529,"end_time":"2020-10-22T04:56:44.419543","exception":false,"start_time":"2020-10-22T04:56:44.369014","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:44.52746Z","iopub.status.busy":"2020-10-22T04:56:44.526523Z","iopub.status.idle":"2020-10-22T04:56:50.71115Z","shell.execute_reply":"2020-10-22T04:56:50.710025Z"},"papermill":{"duration":6.241481,"end_time":"2020-10-22T04:56:50.711269","exception":false,"start_time":"2020-10-22T04:56:44.469788","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\ntest_df = pd.read_csv(\"../input/lish-moa/test_features.csv\")\ntarget_df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\nnon_target_df = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\nsubmit_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:50.826114Z","iopub.status.busy":"2020-10-22T04:56:50.824723Z","iopub.status.idle":"2020-10-22T04:56:50.901923Z","shell.execute_reply":"2020-10-22T04:56:50.901306Z"},"papermill":{"duration":0.138598,"end_time":"2020-10-22T04:56:50.902033","exception":false,"start_time":"2020-10-22T04:56:50.763435","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"train = train_df.copy()\ntest = test_df.copy()\nss = submit_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.055335,"end_time":"2020-10-22T04:56:51.011371","exception":false,"start_time":"2020-10-22T04:56:50.956036","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:51.138588Z","iopub.status.busy":"2020-10-22T04:56:51.137679Z","iopub.status.idle":"2020-10-22T04:56:51.151347Z","shell.execute_reply":"2020-10-22T04:56:51.150796Z"},"papermill":{"duration":0.084081,"end_time":"2020-10-22T04:56:51.151459","exception":false,"start_time":"2020-10-22T04:56:51.067378","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"train.loc[:, \"cp_dose\"] = train.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})\ntest.loc[:, \"cp_dose\"] = test.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})\n\ntrain.loc[:, \"cp_time\"] = train.loc[:, \"cp_time\"].map({24: 0, 48: 1, 72: 2})\ntest.loc[:, \"cp_time\"] = test.loc[:, \"cp_time\"].map({24: 0, 48: 1, 72: 2})","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.055989,"end_time":"2020-10-22T04:56:51.260829","exception":false,"start_time":"2020-10-22T04:56:51.20484","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## cp_type が ctrl_vehicle なものは MoA を持たない\n\nので、学習から除外する"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:51.378498Z","iopub.status.busy":"2020-10-22T04:56:51.377274Z","iopub.status.idle":"2020-10-22T04:56:51.583382Z","shell.execute_reply":"2020-10-22T04:56:51.582753Z"},"papermill":{"duration":0.270416,"end_time":"2020-10-22T04:56:51.58351","exception":false,"start_time":"2020-10-22T04:56:51.313094","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"target_df = target_df.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\nnon_target_df = non_target_df.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\ntrain = train.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:51.744313Z","iopub.status.busy":"2020-10-22T04:56:51.697215Z","iopub.status.idle":"2020-10-22T04:56:51.757123Z","shell.execute_reply":"2020-10-22T04:56:51.75654Z"},"papermill":{"duration":0.120297,"end_time":"2020-10-22T04:56:51.757237","exception":false,"start_time":"2020-10-22T04:56:51.63694","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"train = train.drop(\"cp_type\", axis=1)\ntest = test.drop(\"cp_type\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:51.87809Z","iopub.status.busy":"2020-10-22T04:56:51.877142Z","iopub.status.idle":"2020-10-22T04:56:51.883738Z","shell.execute_reply":"2020-10-22T04:56:51.884277Z"},"papermill":{"duration":0.069608,"end_time":"2020-10-22T04:56:51.884451","exception":false,"start_time":"2020-10-22T04:56:51.814843","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"del train[\"sig_id\"]\ndel target_df[\"sig_id\"]\ndel non_target_df[\"sig_id\"]\ndel test[\"sig_id\"]\ndel ss[\"sig_id\"]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:51.994136Z","iopub.status.busy":"2020-10-22T04:56:51.993287Z","iopub.status.idle":"2020-10-22T04:56:51.995789Z","shell.execute_reply":"2020-10-22T04:56:51.996286Z"},"papermill":{"duration":0.059633,"end_time":"2020-10-22T04:56:51.996468","exception":false,"start_time":"2020-10-22T04:56:51.936835","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"#train","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.05382,"end_time":"2020-10-22T04:56:52.104026","exception":false,"start_time":"2020-10-22T04:56:52.050206","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Rank Gauss\n\nhttps://www.kaggle.com/nayuts/moa-pytorch-nn-pca-rankgauss\n\n連続値を特定の範囲の閉域に押し込めて、分布の偏りを解消する方法です。"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:56:52.222003Z","iopub.status.busy":"2020-10-22T04:56:52.220678Z","iopub.status.idle":"2020-10-22T04:57:01.587879Z","shell.execute_reply":"2020-10-22T04:57:01.586959Z"},"papermill":{"duration":9.431158,"end_time":"2020-10-22T04:57:01.587996","exception":false,"start_time":"2020-10-22T04:56:52.156838","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"g_cols = [col for col in train_df.columns if col.startswith(\"g-\")]\nc_cols = [col for col in train_df.columns if col.startswith(\"c-\")]\n\nfor col in g_cols + c_cols:\n    transformer = QuantileTransformer(n_quantiles=100, random_state=random_seed, output_distribution=\"normal\")\n\n    vec_len = len(train[col].values)\n    vec_len_test = len(test[col].values)\n\n    raw_vec = train[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:01.703761Z","iopub.status.busy":"2020-10-22T04:57:01.70267Z","iopub.status.idle":"2020-10-22T04:57:01.705795Z","shell.execute_reply":"2020-10-22T04:57:01.705116Z"},"papermill":{"duration":0.063421,"end_time":"2020-10-22T04:57:01.70592","exception":false,"start_time":"2020-10-22T04:57:01.642499","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"#train","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.057388,"end_time":"2020-10-22T04:57:01.820092","exception":false,"start_time":"2020-10-22T04:57:01.762704","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## PCA features (+ Existing features)\n\n既存のカラムは残したほうがいいのだろうか？？\n→ このコンペでは残したほうがいい成績が出ている。"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:01.942661Z","iopub.status.busy":"2020-10-22T04:57:01.941568Z","iopub.status.idle":"2020-10-22T04:57:04.485061Z","shell.execute_reply":"2020-10-22T04:57:04.485753Z"},"papermill":{"duration":2.6083,"end_time":"2020-10-22T04:57:04.485951","exception":false,"start_time":"2020-10-22T04:57:01.877651","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# g-\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train[g_cols]), pd.DataFrame(test[g_cols])])\ndata2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[g_cols])\ntrain2 = data2[: train.shape[0]]\ntest2 = data2[-test.shape[0] :]\n\ntrain2 = pd.DataFrame(train2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:04.641421Z","iopub.status.busy":"2020-10-22T04:57:04.640671Z","iopub.status.idle":"2020-10-22T04:57:05.035052Z","shell.execute_reply":"2020-10-22T04:57:05.033905Z"},"papermill":{"duration":0.476098,"end_time":"2020-10-22T04:57:05.035178","exception":false,"start_time":"2020-10-22T04:57:04.55908","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# c-\nn_comp = 15\n\ndata = pd.concat([pd.DataFrame(train[c_cols]), pd.DataFrame(test[c_cols])])\ndata2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[c_cols])\ntrain2 = data2[: train.shape[0]]\ntest2 = data2[-test.shape[0] :]\n\ntrain2 = pd.DataFrame(train2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:05.151187Z","iopub.status.busy":"2020-10-22T04:57:05.149191Z","iopub.status.idle":"2020-10-22T04:57:05.151945Z","shell.execute_reply":"2020-10-22T04:57:05.152491Z"},"papermill":{"duration":0.062642,"end_time":"2020-10-22T04:57:05.152652","exception":false,"start_time":"2020-10-22T04:57:05.09001","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"#train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pca = train.copy()\ntest_pca = test.copy()\n\ntrain_pca.drop(g_cols, axis=1, inplace=True)\ntest_pca.drop(g_cols, axis=1, inplace=True)\n\ntrain_pca.drop(c_cols, axis=1, inplace=True)\ntest_pca.drop(c_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_pca","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.055042,"end_time":"2020-10-22T04:57:05.261857","exception":false,"start_time":"2020-10-22T04:57:05.206815","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## feature Selection using Variance Encoding\n\n分散がしきい値以下の特徴量を捨てます。"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:05.378542Z","iopub.status.busy":"2020-10-22T04:57:05.377237Z","iopub.status.idle":"2020-10-22T04:57:05.981724Z","shell.execute_reply":"2020-10-22T04:57:05.982443Z"},"papermill":{"duration":0.668127,"end_time":"2020-10-22T04:57:05.982596","exception":false,"start_time":"2020-10-22T04:57:05.314469","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"var_thresh = VarianceThreshold(threshold=0.5)\n\ndata = train.append(test)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 2:])\n\ntrain_transformed = data_transformed[: train.shape[0]]\ntest_transformed = data_transformed[-test.shape[0] :]\n\n\ntrain = pd.DataFrame(train[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\ntrain = pd.concat([train, pd.DataFrame(train_transformed)], axis=1, ignore_index=True)\n\n\ntest = pd.DataFrame(test[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\ntest = pd.concat([test, pd.DataFrame(test_transformed)], axis=1, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:06.09261Z","iopub.status.busy":"2020-10-22T04:57:06.091925Z","iopub.status.idle":"2020-10-22T04:57:06.096375Z","shell.execute_reply":"2020-10-22T04:57:06.095841Z"},"papermill":{"duration":0.060516,"end_time":"2020-10-22T04:57:06.096475","exception":false,"start_time":"2020-10-22T04:57:06.035959","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"#train","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.054725,"end_time":"2020-10-22T04:57:06.206195","exception":false,"start_time":"2020-10-22T04:57:06.15147","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Create Model"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:06.329398Z","iopub.status.busy":"2020-10-22T04:57:06.327483Z","iopub.status.idle":"2020-10-22T04:57:06.330118Z","shell.execute_reply":"2020-10-22T04:57:06.330765Z"},"papermill":{"duration":0.067344,"end_time":"2020-10-22T04:57:06.330904","exception":false,"start_time":"2020-10-22T04:57:06.26356","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def create_model_simple_nn(num_col, output_dim):\n    model = tf.keras.Sequential(\n        [\n            L.Input(num_col),\n            L.BatchNormalization(),\n            L.Dropout(0.2),\n            tfa.layers.WeightNormalization(L.Dense(2048, activation=\"relu\")),\n            L.BatchNormalization(),\n            L.Dropout(0.5),\n            tfa.layers.WeightNormalization(L.Dense(1024, activation=\"relu\")),\n            L.BatchNormalization(),\n            L.Dropout(0.5),\n            tfa.layers.WeightNormalization(L.Dense(output_dim, activation=\"sigmoid\")),\n        ]\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.054651,"end_time":"2020-10-22T04:57:06.440321","exception":false,"start_time":"2020-10-22T04:57:06.38567","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Create Model - Split Neural Network\n\nhttps://www.kaggle.com/gogo827jz/split-neural-network-approach-tf-keras"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:06.569125Z","iopub.status.busy":"2020-10-22T04:57:06.568213Z","iopub.status.idle":"2020-10-22T04:57:06.571724Z","shell.execute_reply":"2020-10-22T04:57:06.571201Z"},"papermill":{"duration":0.077635,"end_time":"2020-10-22T04:57:06.571831","exception":false,"start_time":"2020-10-22T04:57:06.494196","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def create_split_nn(num_columns, hidden_units, dropout_rate, output_dim):\n\n    inp1 = L.Input(shape=(num_columns,))\n    x1 = L.BatchNormalization()(inp1)\n\n    for i, units in enumerate(hidden_units[0]):\n        x1 = tfa.layers.WeightNormalization(L.Dense(units, activation=\"elu\"))(x1)\n        x1 = L.Dropout(dropout_rate[0])(x1)\n        x1 = L.BatchNormalization()(x1)\n\n    inp2 = L.Input(shape=(num_columns,))\n    x2 = L.BatchNormalization()(inp2)\n\n    for i, units in enumerate(hidden_units[1]):\n        x2 = tfa.layers.WeightNormalization(L.Dense(units, activation=\"elu\"))(x2)\n        x2 = L.Dropout(dropout_rate[1])(x2)\n        x2 = L.BatchNormalization()(x2)\n\n    inp3 = L.Input(shape=(num_columns,))\n    x3 = L.BatchNormalization()(inp3)\n\n    for i, units in enumerate(hidden_units[2]):\n        x3 = tfa.layers.WeightNormalization(L.Dense(units, activation=\"elu\"))(x3)\n        x3 = L.Dropout(dropout_rate[2])(x3)\n        x3 = L.BatchNormalization()(x3)\n\n    x = L.Concatenate()([x1, x2, x3])\n    x = L.Dropout(dropout_rate[3])(x)\n    x = L.BatchNormalization()(x)\n\n    for units in hidden_units[3]:\n\n        x = tfa.layers.WeightNormalization(L.Dense(units, activation=\"elu\"))(x)\n        x = L.Dropout(dropout_rate[4])(x)\n        x = L.BatchNormalization()(x)\n\n    out = tfa.layers.WeightNormalization(L.Dense(output_dim, activation=\"sigmoid\"))(x)\n\n    model = tf.keras.models.Model(inputs=[inp1, inp2, inp3], outputs=out)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:06.689441Z","iopub.status.busy":"2020-10-22T04:57:06.688607Z","iopub.status.idle":"2020-10-22T04:57:06.692811Z","shell.execute_reply":"2020-10-22T04:57:06.692256Z"},"papermill":{"duration":0.065612,"end_time":"2020-10-22T04:57:06.692921","exception":false,"start_time":"2020-10-22T04:57:06.627309","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def create_model_split_nn(num_col, output_dim):\n    hidden_units = [[2048, 512], [1024, 1024], [1024, 512], [512, 512]]\n    dropout_rate = [0.4, 0.35, 0.3, 0.3, 0.2]\n    size = int(np.ceil(0.8 * num_col))\n\n    model = create_split_nn(size, hidden_units, dropout_rate, output_dim)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.094041,"end_time":"2020-10-22T04:57:06.842542","exception":false,"start_time":"2020-10-22T04:57:06.748501","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Create Model - Multi input ResNet\n\nhttps://www.kaggle.com/rahulsd91/moa-multi-input-resnet-model"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:06.967021Z","iopub.status.busy":"2020-10-22T04:57:06.966309Z","iopub.status.idle":"2020-10-22T04:57:06.970302Z","shell.execute_reply":"2020-10-22T04:57:06.969625Z"},"papermill":{"duration":0.073468,"end_time":"2020-10-22T04:57:06.970445","exception":false,"start_time":"2020-10-22T04:57:06.896977","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def create_model_resnet(n_features, n_features_2, n_labels):\n    input_1 = L.Input(shape=(n_features,), name=\"Input1\")\n    input_2 = L.Input(shape=(n_features_2,), name=\"Input2\")\n\n    head_1 = tf.keras.Sequential(\n        [\n            L.BatchNormalization(),\n            L.Dropout(0.2),\n            tfa.layers.WeightNormalization(L.Dense(512, activation=\"elu\")),\n            L.BatchNormalization(),\n            L.Dropout(0.2),\n            tfa.layers.WeightNormalization(L.Dense(256, activation=\"elu\")),\n        ],\n        name=\"Head1\",\n    )\n\n    input_3 = head_1(input_1)\n    input_3_concat = L.Concatenate()([input_2, input_3])\n\n    head_2 = tf.keras.Sequential(\n        [\n            L.BatchNormalization(),\n            L.Dropout(0.3),\n            tfa.layers.WeightNormalization(L.Dense(512, activation=\"relu\")),\n            L.BatchNormalization(),\n            L.Dropout(0.2),\n            tfa.layers.WeightNormalization(L.Dense(512, activation=\"elu\")),\n            L.BatchNormalization(),\n            L.Dropout(0.2),\n            tfa.layers.WeightNormalization(L.Dense(256, activation=\"relu\")),\n            L.BatchNormalization(),\n            L.Dropout(0.2),\n            tfa.layers.WeightNormalization(L.Dense(256, activation=\"elu\")),\n        ],\n        name=\"Head2\",\n    )\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = L.Average()([input_3, input_4])\n\n    head_3 = tf.keras.Sequential(\n        [\n            L.BatchNormalization(),\n            tfa.layers.WeightNormalization(L.Dense(256, activation=\"selu\")),\n            L.BatchNormalization(),\n            L.Dropout(0.2),\n            tfa.layers.WeightNormalization(L.Dense(256, activation=\"selu\")),\n            L.BatchNormalization(),\n            L.Dense(n_labels, activation=\"sigmoid\"),\n        ],\n        name=\"Head3\",\n    )\n\n    output = head_3(input_4_avg)\n\n    model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=output)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.053658,"end_time":"2020-10-22T04:57:07.077477","exception":false,"start_time":"2020-10-22T04:57:07.023819","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Create Model - NODE\n\nNeural Oblivious Decision Ensembles\n\nhttps://www.kaggle.com/gogo827jz/moa-neural-oblivious-decision-ensembles-tf-keras"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:07.191899Z","iopub.status.busy":"2020-10-22T04:57:07.190201Z","iopub.status.idle":"2020-10-22T04:57:07.192876Z","shell.execute_reply":"2020-10-22T04:57:07.193433Z"},"papermill":{"duration":0.062192,"end_time":"2020-10-22T04:57:07.193583","exception":false,"start_time":"2020-10-22T04:57:07.131391","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"@tf.function\ndef sparsemoid(inputs: tf.Tensor):\n    return tf.clip_by_value(0.5 * inputs + 0.5, 0.0, 1.0)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:07.308314Z","iopub.status.busy":"2020-10-22T04:57:07.307707Z","iopub.status.idle":"2020-10-22T04:57:07.312109Z","shell.execute_reply":"2020-10-22T04:57:07.311583Z"},"papermill":{"duration":0.063806,"end_time":"2020-10-22T04:57:07.312204","exception":false,"start_time":"2020-10-22T04:57:07.248398","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"@tf.function\ndef identity(x: tf.Tensor):\n    return x","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:07.451987Z","iopub.status.busy":"2020-10-22T04:57:07.439104Z","iopub.status.idle":"2020-10-22T04:57:07.454609Z","shell.execute_reply":"2020-10-22T04:57:07.454125Z"},"papermill":{"duration":0.087071,"end_time":"2020-10-22T04:57:07.454704","exception":false,"start_time":"2020-10-22T04:57:07.367633","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class ODST(L.Layer):\n    def __init__(self, n_trees: int = 3, depth: int = 4, units: int = 1, threshold_init_beta: float = 1.0):\n        super(ODST, self).__init__()\n        self.initialized = False\n        self.n_trees = n_trees\n        self.depth = depth\n        self.units = units\n        self.threshold_init_beta = threshold_init_beta\n\n    def build(self, input_shape: tf.TensorShape):\n        feature_selection_logits_init = tf.zeros_initializer()\n        self.feature_selection_logits = tf.Variable(\n            initial_value=feature_selection_logits_init(\n                shape=(input_shape[-1], self.n_trees, self.depth), dtype=\"float32\"\n            ),\n            trainable=True,\n            name=\"feature_selection_logits\",\n        )\n\n        feature_thresholds_init = tf.zeros_initializer()\n        self.feature_thresholds = tf.Variable(\n            initial_value=feature_thresholds_init(shape=(self.n_trees, self.depth), dtype=\"float32\"),\n            trainable=True,\n            name=\"feature_thresholds\",\n        )\n\n        log_temperatures_init = tf.ones_initializer()\n        self.log_temperatures = tf.Variable(\n            initial_value=log_temperatures_init(shape=(self.n_trees, self.depth), dtype=\"float32\"),\n            trainable=True,\n            name=\"log_temperatures\",\n        )\n\n        indices = K.arange(0, 2 ** self.depth, 1)\n        offsets = 2 ** K.arange(0, self.depth, 1)\n        bin_codes = tf.reshape(indices, (1, -1)) // tf.reshape(offsets, (-1, 1)) % 2\n        bin_codes_1hot = tf.stack([bin_codes, 1 - bin_codes], axis=-1)\n        self.bin_codes_1hot = tf.Variable(\n            initial_value=tf.cast(bin_codes_1hot, \"float32\"), trainable=False, name=\"bin_codes_1hot\"\n        )\n\n        response_init = tf.ones_initializer()\n        self.response = tf.Variable(\n            initial_value=response_init(shape=(self.n_trees, self.units, 2 ** self.depth), dtype=\"float32\"),\n            trainable=True,\n            name=\"response\",\n        )\n\n    def initialize(self, inputs):\n        feature_values = self.feature_values(inputs)\n\n        # intialize feature_thresholds\n        percentiles_q = 100 * tfp.distributions.Beta(self.threshold_init_beta, self.threshold_init_beta).sample(\n            [self.n_trees * self.depth]\n        )\n        flattened_feature_values = tf.map_fn(K.flatten, feature_values)\n        init_feature_thresholds = tf.linalg.diag_part(\n            tfp.stats.percentile(flattened_feature_values, percentiles_q, axis=0)\n        )\n\n        self.feature_thresholds.assign(tf.reshape(init_feature_thresholds, self.feature_thresholds.shape))\n\n        # intialize log_temperatures\n        self.log_temperatures.assign(\n            tfp.stats.percentile(tf.math.abs(feature_values - self.feature_thresholds), 50, axis=0)\n        )\n\n    def feature_values(self, inputs: tf.Tensor, training: bool = None):\n        feature_selectors = tfa.activations.sparsemax(self.feature_selection_logits)\n        # ^--[in_features, n_trees, depth]\n\n        feature_values = tf.einsum(\"bi,ind->bnd\", inputs, feature_selectors)\n        # ^--[batch_size, n_trees, depth]\n\n        return feature_values\n\n    def call(self, inputs: tf.Tensor, training: bool = None):\n        if not self.initialized:\n            self.initialize(inputs)\n            self.initialized = True\n\n        feature_values = self.feature_values(inputs)\n\n        threshold_logits_a = (feature_values - self.feature_thresholds) * tf.math.exp(-self.log_temperatures)\n\n        threshold_logits_b = tf.stack([-threshold_logits_a, threshold_logits_a], axis=-1)\n        # ^--[batch_size, n_trees, depth, 2]\n\n        bins = sparsemoid(threshold_logits_b)\n        # ^--[batch_size, n_trees, depth, 2], approximately binary\n\n        bin_matches = tf.einsum(\"btds,dcs->btdc\", bins, self.bin_codes_1hot)\n        # ^--[batch_size, n_trees, depth, 2 ** depth]\n\n        response_weights = tf.math.reduce_prod(bin_matches, axis=-2)\n        # ^-- [batch_size, n_trees, 2 ** depth]\n\n        response = tf.einsum(\"bnd,ncd->bnc\", response_weights, self.response)\n        # ^-- [batch_size, n_trees, units]\n\n        return tf.reduce_sum(response, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:07.583434Z","iopub.status.busy":"2020-10-22T04:57:07.581747Z","iopub.status.idle":"2020-10-22T04:57:07.584403Z","shell.execute_reply":"2020-10-22T04:57:07.584907Z"},"papermill":{"duration":0.077203,"end_time":"2020-10-22T04:57:07.585014","exception":false,"start_time":"2020-10-22T04:57:07.507811","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class NODE(tf.keras.Model):\n    def __init__(\n        self,\n        units: int = 1,\n        n_layers: int = 1,\n        output_dim=1,\n        dropout_rate=0.1,\n        link: tf.function = tf.identity,\n        n_trees: int = 3,\n        depth: int = 4,\n        threshold_init_beta: float = 1.0,\n        feature_column: Optional[L.DenseFeatures] = None,\n    ):\n        super(NODE, self).__init__()\n        self.units = units\n        self.n_layers = n_layers\n        self.n_trees = n_trees\n        self.depth = depth\n        self.units = units\n        self.threshold_init_beta = threshold_init_beta\n        self.feature_column = feature_column\n        self.dropout_rate = dropout_rate\n        self.output_dim = output_dim\n\n        if feature_column is None:\n            self.feature = L.Lambda(identity)\n        else:\n            self.feature = feature_column\n\n        self.bn = [L.BatchNormalization() for _ in range(n_layers + 1)]\n        self.dropout = [L.Dropout(self.dropout_rate) for _ in range(n_layers + 1)]\n        self.ensemble = [\n            ODST(n_trees=n_trees, depth=depth, units=units, threshold_init_beta=threshold_init_beta)\n            for _ in range(n_layers)\n        ]\n\n        self.last_layer = L.Dense(self.output_dim)\n\n        self.link = link\n\n    def call(self, inputs, training=None):\n        X_a = self.feature(inputs)\n        X_b = self.bn[0](X_a, training=training)\n        X_c = self.dropout[0](X_b, training=training)\n\n        X = defaultdict(dict)\n        X[0][0] = X_c\n        for i, tree in enumerate(self.ensemble):\n            X[i][1] = tf.concat([X[i][0], tree(X[i][0])], axis=1)\n            X[i][2] = self.bn[i + 1](X[i][1], training=training)\n            X[i + 1][0] = self.dropout[i + 1](X[i][2], training=training)\n\n        return self.link(self.last_layer(X[i + 1][0]))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:07.701616Z","iopub.status.busy":"2020-10-22T04:57:07.70094Z","iopub.status.idle":"2020-10-22T04:57:07.704983Z","shell.execute_reply":"2020-10-22T04:57:07.704493Z"},"papermill":{"duration":0.065594,"end_time":"2020-10-22T04:57:07.705072","exception":false,"start_time":"2020-10-22T04:57:07.639478","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def create_model_node(output_dim):\n    model = NODE(\n        n_layers=3,\n        units=128,\n        output_dim=output_dim,\n        dropout_rate=0.1,\n        depth=6,\n        n_trees=3,\n        link=tf.keras.activations.sigmoid,\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.058167,"end_time":"2020-10-22T04:57:07.817099","exception":false,"start_time":"2020-10-22T04:57:07.758932","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Create Model - TabNet for MultiLabel\n\nhttps://www.kaggle.com/gogo827jz/moa-stacked-tabnet-baseline-tensorflow-2-0#Model-Functions\n\nTabNetとは。\nhttps://cloud.google.com/blog/ja/products/ai-machine-learning/ml-model-tabnet-is-easy-to-use-on-cloud-ai-platform"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:07.93269Z","iopub.status.busy":"2020-10-22T04:57:07.931917Z","iopub.status.idle":"2020-10-22T04:57:07.936059Z","shell.execute_reply":"2020-10-22T04:57:07.935567Z"},"papermill":{"duration":0.063364,"end_time":"2020-10-22T04:57:07.936146","exception":false,"start_time":"2020-10-22T04:57:07.872782","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def register_keras_custom_object(cls):\n    tf.keras.utils.get_custom_objects()[cls.__name__] = cls\n    return cls","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:08.055487Z","iopub.status.busy":"2020-10-22T04:57:08.054643Z","iopub.status.idle":"2020-10-22T04:57:08.057723Z","shell.execute_reply":"2020-10-22T04:57:08.05716Z"},"papermill":{"duration":0.065059,"end_time":"2020-10-22T04:57:08.057825","exception":false,"start_time":"2020-10-22T04:57:07.992766","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def glu(x, n_units=None):\n    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n    if n_units is None:\n        n_units = tf.shape(x)[-1] // 2\n\n    return x[..., :n_units] * tf.nn.sigmoid(x[..., n_units:])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:08.179008Z","iopub.status.busy":"2020-10-22T04:57:08.17813Z","iopub.status.idle":"2020-10-22T04:57:08.181183Z","shell.execute_reply":"2020-10-22T04:57:08.180714Z"},"papermill":{"duration":0.068366,"end_time":"2020-10-22T04:57:08.181276","exception":false,"start_time":"2020-10-22T04:57:08.11291","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"\"\"\"\nCode replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/activations/sparsemax.py\n\"\"\"\n\n\n@register_keras_custom_object\n@tf.function\ndef sparsemax(logits, axis):\n    \"\"\"Sparsemax activation function [1].\n    For each batch `i` and class `j` we have\n      $$sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0)$$\n    [1]: https://arxiv.org/abs/1602.02068\n    Args:\n        logits: Input tensor.\n        axis: Integer, axis along which the sparsemax operation is applied.\n    Returns:\n        Tensor, output of sparsemax transformation. Has the same type and\n        shape as `logits`.\n    Raises:\n        ValueError: In case `dim(logits) == 1`.\n    \"\"\"\n    logits = tf.convert_to_tensor(logits, name=\"logits\")\n\n    # We need its original shape for shape inference.\n    shape = logits.get_shape()\n    rank = shape.rank\n    is_last_axis = (axis == -1) or (axis == rank - 1)\n\n    if is_last_axis:\n        output = _compute_2d_sparsemax(logits)\n        output.set_shape(shape)\n        return output\n\n    # If dim is not the last dimension, we have to do a transpose so that we can\n    # still perform softmax on its last dimension.\n\n    # Swap logits' dimension of dim and its last dimension.\n    rank_op = tf.rank(logits)\n    axis_norm = axis % rank\n    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n\n    # Do the actual softmax on its last dimension.\n    output = _compute_2d_sparsemax(logits)\n    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n\n    # Make shape inference work since transpose may erase its static shape.\n    output.set_shape(shape)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:08.299153Z","iopub.status.busy":"2020-10-22T04:57:08.298444Z","iopub.status.idle":"2020-10-22T04:57:08.302247Z","shell.execute_reply":"2020-10-22T04:57:08.301643Z"},"papermill":{"duration":0.065336,"end_time":"2020-10-22T04:57:08.302342","exception":false,"start_time":"2020-10-22T04:57:08.237006","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def _swap_axis(logits, dim_index, last_index, **kwargs):\n    return tf.transpose(\n        logits,\n        tf.concat(\n            [\n                tf.range(dim_index),\n                [last_index],\n                tf.range(dim_index + 1, last_index),\n                [dim_index],\n            ],\n            0,\n        ),\n        **kwargs,\n    )","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:08.430906Z","iopub.status.busy":"2020-10-22T04:57:08.430089Z","iopub.status.idle":"2020-10-22T04:57:08.434208Z","shell.execute_reply":"2020-10-22T04:57:08.433693Z"},"papermill":{"duration":0.075729,"end_time":"2020-10-22T04:57:08.434301","exception":false,"start_time":"2020-10-22T04:57:08.358572","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def _compute_2d_sparsemax(logits):\n    \"\"\"Performs the sparsemax operation when axis=-1.\"\"\"\n    shape_op = tf.shape(logits)\n    obs = tf.math.reduce_prod(shape_op[:-1])\n    dims = shape_op[-1]\n\n    # In the paper, they call the logits z.\n    # The mean(logits) can be substracted from logits to make the algorithm\n    # more numerically stable. the instability in this algorithm comes mostly\n    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n    # to zero. However, in practise the numerical instability issues are very\n    # minor and substacting the mean causes extra issues with inf and nan\n    # input.\n    # Reshape to [obs, dims] as it is almost free and means the remanining\n    # code doesn't need to worry about the rank.\n    z = tf.reshape(logits, [obs, dims])\n\n    # sort z\n    z_sorted, _ = tf.nn.top_k(z, k=dims)\n\n    # calculate k(z)\n    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n    z_check = 1 + k * z_sorted > z_cumsum\n    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n    # (index + 1) of the last `1` is the same as just summing the number of 1.\n    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n\n    # calculate tau(z)\n    # If there are inf values or all values are -inf, the k_z will be zero,\n    # this is mathematically invalid and will also cause the gather_nd to fail.\n    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n    # fixed later (see p_safe) by returning p = nan. This results in the same\n    # behavior as softmax.\n    k_z_safe = tf.math.maximum(k_z, 1)\n    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n    tau_sum = tf.gather_nd(z_cumsum, indices)\n    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n\n    # calculate p\n    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n    # If k_z = 0 or if z = nan, then the input is invalid\n    p_safe = tf.where(\n        tf.expand_dims(\n            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n            axis=-1,\n        ),\n        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n        p,\n    )\n\n    # Reshape back to original size\n    p_safe = tf.reshape(p_safe, shape_op)\n    return p_safe","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:08.584032Z","iopub.status.busy":"2020-10-22T04:57:08.547701Z","iopub.status.idle":"2020-10-22T04:57:08.598804Z","shell.execute_reply":"2020-10-22T04:57:08.599265Z"},"papermill":{"duration":0.109298,"end_time":"2020-10-22T04:57:08.599427","exception":false,"start_time":"2020-10-22T04:57:08.490129","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"\"\"\"\nCode replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/normalizations.py\n\"\"\"\n\n\n@register_keras_custom_object\nclass GroupNormalization(L.Layer):\n    \"\"\"Group normalization layer.\n    Group Normalization divides the channels into groups and computes\n    within each group the mean and variance for normalization.\n    Empirically, its accuracy is more stable than batch norm in a wide\n    range of small batch sizes, if learning rate is adjusted linearly\n    with batch sizes.\n    Relation to Layer Normalization:\n    If the number of groups is set to 1, then this operation becomes identical\n    to Layer Normalization.\n    Relation to Instance Normalization:\n    If the number of groups is set to the\n    input dimension (number of groups is equal\n    to number of channels), then this operation becomes\n    identical to Instance Normalization.\n    Arguments\n        groups: Integer, the number of groups for Group Normalization.\n            Can be in the range [1, N] where N is the input dimension.\n            The input dimension must be divisible by the number of groups.\n        axis: Integer, the axis that should be normalized.\n        epsilon: Small float added to variance to avoid dividing by zero.\n        center: If True, add offset of `beta` to normalized tensor.\n            If False, `beta` is ignored.\n        scale: If True, multiply by `gamma`.\n            If False, `gamma` is not used.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n    Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n    Output shape\n        Same shape as input.\n    References\n        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n    \"\"\"\n\n    def __init__(\n        self,\n        groups: int = 2,\n        axis: int = -1,\n        epsilon: float = 1e-3,\n        center: bool = True,\n        scale: bool = True,\n        beta_initializer=\"zeros\",\n        gamma_initializer=\"ones\",\n        beta_regularizer=None,\n        gamma_regularizer=None,\n        beta_constraint=None,\n        gamma_constraint=None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.groups = groups\n        self.axis = axis\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n        self._check_axis()\n\n    def build(self, input_shape):\n\n        self._check_if_input_shape_is_none(input_shape)\n        self._set_number_of_groups_for_instance_norm(input_shape)\n        self._check_size_of_dimensions(input_shape)\n        self._create_input_spec(input_shape)\n\n        self._add_gamma_weight(input_shape)\n        self._add_beta_weight(input_shape)\n        self.built = True\n        super().build(input_shape)\n\n    def call(self, inputs, training=None):\n        # Training=none is just for compat with batchnorm signature call\n        input_shape = K.int_shape(inputs)\n        tensor_input_shape = tf.shape(inputs)\n\n        reshaped_inputs, group_shape = self._reshape_into_groups(inputs, input_shape, tensor_input_shape)\n\n        normalized_inputs = self._apply_normalization(reshaped_inputs, input_shape)\n\n        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n\n        return outputs\n\n    def get_config(self):\n        config = {\n            \"groups\": self.groups,\n            \"axis\": self.axis,\n            \"epsilon\": self.epsilon,\n            \"center\": self.center,\n            \"scale\": self.scale,\n            \"beta_initializer\": tf.keras.initializers.serialize(self.beta_initializer),\n            \"gamma_initializer\": tf.keras.initializers.serialize(self.gamma_initializer),\n            \"beta_regularizer\": tf.keras.regularizers.serialize(self.beta_regularizer),\n            \"gamma_regularizer\": tf.keras.regularizers.serialize(self.gamma_regularizer),\n            \"beta_constraint\": tf.keras.constraints.serialize(self.beta_constraint),\n            \"gamma_constraint\": tf.keras.constraints.serialize(self.gamma_constraint),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n\n        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n        group_shape[self.axis] = input_shape[self.axis] // self.groups\n        group_shape.insert(self.axis, self.groups)\n        group_shape = tf.stack(group_shape)\n        reshaped_inputs = tf.reshape(inputs, group_shape)\n        return reshaped_inputs, group_shape\n\n    def _apply_normalization(self, reshaped_inputs, input_shape):\n\n        group_shape = K.int_shape(reshaped_inputs)\n        group_reduction_axes = list(range(1, len(group_shape)))\n        axis = -2 if self.axis == -1 else self.axis - 1\n        group_reduction_axes.pop(axis)\n\n        mean, variance = tf.nn.moments(reshaped_inputs, group_reduction_axes, keepdims=True)\n\n        gamma, beta = self._get_reshaped_weights(input_shape)\n        normalized_inputs = tf.nn.batch_normalization(\n            reshaped_inputs,\n            mean=mean,\n            variance=variance,\n            scale=gamma,\n            offset=beta,\n            variance_epsilon=self.epsilon,\n        )\n        return normalized_inputs\n\n    def _get_reshaped_weights(self, input_shape):\n        broadcast_shape = self._create_broadcast_shape(input_shape)\n        gamma = None\n        beta = None\n        if self.scale:\n            gamma = tf.reshape(self.gamma, broadcast_shape)\n\n        if self.center:\n            beta = tf.reshape(self.beta, broadcast_shape)\n        return gamma, beta\n\n    def _check_if_input_shape_is_none(self, input_shape):\n        dim = input_shape[self.axis]\n        if dim is None:\n            raise ValueError(\n                \"Axis \" + str(self.axis) + \" of \"\n                \"input tensor should have a defined dimension \"\n                \"but the layer received an input with shape \" + str(input_shape) + \".\"\n            )\n\n    def _set_number_of_groups_for_instance_norm(self, input_shape):\n        dim = input_shape[self.axis]\n\n        if self.groups == -1:\n            self.groups = dim\n\n    def _check_size_of_dimensions(self, input_shape):\n\n        dim = input_shape[self.axis]\n        if dim < self.groups:\n            raise ValueError(\n                \"Number of groups (\" + str(self.groups) + \") cannot be \"\n                \"more than the number of channels (\" + str(dim) + \").\"\n            )\n\n        if dim % self.groups != 0:\n            raise ValueError(\n                \"Number of groups (\" + str(self.groups) + \") must be a \"\n                \"multiple of the number of channels (\" + str(dim) + \").\"\n            )\n\n    def _check_axis(self):\n\n        if self.axis == 0:\n            raise ValueError(\n                \"You are trying to normalize your batch axis. Do you want to \"\n                \"use tf.layer.batch_normalization instead\"\n            )\n\n    def _create_input_spec(self, input_shape):\n\n        dim = input_shape[self.axis]\n        self.input_spec = L.InputSpec(ndim=len(input_shape), axes={self.axis: dim})\n\n    def _add_gamma_weight(self, input_shape):\n\n        dim = input_shape[self.axis]\n        shape = (dim,)\n\n        if self.scale:\n            self.gamma = self.add_weight(\n                shape=shape,\n                name=\"gamma\",\n                initializer=self.gamma_initializer,\n                regularizer=self.gamma_regularizer,\n                constraint=self.gamma_constraint,\n            )\n        else:\n            self.gamma = None\n\n    def _add_beta_weight(self, input_shape):\n\n        dim = input_shape[self.axis]\n        shape = (dim,)\n\n        if self.center:\n            self.beta = self.add_weight(\n                shape=shape,\n                name=\"beta\",\n                initializer=self.beta_initializer,\n                regularizer=self.beta_regularizer,\n                constraint=self.beta_constraint,\n            )\n        else:\n            self.beta = None\n\n    def _create_broadcast_shape(self, input_shape):\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n        broadcast_shape.insert(self.axis, self.groups)\n        return broadcast_shape","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:08.727207Z","iopub.status.busy":"2020-10-22T04:57:08.72647Z","iopub.status.idle":"2020-10-22T04:57:08.728934Z","shell.execute_reply":"2020-10-22T04:57:08.729507Z"},"papermill":{"duration":0.073199,"end_time":"2020-10-22T04:57:08.729626","exception":false,"start_time":"2020-10-22T04:57:08.656427","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class TransformBlock(tf.keras.Model):\n    def __init__(self, features, norm_type, momentum=0.9, virtual_batch_size=None, groups=2, block_name=\"\", **kwargs):\n        super(TransformBlock, self).__init__(**kwargs)\n\n        self.features = features\n        self.norm_type = norm_type\n        self.momentum = momentum\n        self.groups = groups\n        self.virtual_batch_size = virtual_batch_size\n\n        self.transform = L.Dense(self.features, use_bias=False, name=f\"transformblock_dense_{block_name}\")\n\n        if norm_type == \"batch\":\n            self.bn = L.BatchNormalization(\n                axis=-1,\n                momentum=momentum,\n                virtual_batch_size=virtual_batch_size,\n                name=f\"transformblock_bn_{block_name}\",\n            )\n\n        else:\n            self.bn = GroupNormalization(axis=-1, groups=self.groups, name=f\"transformblock_gn_{block_name}\")\n\n    def call(self, inputs, training=None):\n        x = self.transform(inputs)\n        x = self.bn(x, training=training)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:08.882861Z","iopub.status.busy":"2020-10-22T04:57:08.856648Z","iopub.status.idle":"2020-10-22T04:57:08.903277Z","shell.execute_reply":"2020-10-22T04:57:08.903731Z"},"papermill":{"duration":0.117062,"end_time":"2020-10-22T04:57:08.903847","exception":false,"start_time":"2020-10-22T04:57:08.786785","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class TabNet(tf.keras.Model):\n    def __init__(\n        self,\n        feature_columns,\n        feature_dim=64,\n        output_dim=64,\n        num_features=None,\n        num_decision_steps=5,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-5,\n        norm_type=\"group\",\n        batch_momentum=0.98,\n        virtual_batch_size=None,\n        num_groups=2,\n        epsilon=1e-5,\n        **kwargs,\n    ):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(TabNet, self).__init__(**kwargs)\n\n        # Input checks\n        if feature_columns is not None:\n            if type(feature_columns) not in (list, tuple):\n                raise ValueError(\"`feature_columns` must be a list or a tuple.\")\n\n            if len(feature_columns) == 0:\n                raise ValueError(\"`feature_columns` must be contain at least 1 tf.feature_column !\")\n\n            if num_features is None:\n                num_features = len(feature_columns)\n            else:\n                num_features = int(num_features)\n\n        else:\n            if num_features is None:\n                raise ValueError(\"If `feature_columns` is None, then `num_features` cannot be None.\")\n\n        if num_decision_steps < 1:\n            raise ValueError(\"Num decision steps must be greater than 0.\")\n\n        if feature_dim < output_dim:\n            raise ValueError(\"To compute `features_for_coef`, feature_dim must be larger than output dim\")\n\n        feature_dim = int(feature_dim)\n        output_dim = int(output_dim)\n        num_decision_steps = int(num_decision_steps)\n        relaxation_factor = float(relaxation_factor)\n        sparsity_coefficient = float(sparsity_coefficient)\n        batch_momentum = float(batch_momentum)\n        num_groups = max(1, int(num_groups))\n        epsilon = float(epsilon)\n\n        if relaxation_factor < 0.0:\n            raise ValueError(\"`relaxation_factor` cannot be negative !\")\n\n        if sparsity_coefficient < 0.0:\n            raise ValueError(\"`sparsity_coefficient` cannot be negative !\")\n\n        if virtual_batch_size is not None:\n            virtual_batch_size = int(virtual_batch_size)\n\n        if norm_type not in [\"batch\", \"group\"]:\n            raise ValueError(\"`norm_type` must be either `batch` or `group`\")\n\n        self.feature_columns = feature_columns\n        self.num_features = num_features\n        self.feature_dim = feature_dim\n        self.output_dim = output_dim\n\n        self.num_decision_steps = num_decision_steps\n        self.relaxation_factor = relaxation_factor\n        self.sparsity_coefficient = sparsity_coefficient\n        self.norm_type = norm_type\n        self.batch_momentum = batch_momentum\n        self.virtual_batch_size = virtual_batch_size\n        self.num_groups = num_groups\n        self.epsilon = epsilon\n\n        # if num_decision_steps > 1:\n        # features_for_coeff = feature_dim - output_dim\n        # print(f\"[TabNet]: {features_for_coeff} features will be used for decision steps.\")\n\n        if self.feature_columns is not None:\n            self.input_features = L.DenseFeatures(feature_columns, trainable=True)\n\n            if self.norm_type == \"batch\":\n                self.input_bn = L.BatchNormalization(axis=-1, momentum=batch_momentum, name=\"input_bn\")\n            else:\n                self.input_bn = GroupNormalization(axis=-1, groups=self.num_groups, name=\"input_gn\")\n\n        else:\n            self.input_features = None\n            self.input_bn = None\n\n        self.transform_f1 = TransformBlock(\n            2 * self.feature_dim,\n            self.norm_type,\n            self.batch_momentum,\n            self.virtual_batch_size,\n            self.num_groups,\n            block_name=\"f1\",\n        )\n\n        self.transform_f2 = TransformBlock(\n            2 * self.feature_dim,\n            self.norm_type,\n            self.batch_momentum,\n            self.virtual_batch_size,\n            self.num_groups,\n            block_name=\"f2\",\n        )\n\n        self.transform_f3_list = [\n            TransformBlock(\n                2 * self.feature_dim,\n                self.norm_type,\n                self.batch_momentum,\n                self.virtual_batch_size,\n                self.num_groups,\n                block_name=f\"f3_{i}\",\n            )\n            for i in range(self.num_decision_steps)\n        ]\n\n        self.transform_f4_list = [\n            TransformBlock(\n                2 * self.feature_dim,\n                self.norm_type,\n                self.batch_momentum,\n                self.virtual_batch_size,\n                self.num_groups,\n                block_name=f\"f4_{i}\",\n            )\n            for i in range(self.num_decision_steps)\n        ]\n\n        self.transform_coef_list = [\n            TransformBlock(\n                self.num_features,\n                self.norm_type,\n                self.batch_momentum,\n                self.virtual_batch_size,\n                self.num_groups,\n                block_name=f\"coef_{i}\",\n            )\n            for i in range(self.num_decision_steps - 1)\n        ]\n\n        self._step_feature_selection_masks = None\n        self._step_aggregate_feature_selection_mask = None\n\n    def call(self, inputs, training=None):\n        if self.input_features is not None:\n            features = self.input_features(inputs)\n            features = self.input_bn(features, training=training)\n\n        else:\n            features = inputs\n\n        batch_size = tf.shape(features)[0]\n        self._step_feature_selection_masks = []\n        self._step_aggregate_feature_selection_mask = None\n\n        # Initializes decision-step dependent variables.\n        output_aggregated = tf.zeros([batch_size, self.output_dim])\n        masked_features = features\n        mask_values = tf.zeros([batch_size, self.num_features])\n        aggregated_mask_values = tf.zeros([batch_size, self.num_features])\n        complementary_aggregated_mask_values = tf.ones([batch_size, self.num_features])\n\n        total_entropy = 0.0\n        entropy_loss = 0.0\n\n        for ni in range(self.num_decision_steps):\n            # Feature transformer with two shared and two decision step dependent\n            # blocks is used below.=\n            transform_f1 = self.transform_f1(masked_features, training=training)\n            transform_f1 = glu(transform_f1, self.feature_dim)\n\n            transform_f2 = self.transform_f2(transform_f1, training=training)\n            transform_f2 = (glu(transform_f2, self.feature_dim) + transform_f1) * tf.math.sqrt(0.5)\n\n            transform_f3 = self.transform_f3_list[ni](transform_f2, training=training)\n            transform_f3 = (glu(transform_f3, self.feature_dim) + transform_f2) * tf.math.sqrt(0.5)\n\n            transform_f4 = self.transform_f4_list[ni](transform_f3, training=training)\n            transform_f4 = (glu(transform_f4, self.feature_dim) + transform_f3) * tf.math.sqrt(0.5)\n\n            if ni > 0 or self.num_decision_steps == 1:\n                decision_out = tf.nn.relu(transform_f4[:, : self.output_dim])\n\n                # Decision aggregation.\n                output_aggregated += decision_out\n\n                # Aggregated masks are used for visualization of the\n                # feature importance attributes.\n                scale_agg = tf.reduce_sum(decision_out, axis=1, keepdims=True)\n\n                if self.num_decision_steps > 1:\n                    scale_agg = scale_agg / tf.cast(self.num_decision_steps - 1, tf.float32)\n\n                aggregated_mask_values += mask_values * scale_agg\n\n            features_for_coef = transform_f4[:, self.output_dim :]\n\n            if ni < (self.num_decision_steps - 1):\n                # Determines the feature masks via linear and nonlinear\n                # transformations, taking into account of aggregated feature use.\n                mask_values = self.transform_coef_list[ni](features_for_coef, training=training)\n                mask_values *= complementary_aggregated_mask_values\n                mask_values = sparsemax(mask_values, axis=-1)\n\n                # Relaxation factor controls the amount of reuse of features between\n                # different decision blocks and updated with the values of\n                # coefficients.\n                complementary_aggregated_mask_values *= self.relaxation_factor - mask_values\n\n                # Entropy is used to penalize the amount of sparsity in feature\n                # selection.\n                total_entropy += tf.reduce_mean(\n                    tf.reduce_sum(-mask_values * tf.math.log(mask_values + self.epsilon), axis=1)\n                ) / (tf.cast(self.num_decision_steps - 1, tf.float32))\n\n                # Add entropy loss\n                entropy_loss = total_entropy\n\n                # Feature selection.\n                masked_features = tf.multiply(mask_values, features)\n\n                # Visualization of the feature selection mask at decision step ni\n                # tf.summary.image(\n                #     \"Mask for step\" + str(ni),\n                #     tf.expand_dims(tf.expand_dims(mask_values, 0), 3),\n                #     max_outputs=1)\n                mask_at_step_i = tf.expand_dims(tf.expand_dims(mask_values, 0), 3)\n                self._step_feature_selection_masks.append(mask_at_step_i)\n\n            else:\n                # This branch is needed for correct compilation by tf.autograph\n                entropy_loss = 0.0\n\n        # Adds the loss automatically\n        self.add_loss(self.sparsity_coefficient * entropy_loss)\n\n        # Visualization of the aggregated feature importances\n        # tf.summary.image(\n        #     \"Aggregated mask\",\n        #     tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3),\n        #     max_outputs=1)\n\n        agg_mask = tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3)\n        self._step_aggregate_feature_selection_mask = agg_mask\n\n        return output_aggregated\n\n    @property\n    def feature_selection_masks(self):\n        return self._step_feature_selection_masks\n\n    @property\n    def aggregate_feature_selection_mask(self):\n        return self._step_aggregate_feature_selection_mask","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:09.032853Z","iopub.status.busy":"2020-10-22T04:57:09.031716Z","iopub.status.idle":"2020-10-22T04:57:09.034095Z","shell.execute_reply":"2020-10-22T04:57:09.034637Z"},"papermill":{"duration":0.076062,"end_time":"2020-10-22T04:57:09.034772","exception":false,"start_time":"2020-10-22T04:57:08.95871","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class TabNetClassifier(tf.keras.Model):\n    def __init__(\n        self,\n        feature_columns,\n        num_classes,\n        num_features=None,\n        feature_dim=64,\n        output_dim=64,\n        num_decision_steps=5,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-5,\n        norm_type=\"group\",\n        batch_momentum=0.98,\n        virtual_batch_size=None,\n        num_groups=1,\n        epsilon=1e-5,\n        multi_label=False,\n        **kwargs\n    ):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_classes: Number of classes.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'group' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(TabNetClassifier, self).__init__(**kwargs)\n\n        self.num_classes = num_classes\n\n        self.tabnet = TabNet(\n            feature_columns=feature_columns,\n            num_features=num_features,\n            feature_dim=feature_dim,\n            output_dim=output_dim,\n            num_decision_steps=num_decision_steps,\n            relaxation_factor=relaxation_factor,\n            sparsity_coefficient=sparsity_coefficient,\n            norm_type=norm_type,\n            batch_momentum=batch_momentum,\n            virtual_batch_size=virtual_batch_size,\n            num_groups=num_groups,\n            epsilon=epsilon,\n            **kwargs\n        )\n\n        if multi_label:\n\n            self.clf = L.Dense(num_classes, activation=\"sigmoid\", use_bias=False, name=\"classifier\")\n\n        else:\n\n            self.clf = L.Dense(num_classes, activation=\"softmax\", use_bias=False, name=\"classifier\")\n\n    def call(self, inputs, training=None):\n        self.activations = self.tabnet(inputs, training=training)\n        out = self.clf(self.activations)\n\n        return out\n\n    def summary(self, *super_args, **super_kwargs):\n        super().summary(*super_args, **super_kwargs)\n        self.tabnet.summary(*super_args, **super_kwargs)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:09.163892Z","iopub.status.busy":"2020-10-22T04:57:09.155154Z","iopub.status.idle":"2020-10-22T04:57:09.166677Z","shell.execute_reply":"2020-10-22T04:57:09.166117Z"},"papermill":{"duration":0.075922,"end_time":"2020-10-22T04:57:09.166789","exception":false,"start_time":"2020-10-22T04:57:09.090867","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class TabNetRegressor(tf.keras.Model):\n    def __init__(\n        self,\n        feature_columns,\n        num_regressors,\n        num_features=None,\n        feature_dim=64,\n        output_dim=64,\n        num_decision_steps=5,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-5,\n        norm_type=\"group\",\n        batch_momentum=0.98,\n        virtual_batch_size=None,\n        num_groups=1,\n        epsilon=1e-5,\n        **kwargs\n    ):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_regressors: Number of regression variables.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'group' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(TabNetRegressor, self).__init__(**kwargs)\n\n        self.num_regressors = num_regressors\n\n        self.tabnet = TabNet(\n            feature_columns=feature_columns,\n            num_features=num_features,\n            feature_dim=feature_dim,\n            output_dim=output_dim,\n            num_decision_steps=num_decision_steps,\n            relaxation_factor=relaxation_factor,\n            sparsity_coefficient=sparsity_coefficient,\n            norm_type=norm_type,\n            batch_momentum=batch_momentum,\n            virtual_batch_size=virtual_batch_size,\n            num_groups=num_groups,\n            epsilon=epsilon,\n            **kwargs\n        )\n\n        self.regressor = L.Dense(num_regressors, use_bias=False, name=\"regressor\")\n\n    def call(self, inputs, training=None):\n        self.activations = self.tabnet(inputs, training=training)\n        out = self.regressor(self.activations)\n        return out\n\n    def summary(self, *super_args, **super_kwargs):\n        super().summary(*super_args, **super_kwargs)\n        self.tabnet.summary(*super_args, **super_kwargs)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:09.291369Z","iopub.status.busy":"2020-10-22T04:57:09.2906Z","iopub.status.idle":"2020-10-22T04:57:09.293878Z","shell.execute_reply":"2020-10-22T04:57:09.293379Z"},"papermill":{"duration":0.069836,"end_time":"2020-10-22T04:57:09.293992","exception":false,"start_time":"2020-10-22T04:57:09.224156","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Aliases\nTabNetClassification = TabNetClassifier\nTabNetRegression = TabNetRegressor","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:09.429522Z","iopub.status.busy":"2020-10-22T04:57:09.42325Z","iopub.status.idle":"2020-10-22T04:57:09.432131Z","shell.execute_reply":"2020-10-22T04:57:09.431676Z"},"papermill":{"duration":0.082299,"end_time":"2020-10-22T04:57:09.432221","exception":false,"start_time":"2020-10-22T04:57:09.349922","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class StackedTabNet(tf.keras.Model):\n    def __init__(\n        self,\n        feature_columns,\n        num_layers=1,\n        feature_dim=64,\n        output_dim=64,\n        num_features=None,\n        num_decision_steps=5,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-5,\n        norm_type=\"group\",\n        batch_momentum=0.98,\n        virtual_batch_size=None,\n        num_groups=2,\n        epsilon=1e-5,\n        **kwargs\n    ):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_layers: Number of TabNets to stack together.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer. Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n                Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(StackedTabNet, self).__init__(**kwargs)\n\n        if num_layers < 1:\n            raise ValueError(\"`num_layers` cannot be less than 1\")\n\n        if type(feature_dim) not in [list, tuple]:\n            feature_dim = [feature_dim] * num_layers\n\n        if type(output_dim) not in [list, tuple]:\n            output_dim = [output_dim] * num_layers\n\n        if len(feature_dim) != num_layers:\n            raise ValueError(\"`feature_dim` must be a list of length `num_layers`\")\n\n        if len(output_dim) != num_layers:\n            raise ValueError(\"`output_dim` must be a list of length `num_layers`\")\n\n        self.num_layers = num_layers\n\n        layers = []\n        layers.append(\n            TabNet(\n                feature_columns=feature_columns,\n                num_features=num_features,\n                feature_dim=feature_dim[0],\n                output_dim=output_dim[0],\n                num_decision_steps=num_decision_steps,\n                relaxation_factor=relaxation_factor,\n                sparsity_coefficient=sparsity_coefficient,\n                norm_type=norm_type,\n                batch_momentum=batch_momentum,\n                virtual_batch_size=virtual_batch_size,\n                num_groups=num_groups,\n                epsilon=epsilon,\n            )\n        )\n\n        for layer_idx in range(1, num_layers):\n            layers.append(\n                TabNet(\n                    feature_columns=None,\n                    num_features=output_dim[layer_idx - 1],\n                    feature_dim=feature_dim[layer_idx],\n                    output_dim=output_dim[layer_idx],\n                    num_decision_steps=num_decision_steps,\n                    relaxation_factor=relaxation_factor,\n                    sparsity_coefficient=sparsity_coefficient,\n                    norm_type=norm_type,\n                    batch_momentum=batch_momentum,\n                    virtual_batch_size=virtual_batch_size,\n                    num_groups=num_groups,\n                    epsilon=epsilon,\n                )\n            )\n\n        self.tabnet_layers = layers\n\n    def call(self, inputs, training=None):\n        x = self.tabnet_layers[0](inputs, training=training)\n\n        for layer_idx in range(1, self.num_layers):\n            x = self.tabnet_layers[layer_idx](x, training=training)\n\n        return x\n\n    @property\n    def tabnets(self):\n        return self.tabnet_layers\n\n    @property\n    def feature_selection_masks(self):\n        return [tabnet.feature_selection_masks for tabnet in self.tabnet_layers]\n\n    @property\n    def aggregate_feature_selection_mask(self):\n        return [tabnet.aggregate_feature_selection_mask for tabnet in self.tabnet_layers]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:09.561387Z","iopub.status.busy":"2020-10-22T04:57:09.560496Z","iopub.status.idle":"2020-10-22T04:57:09.563321Z","shell.execute_reply":"2020-10-22T04:57:09.562847Z"},"papermill":{"duration":0.075035,"end_time":"2020-10-22T04:57:09.563441","exception":false,"start_time":"2020-10-22T04:57:09.488406","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class StackedTabNetClassifier(tf.keras.Model):\n    def __init__(\n        self,\n        feature_columns,\n        num_classes,\n        num_layers=1,\n        feature_dim=64,\n        output_dim=64,\n        num_features=None,\n        num_decision_steps=5,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-5,\n        norm_type=\"group\",\n        batch_momentum=0.98,\n        virtual_batch_size=None,\n        num_groups=2,\n        epsilon=1e-5,\n        multi_label=False,\n        **kwargs\n    ):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_classes: Number of classes.\n            num_layers: Number of TabNets to stack together.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer. Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n                Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(StackedTabNetClassifier, self).__init__(**kwargs)\n\n        self.num_classes = num_classes\n\n        self.stacked_tabnet = StackedTabNet(\n            feature_columns=feature_columns,\n            num_layers=num_layers,\n            feature_dim=feature_dim,\n            output_dim=output_dim,\n            num_features=num_features,\n            num_decision_steps=num_decision_steps,\n            relaxation_factor=relaxation_factor,\n            sparsity_coefficient=sparsity_coefficient,\n            norm_type=norm_type,\n            batch_momentum=batch_momentum,\n            virtual_batch_size=virtual_batch_size,\n            num_groups=num_groups,\n            epsilon=epsilon,\n        )\n        if multi_label:\n\n            self.clf = L.Dense(num_classes, activation=\"sigmoid\", use_bias=False)\n\n        else:\n\n            self.clf = L.Dense(num_classes, activation=\"softmax\", use_bias=False)\n\n    def call(self, inputs, training=None):\n        self.activations = self.stacked_tabnet(inputs, training=training)\n        out = self.clf(self.activations)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:09.693765Z","iopub.status.busy":"2020-10-22T04:57:09.685768Z","iopub.status.idle":"2020-10-22T04:57:09.696519Z","shell.execute_reply":"2020-10-22T04:57:09.69601Z"},"papermill":{"duration":0.07632,"end_time":"2020-10-22T04:57:09.696619","exception":false,"start_time":"2020-10-22T04:57:09.620299","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class StackedTabNetRegressor(tf.keras.Model):\n    def __init__(\n        self,\n        feature_columns,\n        num_regressors,\n        num_layers=1,\n        feature_dim=64,\n        output_dim=64,\n        num_features=None,\n        num_decision_steps=5,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-5,\n        norm_type=\"group\",\n        batch_momentum=0.98,\n        virtual_batch_size=None,\n        num_groups=2,\n        epsilon=1e-5,\n        **kwargs\n    ):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_regressors: Number of regressors.\n            num_layers: Number of TabNets to stack together.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer. Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n                Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(StackedTabNetRegressor, self).__init__(**kwargs)\n\n        self.num_regressors = num_regressors\n\n        self.stacked_tabnet = StackedTabNet(\n            feature_columns=feature_columns,\n            num_layers=num_layers,\n            feature_dim=feature_dim,\n            output_dim=output_dim,\n            num_features=num_features,\n            num_decision_steps=num_decision_steps,\n            relaxation_factor=relaxation_factor,\n            sparsity_coefficient=sparsity_coefficient,\n            norm_type=norm_type,\n            batch_momentum=batch_momentum,\n            virtual_batch_size=virtual_batch_size,\n            num_groups=num_groups,\n            epsilon=epsilon,\n        )\n\n        self.regressor = L.Dense(num_regressors, use_bias=False)\n\n    def call(self, inputs, training=None):\n        self.activations = self.tabnet(inputs, training=training)\n        out = self.regressor(self.activations)\n        return outl","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:09.819497Z","iopub.status.busy":"2020-10-22T04:57:09.81874Z","iopub.status.idle":"2020-10-22T04:57:09.821437Z","shell.execute_reply":"2020-10-22T04:57:09.821932Z"},"papermill":{"duration":0.0683,"end_time":"2020-10-22T04:57:09.822043","exception":false,"start_time":"2020-10-22T04:57:09.753743","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def create_model_tabnet(num_col, output_dim):\n    model = StackedTabNetClassifier(\n        feature_columns=None,\n        num_classes=output_dim,\n        num_layers=2,\n        feature_dim=1024,\n        output_dim=1024,\n        num_features=num_col,\n        num_decision_steps=1,\n        relaxation_factor=1.5,\n        sparsity_coefficient=0,\n        batch_momentum=0.98,\n        virtual_batch_size=None,\n        norm_type=\"group\",\n        num_groups=-1,\n        multi_label=True,\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.055947,"end_time":"2020-10-22T04:57:09.934344","exception":false,"start_time":"2020-10-22T04:57:09.878397","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Learning"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:10.050967Z","iopub.status.busy":"2020-10-22T04:57:10.050174Z","iopub.status.idle":"2020-10-22T04:57:10.053063Z","shell.execute_reply":"2020-10-22T04:57:10.052598Z"},"papermill":{"duration":0.063141,"end_time":"2020-10-22T04:57:10.053156","exception":false,"start_time":"2020-10-22T04:57:09.990015","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"models = [\"SimpleNN\", \"ResNet\", \"SplitNN\", \"TabNet\"]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:10.17282Z","iopub.status.busy":"2020-10-22T04:57:10.172048Z","iopub.status.idle":"2020-10-22T04:57:10.17513Z","shell.execute_reply":"2020-10-22T04:57:10.174669Z"},"papermill":{"duration":0.065604,"end_time":"2020-10-22T04:57:10.175223","exception":false,"start_time":"2020-10-22T04:57:10.109619","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"N_STARTS = len(models) * 2\n# N_STARTS = 1\nN_SPLITS = 10","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:10.301615Z","iopub.status.busy":"2020-10-22T04:57:10.300856Z","iopub.status.idle":"2020-10-22T04:57:10.303992Z","shell.execute_reply":"2020-10-22T04:57:10.303319Z"},"papermill":{"duration":0.071269,"end_time":"2020-10-22T04:57:10.304184","exception":false,"start_time":"2020-10-22T04:57:10.232915","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"pre_train_models = [\"SimpleNN\", \"ResNet\"]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:10.447154Z","iopub.status.busy":"2020-10-22T04:57:10.431233Z","iopub.status.idle":"2020-10-22T04:57:10.465857Z","shell.execute_reply":"2020-10-22T04:57:10.466322Z"},"papermill":{"duration":0.10564,"end_time":"2020-10-22T04:57:10.46646","exception":false,"start_time":"2020-10-22T04:57:10.36082","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def learning(target, N_STARTS, N_SPLITS, do_predict=False, do_transfer_learning=False):\n    oof = {}\n    predictions = {}\n\n    for seed in range(N_STARTS):\n        model_name = models[seed % len(models)]\n\n        if not do_predict and model_name not in pre_train_models:\n            continue\n\n        seed_result = target.copy()\n        seed_result.loc[:, target.columns] = 0\n        prediction = ss.copy()\n        prediction.loc[:, ss.columns] = 0\n\n        # for SplitNN\n        split_cols = []\n        for _ in range(3):  # len(hidden_units) - 1\n            split_cols.append(np.random.choice(range(len(train.columns)), int(np.ceil(0.8 * len(train.columns)))))\n\n        if do_predict:\n            kfold_seed = random_seed + seed\n        else:\n            kfold_seed = seed\n\n        fix_seed(kfold_seed)\n\n        for n, (tr, te) in enumerate(\n            MultilabelStratifiedKFold(n_splits=N_SPLITS, random_state=kfold_seed, shuffle=True).split(target, target)\n        ):\n            start_time = time()\n\n            # Build Model\n            if model_name == \"SimpleNN\":\n                model = create_model_simple_nn(len(train.columns), len(target.columns))\n\n                if do_transfer_learning:\n                    model_base = create_model_simple_nn(len(train.columns), len(non_target_df.columns))\n\n            elif model_name == \"ResNet\":\n                model = create_model_resnet(len(train.columns), len(train_pca.columns), len(target.columns))\n\n                if do_transfer_learning:\n                    model_base = create_model_resnet(len(train.columns), len(train_pca.columns), len(non_target_df.columns))\n\n            elif model_name == \"SplitNN\":\n                model = create_model_split_nn(len(train.columns), len(target.columns))\n\n                # if do_transfer_learning:\n                #    model_base = create_model_split_nn(len(train.columns), len(non_target_df.columns))\n\n            elif model_name == \"NODE\":\n                model = create_model_node(len(target.columns))\n\n                # if do_transfer_learning:\n                #    model = create_model_node(len(non_target_df.columns))\n\n            elif model_name == \"KernelRidge\":\n                model = KernelRidge(alpha=80, kernel=\"rbf\")\n\n                # if do_transfer_learning:\n                #    model = create_model_node(len(non_target_df.columns))\n\n            elif model_name == \"TabNet\":\n                model = create_model_tabnet(len(train.columns), len(target.columns))\n\n                # if do_transfer_learning:\n                #    model_base = create_model_tabnet(len(train.columns), len(non_target_df.columns))\n\n            else:\n                raise \"Model name is invalid.\"\n\n            # Build Data Sets\n            if model_name == \"SplitNN\":\n                x_tr = [\n                    train.values[tr][:, split_cols[0]],\n                    train.values[tr][:, split_cols[1]],\n                    train.values[tr][:, split_cols[2]],\n                ]\n                x_val = [\n                    train.values[te][:, split_cols[0]],\n                    train.values[te][:, split_cols[1]],\n                    train.values[te][:, split_cols[2]],\n                ]\n                y_tr, y_val = target.astype(float).values[tr], target.astype(float).values[te]\n                x_tt = [test.values[:, split_cols[0]], test.values[:, split_cols[1]], test.values[:, split_cols[2]]]\n\n            elif model_name == \"ResNet\":\n                x_tr = [\n                    train.values[tr],\n                    train_pca.values[tr],\n                ]\n                x_val = [\n                    train.values[te],\n                    train_pca.values[te],\n                ]\n                y_tr, y_val = target.astype(float).values[tr], target.astype(float).values[te]\n                x_tt = [test.values, test_pca.values]\n\n            else:\n                x_tr, x_val = train.values[tr], train.values[te]\n                y_tr, y_val = target.astype(float).values[tr], target.astype(float).values[te]\n                x_tt = test.values\n\n            if model_name == \"KernelRidge\":\n                model.fit(x_tr, y_tr)\n            else:\n                model.compile(\n                    optimizer=tfa.optimizers.AdamW(lr=1e-3, weight_decay=1e-5, clipvalue=756),\n                    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001),\n                    metrics=logloss,\n                )\n\n                checkpoint_path = f\"{model_name}_repeat:{seed}_fold:{n}.hdf5\"\n\n                if do_transfer_learning and model_name not in [\"SplitNN\", \"NODE\", \"TabNet\"]:\n                    model_base.load_weights(checkpoint_path)\n                    for layer in range(len(model_base.layers[:-1])):\n                        model.layers[layer].set_weights(model_base.layers[layer].get_weights())\n\n                cb_checkpt = ModelCheckpoint(\n                    checkpoint_path,\n                    monitor=\"val_loss\",\n                    verbose=0,\n                    save_best_only=True,\n                    save_weights_only=True,\n                    mode=\"min\",\n                )\n                reduce_lr_loss = ReduceLROnPlateau(\n                    monitor=\"val_loss\", factor=0.1, patience=3, verbose=0, min_delta=1e-4, mode=\"min\"\n                )\n                early_stopping = EarlyStopping(\n                    monitor=\"val_loss\",\n                    patience=10,\n                    mode=\"min\",\n                    verbose=0,\n                    min_delta=1e-4,\n                    restore_best_weights=True,\n                )\n                model.fit(\n                    x_tr,\n                    y_tr,\n                    validation_data=(x_val, y_val),\n                    epochs=100,\n                    batch_size=128,\n                    callbacks=[cb_checkpt, reduce_lr_loss, early_stopping],\n                    verbose=0,\n                )\n\n            val_predict = model.predict(x_val)\n            fold_score = metric(target.loc[te].values, val_predict)\n            seed_result.loc[te, target.columns] += val_predict\n\n            if do_predict:\n                test_predict = model.predict(x_tt)\n                prediction.loc[:, target.columns] += test_predict / N_SPLITS\n\n            print(\n                f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] {model_name}: Seed {seed}, Fold {n}:\",\n                fold_score,\n            )\n\n            K.clear_session()\n            del model\n            x = gc.collect()\n\n        oof[f\"{model_name}_{seed}\"] = seed_result\n        predictions[f\"{model_name}_{seed}\"] = prediction\n\n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T04:57:10.588844Z","iopub.status.busy":"2020-10-22T04:57:10.587619Z","iopub.status.idle":"2020-10-22T05:23:50.588386Z","shell.execute_reply":"2020-10-22T05:23:50.587532Z"},"papermill":{"duration":1600.06294,"end_time":"2020-10-22T05:23:50.588593","exception":false,"start_time":"2020-10-22T04:57:10.525653","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Pre train with non-scored labels\n_, _ = learning(non_target_df, N_STARTS, N_SPLITS)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T05:23:50.740436Z","iopub.status.busy":"2020-10-22T05:23:50.739452Z","iopub.status.idle":"2020-10-22T06:28:17.542161Z","shell.execute_reply":"2020-10-22T06:28:17.541295Z"},"papermill":{"duration":3866.884909,"end_time":"2020-10-22T06:28:17.542328","exception":false,"start_time":"2020-10-22T05:23:50.657419","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"oof, predictions = learning(target_df, N_STARTS, N_SPLITS, True, True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.093878,"end_time":"2020-10-22T06:28:17.73127","exception":false,"start_time":"2020-10-22T06:28:17.637392","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Learning - Platt Scaling\n\nhttps://www.kaggle.com/gogo827jz/kernel-logistic-regression-one-for-206-targets?scriptVersionId=43366198"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T06:28:17.952159Z","iopub.status.busy":"2020-10-22T06:28:17.950347Z","iopub.status.idle":"2020-10-22T06:28:17.952915Z","shell.execute_reply":"2020-10-22T06:28:17.953391Z"},"papermill":{"duration":0.127886,"end_time":"2020-10-22T06:28:17.953511","exception":false,"start_time":"2020-10-22T06:28:17.825625","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"for key in oof.keys():\n    if \"KernelRidge\" not in key:\n        continue\n\n    start_time = time()\n\n    x_new = oof[key].values\n    x_tt_new = predictions[key].values\n\n    seed_result = target_df.copy()\n    seed_result.loc[:, target_df.columns] = 0\n    prediction = ss.copy()\n    prediction.loc[:, ss.columns] = 0\n\n    for col in range(target_df.shape[1]):\n        target = target_df.values[:, col]\n\n        if target.sum() >= N_SPLITS:\n            kfold_seed = random_seed + N_STARTS + N_SPLITS + col\n            skf = StratifiedKFold(n_splits=N_SPLITS, random_state=kfold_seed, shuffle=True)\n\n            for n, (tr, te) in enumerate(skf.split(target, target)):\n                x_tr, x_val = x_new[tr, col].reshape(-1, 1), x_new[te, col].reshape(-1, 1)\n                y_tr, y_val = target[tr], target[te]\n                x_tt = x_tt_new[:, col].reshape(-1, 1)\n\n                model = LogisticRegression(penalty=\"none\", max_iter=1000)\n                model.fit(x_tr, y_tr)\n\n                val_predict = model.predict_proba(x_val)\n                seed_result.loc[te, target_df.columns[col]] += val_predict[:, 1]\n\n                test_predict = model.predict_proba(x_tt)\n                prediction.loc[:, target_df.columns[col]] += test_predict[:, 1] / N_SPLITS\n        else:\n            seed_result.loc[:, target_df.columns[col]] += x_new[:, col]\n            prediction.loc[:, target_df.columns[col]] += x_tt_new[:, col]\n\n    seed_score = metric(target_df.values, seed_result.values)\n    print(\n        f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] {key}: \",\n        seed_score,\n    )\n\n    oof[key] = seed_result\n    predictions[key] = prediction","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.094121,"end_time":"2020-10-22T06:28:18.142165","exception":false,"start_time":"2020-10-22T06:28:18.048044","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Cross Validation"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T06:28:18.344531Z","iopub.status.busy":"2020-10-22T06:28:18.343007Z","iopub.status.idle":"2020-10-22T06:28:19.179657Z","shell.execute_reply":"2020-10-22T06:28:19.180169Z"},"papermill":{"duration":0.942841,"end_time":"2020-10-22T06:28:19.180309","exception":false,"start_time":"2020-10-22T06:28:18.237468","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"initial_weights = [1.0 / N_STARTS for _ in range(N_STARTS)] + [1.0]\n\n# https://www.kaggle.com/gogo827jz/optimise-blending-weights-with-bonus-0#Bonus-(Lagrange-Multiplier)\n\n\ndef lagrange_func(params):\n    # weights, _lambda = params\n    blend_ = blend(target_df.values.shape, params[:-1], oof)\n    return metric(target_df.values, blend_) - params[-1] * (sum(params[:-1]) - 1)\n\n\ngrad_l = grad(lagrange_func)\n\n\ndef lagrange_obj(params):\n    # weights, _lambda = params\n    d = grad_l(params).tolist()\n    return d[:-1] + [sum(params[:-1]) - 1]\n\n\nblend_ = blend(target_df.values.shape, initial_weights[:-1], oof)\nprint(f\"Initial blend CV: {metric(target_df.values, blend_)}\")\n\noptimize = False\nif optimize:\n    optimized_weights = fsolve(lagrange_obj, initial_weights)\nelse:\n    optimized_weights = initial_weights\n    \nblend_ = blend(target_df.values.shape, optimized_weights[:-1], oof)\nprint(f\"Optimized blend CV: {metric(target_df.values, blend_)}\")\n\nprint(f\"Optimized weights: {optimized_weights[:-1]}\")\nprint(f\"Check the sum of all weights: {sum(optimized_weights[:-1])}\")","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.097817,"end_time":"2020-10-22T06:28:19.389121","exception":false,"start_time":"2020-10-22T06:28:19.291304","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Postprocessing"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T06:28:19.586635Z","iopub.status.busy":"2020-10-22T06:28:19.585634Z","iopub.status.idle":"2020-10-22T06:28:19.891517Z","shell.execute_reply":"2020-10-22T06:28:19.890886Z"},"papermill":{"duration":0.406065,"end_time":"2020-10-22T06:28:19.891637","exception":false,"start_time":"2020-10-22T06:28:19.485572","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Weighted blend\nsubmit_df.loc[:, target_df.columns] = blend(ss.shape, optimized_weights[:-1], predictions)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T06:28:20.095912Z","iopub.status.busy":"2020-10-22T06:28:20.095025Z","iopub.status.idle":"2020-10-22T06:28:20.136938Z","shell.execute_reply":"2020-10-22T06:28:20.136423Z"},"papermill":{"duration":0.145401,"end_time":"2020-10-22T06:28:20.137037","exception":false,"start_time":"2020-10-22T06:28:19.991636","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Clipping\nsubmit_df.loc[:, target_df.columns] = submit_df.loc[:, target_df.columns].clip(1e-7, 1 - 1e-7)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T06:28:20.352088Z","iopub.status.busy":"2020-10-22T06:28:20.351174Z","iopub.status.idle":"2020-10-22T06:28:20.383311Z","shell.execute_reply":"2020-10-22T06:28:20.382783Z"},"papermill":{"duration":0.145514,"end_time":"2020-10-22T06:28:20.383423","exception":false,"start_time":"2020-10-22T06:28:20.237909","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"submit_df.loc[test_df[\"cp_type\"] == \"ctl_vehicle\", target_df.columns] = 0","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.095059,"end_time":"2020-10-22T06:28:20.574175","exception":false,"start_time":"2020-10-22T06:28:20.479116","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Output"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T06:28:20.77045Z","iopub.status.busy":"2020-10-22T06:28:20.769673Z","iopub.status.idle":"2020-10-22T06:28:23.215393Z","shell.execute_reply":"2020-10-22T06:28:23.214197Z"},"papermill":{"duration":2.545234,"end_time":"2020-10-22T06:28:23.215535","exception":false,"start_time":"2020-10-22T06:28:20.670301","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"submit_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}