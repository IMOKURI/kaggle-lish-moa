{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021802,
     "end_time": "2020-10-12T13:26:44.600780",
     "exception": false,
     "start_time": "2020-10-12T13:26:44.578978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MoA\n",
    "\n",
    "Fork from [here](https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.963258,
     "end_time": "2020-10-12T13:26:45.585220",
     "exception": false,
     "start_time": "2020-10-12T13:26:44.621962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 5.615106,
     "end_time": "2020-10-12T13:26:51.222425",
     "exception": false,
     "start_time": "2020-10-12T13:26:45.607319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import optuna\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022202,
     "end_time": "2020-10-12T13:26:51.266905",
     "exception": false,
     "start_time": "2020-10-12T13:26:51.244703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.031275,
     "end_time": "2020-10-12T13:26:51.320334",
     "exception": false,
     "start_time": "2020-10-12T13:26:51.289059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_seed(seed=2020):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "random_seed = 22\n",
    "fix_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022355,
     "end_time": "2020-10-12T13:26:51.364546",
     "exception": false,
     "start_time": "2020-10-12T13:26:51.342191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Label Smoothing\n",
    "\n",
    "https://www.kaggle.com/rahulsd91/moa-label-smoothing?scriptVersionId=44383706\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.0313,
     "end_time": "2020-10-12T13:26:51.417651",
     "exception": false,
     "start_time": "2020-10-12T13:26:51.386351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction Clipping Thresholds\n",
    "p_min = 0.001\n",
    "p_max = 0.999\n",
    "\n",
    "# Evaluation Metric with clipping and no label smoothing\n",
    "def metric(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, p_min, p_max)\n",
    "    return -K.mean(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021577,
     "end_time": "2020-10-12T13:26:51.461017",
     "exception": false,
     "start_time": "2020-10-12T13:26:51.439440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## データロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 6.163534,
     "end_time": "2020-10-12T13:26:57.646712",
     "exception": false,
     "start_time": "2020-10-12T13:26:51.483178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n",
    "test_df = pd.read_csv(\"../input/lish-moa/test_features.csv\")\n",
    "target_df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\n",
    "non_target_df = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\n",
    "submit_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.09993,
     "end_time": "2020-10-12T13:26:57.769419",
     "exception": false,
     "start_time": "2020-10-12T13:26:57.669489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train_df.copy()\n",
    "test = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022232,
     "end_time": "2020-10-12T13:26:57.814942",
     "exception": false,
     "start_time": "2020-10-12T13:26:57.792710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.04466,
     "end_time": "2020-10-12T13:26:57.882000",
     "exception": false,
     "start_time": "2020-10-12T13:26:57.837340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.loc[:, \"cp_dose\"] = train.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})\n",
    "test.loc[:, \"cp_dose\"] = test.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022031,
     "end_time": "2020-10-12T13:26:57.926657",
     "exception": false,
     "start_time": "2020-10-12T13:26:57.904626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### cp_type が ctrl_vehicle なものは MoA を持たない\n",
    "\n",
    "ので、学習から除外する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.063162,
     "end_time": "2020-10-12T13:26:58.012052",
     "exception": false,
     "start_time": "2020-10-12T13:26:57.948890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_df = target_df.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "train = train.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.046507,
     "end_time": "2020-10-12T13:26:58.081418",
     "exception": false,
     "start_time": "2020-10-12T13:26:58.034911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.drop(\"cp_type\", axis=1)\n",
    "test = test.drop(\"cp_type\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.032336,
     "end_time": "2020-10-12T13:26:58.136751",
     "exception": false,
     "start_time": "2020-10-12T13:26:58.104415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del train[\"sig_id\"]\n",
    "del target_df[\"sig_id\"]\n",
    "del test[\"sig_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.3372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>-0.1498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1608</td>\n",
       "      <td>-1.0500</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>-0.2239</td>\n",
       "      <td>-0.2431</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>-0.1166</td>\n",
       "      <td>-0.1777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.3538</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.3377</td>\n",
       "      <td>-0.4753</td>\n",
       "      <td>-0.2504</td>\n",
       "      <td>-0.7415</td>\n",
       "      <td>0.8413</td>\n",
       "      <td>-0.4259</td>\n",
       "      <td>0.2434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>-0.2252</td>\n",
       "      <td>-0.5565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>-1.2420</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 874 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time  cp_dose     g-0     g-1     g-2     g-3     g-4     g-5  \\\n",
       "0           24        0  1.0620  0.5577 -0.2479 -0.6208 -0.1944 -1.0120   \n",
       "1           72        0  0.0743  0.4087  0.2991  0.0604  1.0190  0.5207   \n",
       "2           48        0  0.6280  0.5817  1.5540 -0.0764 -0.0323  1.2390   \n",
       "3           48        0 -0.5138 -0.2491 -0.2656  0.5288  4.0620 -0.8095   \n",
       "4           72        1 -0.3254 -0.4009  0.9700  0.6919  1.4180 -0.8244   \n",
       "...        ...      ...     ...     ...     ...     ...     ...     ...   \n",
       "21943       72        0  0.1608 -1.0500  0.2551 -0.2239 -0.2431  0.4256   \n",
       "21944       24        1  0.1394 -0.0636 -0.1112 -0.5080 -0.4713  0.7201   \n",
       "21945       24        1 -1.3260  0.3478 -0.3743  0.9905 -0.7178  0.6621   \n",
       "21946       24        0  0.6660  0.2324  0.4392  0.2044  0.8531 -0.0343   \n",
       "21947       72        0 -0.8598  1.0240 -0.1361  0.7952 -0.3611 -3.6750   \n",
       "\n",
       "          g-6     g-7  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n",
       "0     -1.0220 -0.0326  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n",
       "1      0.2341  0.3372  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n",
       "2      0.1715  0.2155  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n",
       "3     -1.9590  0.1792  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880   \n",
       "4     -0.2800 -0.1498  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n",
       "...       ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "21943 -0.1166 -0.1777  ...  0.0789  0.3538  0.0558  0.3377 -0.4753 -0.2504   \n",
       "21944  0.5773  0.3055  ...  0.1969  0.0262 -0.8121  0.3434  0.5372 -0.3246   \n",
       "21945 -0.2252 -0.5565  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086 -0.9798   \n",
       "21946  0.0323  0.0463  ... -0.1105  0.4258 -0.2012  0.1506  1.5230  0.7101   \n",
       "21947 -1.2420  0.9146  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860 -1.4160   \n",
       "\n",
       "         c-96    c-97    c-98    c-99  \n",
       "0     -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4      0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...  \n",
       "21943 -0.7415  0.8413 -0.4259  0.2434  \n",
       "21944  0.0631  0.9171  0.5258  0.4680  \n",
       "21945 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "21946  0.1732  0.7015 -0.6290  0.0740  \n",
       "21947 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[21948 rows x 874 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023429,
     "end_time": "2020-10-12T13:26:58.183729",
     "exception": false,
     "start_time": "2020-10-12T13:26:58.160300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Rank Gauss\n",
    "\n",
    "https://www.kaggle.com/nayuts/moa-pytorch-nn-pca-rankgauss\n",
    "\n",
    "連続値を特定の範囲の閉域に押し込めて、分布の偏りを解消する方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 4.653824,
     "end_time": "2020-10-12T13:27:02.861515",
     "exception": false,
     "start_time": "2020-10-12T13:26:58.207691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_cols = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "c_cols = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "\n",
    "for col in g_cols + c_cols:\n",
    "    transformer = QuantileTransformer(n_quantiles=100, random_state=random_seed, output_distribution=\"normal\")\n",
    "\n",
    "    vec_len = len(train[col].values)\n",
    "    vec_len_test = len(test[col].values)\n",
    "\n",
    "    raw_vec = train[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.067291,
     "end_time": "2020-10-12T13:27:02.952403",
     "exception": false,
     "start_time": "2020-10-12T13:27:02.885112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1.111801</td>\n",
       "      <td>0.903367</td>\n",
       "      <td>-0.433829</td>\n",
       "      <td>-0.971728</td>\n",
       "      <td>-0.286559</td>\n",
       "      <td>-1.011388</td>\n",
       "      <td>-1.357431</td>\n",
       "      <td>-0.041716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435228</td>\n",
       "      <td>0.388106</td>\n",
       "      <td>1.297345</td>\n",
       "      <td>0.882752</td>\n",
       "      <td>-0.202495</td>\n",
       "      <td>1.052112</td>\n",
       "      <td>-0.472513</td>\n",
       "      <td>0.345458</td>\n",
       "      <td>0.591507</td>\n",
       "      <td>0.692516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105667</td>\n",
       "      <td>0.672509</td>\n",
       "      <td>0.257486</td>\n",
       "      <td>0.086759</td>\n",
       "      <td>1.199685</td>\n",
       "      <td>0.691813</td>\n",
       "      <td>0.353695</td>\n",
       "      <td>0.558374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.491941</td>\n",
       "      <td>1.148246</td>\n",
       "      <td>0.728406</td>\n",
       "      <td>0.097171</td>\n",
       "      <td>0.454821</td>\n",
       "      <td>0.773468</td>\n",
       "      <td>0.233309</td>\n",
       "      <td>0.207813</td>\n",
       "      <td>0.964312</td>\n",
       "      <td>1.223121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767036</td>\n",
       "      <td>0.942499</td>\n",
       "      <td>1.408911</td>\n",
       "      <td>-0.126492</td>\n",
       "      <td>-0.028694</td>\n",
       "      <td>1.490985</td>\n",
       "      <td>0.272541</td>\n",
       "      <td>0.359490</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.794302</td>\n",
       "      <td>-0.715229</td>\n",
       "      <td>0.962055</td>\n",
       "      <td>0.096127</td>\n",
       "      <td>-1.176291</td>\n",
       "      <td>-0.361225</td>\n",
       "      <td>-0.727620</td>\n",
       "      <td>-0.248613</td>\n",
       "      <td>-1.076346</td>\n",
       "      <td>1.142699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.755626</td>\n",
       "      <td>-0.297077</td>\n",
       "      <td>-0.455058</td>\n",
       "      <td>0.765972</td>\n",
       "      <td>2.343522</td>\n",
       "      <td>-0.852713</td>\n",
       "      <td>-2.316440</td>\n",
       "      <td>0.301512</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.381920</td>\n",
       "      <td>-0.730154</td>\n",
       "      <td>-1.612183</td>\n",
       "      <td>-1.211000</td>\n",
       "      <td>-0.911943</td>\n",
       "      <td>-1.191839</td>\n",
       "      <td>-1.286279</td>\n",
       "      <td>-0.943448</td>\n",
       "      <td>-0.439482</td>\n",
       "      <td>-0.881278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.468806</td>\n",
       "      <td>-0.504196</td>\n",
       "      <td>0.956769</td>\n",
       "      <td>0.975864</td>\n",
       "      <td>1.447729</td>\n",
       "      <td>-0.863807</td>\n",
       "      <td>-0.346926</td>\n",
       "      <td>-0.227072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045906</td>\n",
       "      <td>0.023813</td>\n",
       "      <td>1.057944</td>\n",
       "      <td>1.737007</td>\n",
       "      <td>0.844923</td>\n",
       "      <td>-0.344784</td>\n",
       "      <td>0.176914</td>\n",
       "      <td>0.457388</td>\n",
       "      <td>-0.428668</td>\n",
       "      <td>1.176713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225332</td>\n",
       "      <td>-1.269833</td>\n",
       "      <td>0.203082</td>\n",
       "      <td>-0.361744</td>\n",
       "      <td>-0.365285</td>\n",
       "      <td>0.574150</td>\n",
       "      <td>-0.118761</td>\n",
       "      <td>-0.271980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144679</td>\n",
       "      <td>0.531217</td>\n",
       "      <td>0.104134</td>\n",
       "      <td>0.547313</td>\n",
       "      <td>-0.547039</td>\n",
       "      <td>-0.278963</td>\n",
       "      <td>-0.822632</td>\n",
       "      <td>1.339243</td>\n",
       "      <td>-0.480911</td>\n",
       "      <td>0.419996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195938</td>\n",
       "      <td>-0.039550</td>\n",
       "      <td>-0.255311</td>\n",
       "      <td>-0.799750</td>\n",
       "      <td>-0.721089</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.779755</td>\n",
       "      <td>0.509043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303342</td>\n",
       "      <td>0.053359</td>\n",
       "      <td>-0.846121</td>\n",
       "      <td>0.555644</td>\n",
       "      <td>0.821718</td>\n",
       "      <td>-0.369418</td>\n",
       "      <td>0.114555</td>\n",
       "      <td>1.470674</td>\n",
       "      <td>0.826858</td>\n",
       "      <td>0.772835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.946077</td>\n",
       "      <td>0.575730</td>\n",
       "      <td>-0.604782</td>\n",
       "      <td>1.298848</td>\n",
       "      <td>-1.057997</td>\n",
       "      <td>0.856044</td>\n",
       "      <td>-0.274794</td>\n",
       "      <td>-0.731116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.655890</td>\n",
       "      <td>0.665706</td>\n",
       "      <td>0.084499</td>\n",
       "      <td>-0.350560</td>\n",
       "      <td>-0.865833</td>\n",
       "      <td>-1.005158</td>\n",
       "      <td>-0.245946</td>\n",
       "      <td>-0.123235</td>\n",
       "      <td>-0.310856</td>\n",
       "      <td>0.613841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.803911</td>\n",
       "      <td>0.405671</td>\n",
       "      <td>0.418646</td>\n",
       "      <td>0.309494</td>\n",
       "      <td>1.068126</td>\n",
       "      <td>-0.020533</td>\n",
       "      <td>0.084007</td>\n",
       "      <td>0.087675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105613</td>\n",
       "      <td>0.641177</td>\n",
       "      <td>-0.236051</td>\n",
       "      <td>0.272655</td>\n",
       "      <td>2.254169</td>\n",
       "      <td>1.132558</td>\n",
       "      <td>0.263331</td>\n",
       "      <td>1.109534</td>\n",
       "      <td>-0.658578</td>\n",
       "      <td>0.173880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.270703</td>\n",
       "      <td>1.569603</td>\n",
       "      <td>-0.287824</td>\n",
       "      <td>1.089356</td>\n",
       "      <td>-0.554418</td>\n",
       "      <td>-2.097418</td>\n",
       "      <td>-1.625089</td>\n",
       "      <td>1.460639</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.533823</td>\n",
       "      <td>-1.358581</td>\n",
       "      <td>-1.700548</td>\n",
       "      <td>-1.593004</td>\n",
       "      <td>-1.710786</td>\n",
       "      <td>-1.249711</td>\n",
       "      <td>-1.531398</td>\n",
       "      <td>-0.551125</td>\n",
       "      <td>-1.254363</td>\n",
       "      <td>-1.809253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 874 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time  cp_dose       g-0       g-1       g-2       g-3       g-4  \\\n",
       "0           24        0  1.111801  0.903367 -0.433829 -0.971728 -0.286559   \n",
       "1           72        0  0.105667  0.672509  0.257486  0.086759  1.199685   \n",
       "2           48        0  0.767036  0.942499  1.408911 -0.126492 -0.028694   \n",
       "3           48        0 -0.755626 -0.297077 -0.455058  0.765972  2.343522   \n",
       "4           72        1 -0.468806 -0.504196  0.956769  0.975864  1.447729   \n",
       "...        ...      ...       ...       ...       ...       ...       ...   \n",
       "21943       72        0  0.225332 -1.269833  0.203082 -0.361744 -0.365285   \n",
       "21944       24        1  0.195938 -0.039550 -0.255311 -0.799750 -0.721089   \n",
       "21945       24        1 -1.946077  0.575730 -0.604782  1.298848 -1.057997   \n",
       "21946       24        0  0.803911  0.405671  0.418646  0.309494  1.068126   \n",
       "21947       72        0 -1.270703  1.569603 -0.287824  1.089356 -0.554418   \n",
       "\n",
       "            g-5       g-6       g-7  ...      c-90      c-91      c-92  \\\n",
       "0     -1.011388 -1.357431 -0.041716  ...  0.435228  0.388106  1.297345   \n",
       "1      0.691813  0.353695  0.558374  ... -0.491941  1.148246  0.728406   \n",
       "2      1.490985  0.272541  0.359490  ... -0.794302 -0.715229  0.962055   \n",
       "3     -0.852713 -2.316440  0.301512  ... -1.381920 -0.730154 -1.612183   \n",
       "4     -0.863807 -0.346926 -0.227072  ...  0.045906  0.023813  1.057944   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "21943  0.574150 -0.118761 -0.271980  ...  0.144679  0.531217  0.104134   \n",
       "21944  0.924354  0.779755  0.509043  ...  0.303342  0.053359 -0.846121   \n",
       "21945  0.856044 -0.274794 -0.731116  ...  0.655890  0.665706  0.084499   \n",
       "21946 -0.020533  0.084007  0.087675  ... -0.105613  0.641177 -0.236051   \n",
       "21947 -2.097418 -1.625089  1.460639  ... -1.533823 -1.358581 -1.700548   \n",
       "\n",
       "           c-93      c-94      c-95      c-96      c-97      c-98      c-99  \n",
       "0      0.882752 -0.202495  1.052112 -0.472513  0.345458  0.591507  0.692516  \n",
       "1      0.097171  0.454821  0.773468  0.233309  0.207813  0.964312  1.223121  \n",
       "2      0.096127 -1.176291 -0.361225 -0.727620 -0.248613 -1.076346  1.142699  \n",
       "3     -1.211000 -0.911943 -1.191839 -1.286279 -0.943448 -0.439482 -0.881278  \n",
       "4      1.737007  0.844923 -0.344784  0.176914  0.457388 -0.428668  1.176713  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "21943  0.547313 -0.547039 -0.278963 -0.822632  1.339243 -0.480911  0.419996  \n",
       "21944  0.555644  0.821718 -0.369418  0.114555  1.470674  0.826858  0.772835  \n",
       "21945 -0.350560 -0.865833 -1.005158 -0.245946 -0.123235 -0.310856  0.613841  \n",
       "21946  0.272655  2.254169  1.132558  0.263331  1.109534 -0.658578  0.173880  \n",
       "21947 -1.593004 -1.710786 -1.249711 -1.531398 -0.551125 -1.254363 -1.809253  \n",
       "\n",
       "[21948 rows x 874 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024422,
     "end_time": "2020-10-12T13:27:03.001749",
     "exception": false,
     "start_time": "2020-10-12T13:27:02.977327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PCA features + Existing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.617329,
     "end_time": "2020-10-12T13:27:03.644535",
     "exception": false,
     "start_time": "2020-10-12T13:27:03.027206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# g-\n",
    "n_comp = 50\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train[g_cols]), pd.DataFrame(test[g_cols])])\n",
    "data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[g_cols])\n",
    "train2 = data2[: train.shape[0]]\n",
    "test2 = data2[-test.shape[0] :]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
    "\n",
    "train = pd.concat((train, train2), axis=1)\n",
    "test = pd.concat((test, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.119545,
     "end_time": "2020-10-12T13:27:03.788341",
     "exception": false,
     "start_time": "2020-10-12T13:27:03.668796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# c-\n",
    "n_comp = 15\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train[c_cols]), pd.DataFrame(test[c_cols])])\n",
    "data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[c_cols])\n",
    "train2 = data2[: train.shape[0]]\n",
    "test2 = data2[-test.shape[0] :]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
    "\n",
    "train = pd.concat((train, train2), axis=1)\n",
    "test = pd.concat((test, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 0.060554,
     "end_time": "2020-10-12T13:27:03.874325",
     "exception": false,
     "start_time": "2020-10-12T13:27:03.813771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_C-5</th>\n",
       "      <th>pca_C-6</th>\n",
       "      <th>pca_C-7</th>\n",
       "      <th>pca_C-8</th>\n",
       "      <th>pca_C-9</th>\n",
       "      <th>pca_C-10</th>\n",
       "      <th>pca_C-11</th>\n",
       "      <th>pca_C-12</th>\n",
       "      <th>pca_C-13</th>\n",
       "      <th>pca_C-14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1.111801</td>\n",
       "      <td>0.903367</td>\n",
       "      <td>-0.433829</td>\n",
       "      <td>-0.971728</td>\n",
       "      <td>-0.286559</td>\n",
       "      <td>-1.011388</td>\n",
       "      <td>-1.357431</td>\n",
       "      <td>-0.041716</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128005</td>\n",
       "      <td>0.425778</td>\n",
       "      <td>-0.343062</td>\n",
       "      <td>-0.195748</td>\n",
       "      <td>0.352685</td>\n",
       "      <td>0.403829</td>\n",
       "      <td>0.277811</td>\n",
       "      <td>0.330617</td>\n",
       "      <td>-0.981467</td>\n",
       "      <td>0.679548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105667</td>\n",
       "      <td>0.672509</td>\n",
       "      <td>0.257486</td>\n",
       "      <td>0.086759</td>\n",
       "      <td>1.199685</td>\n",
       "      <td>0.691813</td>\n",
       "      <td>0.353695</td>\n",
       "      <td>0.558374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.676575</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>-0.388366</td>\n",
       "      <td>-1.150616</td>\n",
       "      <td>0.137138</td>\n",
       "      <td>0.875934</td>\n",
       "      <td>0.165417</td>\n",
       "      <td>-0.475121</td>\n",
       "      <td>-1.175414</td>\n",
       "      <td>-0.739302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767036</td>\n",
       "      <td>0.942499</td>\n",
       "      <td>1.408911</td>\n",
       "      <td>-0.126492</td>\n",
       "      <td>-0.028694</td>\n",
       "      <td>1.490985</td>\n",
       "      <td>0.272541</td>\n",
       "      <td>0.359490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925477</td>\n",
       "      <td>-0.989601</td>\n",
       "      <td>-0.858368</td>\n",
       "      <td>-0.691935</td>\n",
       "      <td>-0.550415</td>\n",
       "      <td>0.581527</td>\n",
       "      <td>0.051508</td>\n",
       "      <td>0.303663</td>\n",
       "      <td>0.545320</td>\n",
       "      <td>-0.118935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.755626</td>\n",
       "      <td>-0.297077</td>\n",
       "      <td>-0.455058</td>\n",
       "      <td>0.765972</td>\n",
       "      <td>2.343522</td>\n",
       "      <td>-0.852713</td>\n",
       "      <td>-2.316440</td>\n",
       "      <td>0.301512</td>\n",
       "      <td>...</td>\n",
       "      <td>1.010039</td>\n",
       "      <td>0.635626</td>\n",
       "      <td>-0.677750</td>\n",
       "      <td>-1.010033</td>\n",
       "      <td>0.952495</td>\n",
       "      <td>2.045002</td>\n",
       "      <td>1.416006</td>\n",
       "      <td>-0.397999</td>\n",
       "      <td>-0.616688</td>\n",
       "      <td>0.142138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.468806</td>\n",
       "      <td>-0.504196</td>\n",
       "      <td>0.956769</td>\n",
       "      <td>0.975864</td>\n",
       "      <td>1.447729</td>\n",
       "      <td>-0.863807</td>\n",
       "      <td>-0.346926</td>\n",
       "      <td>-0.227072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263976</td>\n",
       "      <td>-0.560569</td>\n",
       "      <td>-0.483119</td>\n",
       "      <td>0.088108</td>\n",
       "      <td>-0.381305</td>\n",
       "      <td>0.687056</td>\n",
       "      <td>-0.857168</td>\n",
       "      <td>0.024852</td>\n",
       "      <td>-0.463927</td>\n",
       "      <td>0.182058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225332</td>\n",
       "      <td>-1.269833</td>\n",
       "      <td>0.203082</td>\n",
       "      <td>-0.361744</td>\n",
       "      <td>-0.365285</td>\n",
       "      <td>0.574150</td>\n",
       "      <td>-0.118761</td>\n",
       "      <td>-0.271980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.529939</td>\n",
       "      <td>1.351794</td>\n",
       "      <td>-0.019124</td>\n",
       "      <td>-0.352074</td>\n",
       "      <td>-0.098632</td>\n",
       "      <td>0.090319</td>\n",
       "      <td>-0.409949</td>\n",
       "      <td>1.080525</td>\n",
       "      <td>-0.232746</td>\n",
       "      <td>0.303632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195938</td>\n",
       "      <td>-0.039550</td>\n",
       "      <td>-0.255311</td>\n",
       "      <td>-0.799750</td>\n",
       "      <td>-0.721089</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.779755</td>\n",
       "      <td>0.509043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.697863</td>\n",
       "      <td>0.200118</td>\n",
       "      <td>1.212996</td>\n",
       "      <td>-1.002997</td>\n",
       "      <td>-1.273071</td>\n",
       "      <td>0.963687</td>\n",
       "      <td>0.770501</td>\n",
       "      <td>0.796243</td>\n",
       "      <td>-0.197498</td>\n",
       "      <td>-1.433636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.946077</td>\n",
       "      <td>0.575730</td>\n",
       "      <td>-0.604782</td>\n",
       "      <td>1.298848</td>\n",
       "      <td>-1.057997</td>\n",
       "      <td>0.856044</td>\n",
       "      <td>-0.274794</td>\n",
       "      <td>-0.731116</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.749574</td>\n",
       "      <td>-1.653004</td>\n",
       "      <td>0.925076</td>\n",
       "      <td>0.404705</td>\n",
       "      <td>-0.663696</td>\n",
       "      <td>0.914806</td>\n",
       "      <td>0.134995</td>\n",
       "      <td>-1.697094</td>\n",
       "      <td>0.385734</td>\n",
       "      <td>0.509971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.803911</td>\n",
       "      <td>0.405671</td>\n",
       "      <td>0.418646</td>\n",
       "      <td>0.309494</td>\n",
       "      <td>1.068126</td>\n",
       "      <td>-0.020533</td>\n",
       "      <td>0.084007</td>\n",
       "      <td>0.087675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.492358</td>\n",
       "      <td>0.445483</td>\n",
       "      <td>0.295951</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>1.863752</td>\n",
       "      <td>0.101821</td>\n",
       "      <td>-2.197211</td>\n",
       "      <td>0.720895</td>\n",
       "      <td>-0.241714</td>\n",
       "      <td>-0.673445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.270703</td>\n",
       "      <td>1.569603</td>\n",
       "      <td>-0.287824</td>\n",
       "      <td>1.089356</td>\n",
       "      <td>-0.554418</td>\n",
       "      <td>-2.097418</td>\n",
       "      <td>-1.625089</td>\n",
       "      <td>1.460639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321422</td>\n",
       "      <td>-0.618311</td>\n",
       "      <td>-0.525942</td>\n",
       "      <td>0.268679</td>\n",
       "      <td>0.387513</td>\n",
       "      <td>-0.048869</td>\n",
       "      <td>-0.231102</td>\n",
       "      <td>1.013645</td>\n",
       "      <td>0.226134</td>\n",
       "      <td>-0.864912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 939 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time  cp_dose       g-0       g-1       g-2       g-3       g-4  \\\n",
       "0           24        0  1.111801  0.903367 -0.433829 -0.971728 -0.286559   \n",
       "1           72        0  0.105667  0.672509  0.257486  0.086759  1.199685   \n",
       "2           48        0  0.767036  0.942499  1.408911 -0.126492 -0.028694   \n",
       "3           48        0 -0.755626 -0.297077 -0.455058  0.765972  2.343522   \n",
       "4           72        1 -0.468806 -0.504196  0.956769  0.975864  1.447729   \n",
       "...        ...      ...       ...       ...       ...       ...       ...   \n",
       "21943       72        0  0.225332 -1.269833  0.203082 -0.361744 -0.365285   \n",
       "21944       24        1  0.195938 -0.039550 -0.255311 -0.799750 -0.721089   \n",
       "21945       24        1 -1.946077  0.575730 -0.604782  1.298848 -1.057997   \n",
       "21946       24        0  0.803911  0.405671  0.418646  0.309494  1.068126   \n",
       "21947       72        0 -1.270703  1.569603 -0.287824  1.089356 -0.554418   \n",
       "\n",
       "            g-5       g-6       g-7  ...   pca_C-5   pca_C-6   pca_C-7  \\\n",
       "0     -1.011388 -1.357431 -0.041716  ...  1.128005  0.425778 -0.343062   \n",
       "1      0.691813  0.353695  0.558374  ... -0.676575  0.021277 -0.388366   \n",
       "2      1.490985  0.272541  0.359490  ...  0.925477 -0.989601 -0.858368   \n",
       "3     -0.852713 -2.316440  0.301512  ...  1.010039  0.635626 -0.677750   \n",
       "4     -0.863807 -0.346926 -0.227072  ... -0.263976 -0.560569 -0.483119   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "21943  0.574150 -0.118761 -0.271980  ... -0.529939  1.351794 -0.019124   \n",
       "21944  0.924354  0.779755  0.509043  ... -0.697863  0.200118  1.212996   \n",
       "21945  0.856044 -0.274794 -0.731116  ... -1.749574 -1.653004  0.925076   \n",
       "21946 -0.020533  0.084007  0.087675  ... -0.492358  0.445483  0.295951   \n",
       "21947 -2.097418 -1.625089  1.460639  ...  0.321422 -0.618311 -0.525942   \n",
       "\n",
       "        pca_C-8   pca_C-9  pca_C-10  pca_C-11  pca_C-12  pca_C-13  pca_C-14  \n",
       "0     -0.195748  0.352685  0.403829  0.277811  0.330617 -0.981467  0.679548  \n",
       "1     -1.150616  0.137138  0.875934  0.165417 -0.475121 -1.175414 -0.739302  \n",
       "2     -0.691935 -0.550415  0.581527  0.051508  0.303663  0.545320 -0.118935  \n",
       "3     -1.010033  0.952495  2.045002  1.416006 -0.397999 -0.616688  0.142138  \n",
       "4      0.088108 -0.381305  0.687056 -0.857168  0.024852 -0.463927  0.182058  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "21943 -0.352074 -0.098632  0.090319 -0.409949  1.080525 -0.232746  0.303632  \n",
       "21944 -1.002997 -1.273071  0.963687  0.770501  0.796243 -0.197498 -1.433636  \n",
       "21945  0.404705 -0.663696  0.914806  0.134995 -1.697094  0.385734  0.509971  \n",
       "21946  0.073333  1.863752  0.101821 -2.197211  0.720895 -0.241714 -0.673445  \n",
       "21947  0.268679  0.387513 -0.048869 -0.231102  1.013645  0.226134 -0.864912  \n",
       "\n",
       "[21948 rows x 939 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026575,
     "end_time": "2020-10-12T13:27:03.927532",
     "exception": false,
     "start_time": "2020-10-12T13:27:03.900957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### feature Selection using Variance Encoding\n",
    "\n",
    "分散がしきい値以下の特徴量を捨てます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 0.324466,
     "end_time": "2020-10-12T13:27:04.277321",
     "exception": false,
     "start_time": "2020-10-12T13:27:03.952855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "var_thresh = VarianceThreshold(threshold=0.5)\n",
    "\n",
    "data = train.append(test)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 2:])\n",
    "\n",
    "train_transformed = data_transformed[: train.shape[0]]\n",
    "test_transformed = data_transformed[-test.shape[0] :]\n",
    "\n",
    "\n",
    "train = pd.DataFrame(train[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
    "train = pd.concat([train, pd.DataFrame(train_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test = pd.DataFrame(test[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
    "test = pd.concat([test, pd.DataFrame(test_transformed)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>927</th>\n",
       "      <th>928</th>\n",
       "      <th>929</th>\n",
       "      <th>930</th>\n",
       "      <th>931</th>\n",
       "      <th>932</th>\n",
       "      <th>933</th>\n",
       "      <th>934</th>\n",
       "      <th>935</th>\n",
       "      <th>936</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1.111801</td>\n",
       "      <td>0.903367</td>\n",
       "      <td>-0.433829</td>\n",
       "      <td>-0.971728</td>\n",
       "      <td>-0.286559</td>\n",
       "      <td>-1.011388</td>\n",
       "      <td>-1.357431</td>\n",
       "      <td>-0.041716</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128005</td>\n",
       "      <td>0.425778</td>\n",
       "      <td>-0.343062</td>\n",
       "      <td>-0.195748</td>\n",
       "      <td>0.352685</td>\n",
       "      <td>0.403829</td>\n",
       "      <td>0.277811</td>\n",
       "      <td>0.330617</td>\n",
       "      <td>-0.981467</td>\n",
       "      <td>0.679548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105667</td>\n",
       "      <td>0.672509</td>\n",
       "      <td>0.257486</td>\n",
       "      <td>0.086759</td>\n",
       "      <td>1.199685</td>\n",
       "      <td>0.691813</td>\n",
       "      <td>0.353695</td>\n",
       "      <td>0.558374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.676575</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>-0.388366</td>\n",
       "      <td>-1.150616</td>\n",
       "      <td>0.137138</td>\n",
       "      <td>0.875934</td>\n",
       "      <td>0.165417</td>\n",
       "      <td>-0.475121</td>\n",
       "      <td>-1.175414</td>\n",
       "      <td>-0.739302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767036</td>\n",
       "      <td>0.942499</td>\n",
       "      <td>1.408911</td>\n",
       "      <td>-0.126492</td>\n",
       "      <td>-0.028694</td>\n",
       "      <td>1.490985</td>\n",
       "      <td>0.272541</td>\n",
       "      <td>0.359490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925477</td>\n",
       "      <td>-0.989601</td>\n",
       "      <td>-0.858368</td>\n",
       "      <td>-0.691935</td>\n",
       "      <td>-0.550415</td>\n",
       "      <td>0.581527</td>\n",
       "      <td>0.051508</td>\n",
       "      <td>0.303663</td>\n",
       "      <td>0.545320</td>\n",
       "      <td>-0.118935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.755626</td>\n",
       "      <td>-0.297077</td>\n",
       "      <td>-0.455058</td>\n",
       "      <td>0.765972</td>\n",
       "      <td>2.343522</td>\n",
       "      <td>-0.852713</td>\n",
       "      <td>-2.316440</td>\n",
       "      <td>0.301512</td>\n",
       "      <td>...</td>\n",
       "      <td>1.010039</td>\n",
       "      <td>0.635626</td>\n",
       "      <td>-0.677750</td>\n",
       "      <td>-1.010033</td>\n",
       "      <td>0.952495</td>\n",
       "      <td>2.045002</td>\n",
       "      <td>1.416006</td>\n",
       "      <td>-0.397999</td>\n",
       "      <td>-0.616688</td>\n",
       "      <td>0.142138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.468806</td>\n",
       "      <td>-0.504196</td>\n",
       "      <td>0.956769</td>\n",
       "      <td>0.975864</td>\n",
       "      <td>1.447729</td>\n",
       "      <td>-0.863807</td>\n",
       "      <td>-0.346926</td>\n",
       "      <td>-0.227072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263976</td>\n",
       "      <td>-0.560569</td>\n",
       "      <td>-0.483119</td>\n",
       "      <td>0.088108</td>\n",
       "      <td>-0.381305</td>\n",
       "      <td>0.687056</td>\n",
       "      <td>-0.857168</td>\n",
       "      <td>0.024852</td>\n",
       "      <td>-0.463927</td>\n",
       "      <td>0.182058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225332</td>\n",
       "      <td>-1.269833</td>\n",
       "      <td>0.203082</td>\n",
       "      <td>-0.361744</td>\n",
       "      <td>-0.365285</td>\n",
       "      <td>0.574150</td>\n",
       "      <td>-0.118761</td>\n",
       "      <td>-0.271980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.529939</td>\n",
       "      <td>1.351794</td>\n",
       "      <td>-0.019124</td>\n",
       "      <td>-0.352074</td>\n",
       "      <td>-0.098632</td>\n",
       "      <td>0.090319</td>\n",
       "      <td>-0.409949</td>\n",
       "      <td>1.080525</td>\n",
       "      <td>-0.232746</td>\n",
       "      <td>0.303632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195938</td>\n",
       "      <td>-0.039550</td>\n",
       "      <td>-0.255311</td>\n",
       "      <td>-0.799750</td>\n",
       "      <td>-0.721089</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.779755</td>\n",
       "      <td>0.509043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.697863</td>\n",
       "      <td>0.200118</td>\n",
       "      <td>1.212996</td>\n",
       "      <td>-1.002997</td>\n",
       "      <td>-1.273071</td>\n",
       "      <td>0.963687</td>\n",
       "      <td>0.770501</td>\n",
       "      <td>0.796243</td>\n",
       "      <td>-0.197498</td>\n",
       "      <td>-1.433636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.946077</td>\n",
       "      <td>0.575730</td>\n",
       "      <td>-0.604782</td>\n",
       "      <td>1.298848</td>\n",
       "      <td>-1.057997</td>\n",
       "      <td>0.856044</td>\n",
       "      <td>-0.274794</td>\n",
       "      <td>-0.731116</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.749574</td>\n",
       "      <td>-1.653004</td>\n",
       "      <td>0.925076</td>\n",
       "      <td>0.404705</td>\n",
       "      <td>-0.663696</td>\n",
       "      <td>0.914806</td>\n",
       "      <td>0.134995</td>\n",
       "      <td>-1.697094</td>\n",
       "      <td>0.385734</td>\n",
       "      <td>0.509971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.803911</td>\n",
       "      <td>0.405671</td>\n",
       "      <td>0.418646</td>\n",
       "      <td>0.309494</td>\n",
       "      <td>1.068126</td>\n",
       "      <td>-0.020533</td>\n",
       "      <td>0.084007</td>\n",
       "      <td>0.087675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.492358</td>\n",
       "      <td>0.445483</td>\n",
       "      <td>0.295951</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>1.863752</td>\n",
       "      <td>0.101821</td>\n",
       "      <td>-2.197211</td>\n",
       "      <td>0.720895</td>\n",
       "      <td>-0.241714</td>\n",
       "      <td>-0.673445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.270703</td>\n",
       "      <td>1.569603</td>\n",
       "      <td>-0.287824</td>\n",
       "      <td>1.089356</td>\n",
       "      <td>-0.554418</td>\n",
       "      <td>-2.097418</td>\n",
       "      <td>-1.625089</td>\n",
       "      <td>1.460639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321422</td>\n",
       "      <td>-0.618311</td>\n",
       "      <td>-0.525942</td>\n",
       "      <td>0.268679</td>\n",
       "      <td>0.387513</td>\n",
       "      <td>-0.048869</td>\n",
       "      <td>-0.231102</td>\n",
       "      <td>1.013645</td>\n",
       "      <td>0.226134</td>\n",
       "      <td>-0.864912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 939 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time  cp_dose         0         1         2         3         4  \\\n",
       "0           24        0  1.111801  0.903367 -0.433829 -0.971728 -0.286559   \n",
       "1           72        0  0.105667  0.672509  0.257486  0.086759  1.199685   \n",
       "2           48        0  0.767036  0.942499  1.408911 -0.126492 -0.028694   \n",
       "3           48        0 -0.755626 -0.297077 -0.455058  0.765972  2.343522   \n",
       "4           72        1 -0.468806 -0.504196  0.956769  0.975864  1.447729   \n",
       "...        ...      ...       ...       ...       ...       ...       ...   \n",
       "21943       72        0  0.225332 -1.269833  0.203082 -0.361744 -0.365285   \n",
       "21944       24        1  0.195938 -0.039550 -0.255311 -0.799750 -0.721089   \n",
       "21945       24        1 -1.946077  0.575730 -0.604782  1.298848 -1.057997   \n",
       "21946       24        0  0.803911  0.405671  0.418646  0.309494  1.068126   \n",
       "21947       72        0 -1.270703  1.569603 -0.287824  1.089356 -0.554418   \n",
       "\n",
       "              5         6         7  ...       927       928       929  \\\n",
       "0     -1.011388 -1.357431 -0.041716  ...  1.128005  0.425778 -0.343062   \n",
       "1      0.691813  0.353695  0.558374  ... -0.676575  0.021277 -0.388366   \n",
       "2      1.490985  0.272541  0.359490  ...  0.925477 -0.989601 -0.858368   \n",
       "3     -0.852713 -2.316440  0.301512  ...  1.010039  0.635626 -0.677750   \n",
       "4     -0.863807 -0.346926 -0.227072  ... -0.263976 -0.560569 -0.483119   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "21943  0.574150 -0.118761 -0.271980  ... -0.529939  1.351794 -0.019124   \n",
       "21944  0.924354  0.779755  0.509043  ... -0.697863  0.200118  1.212996   \n",
       "21945  0.856044 -0.274794 -0.731116  ... -1.749574 -1.653004  0.925076   \n",
       "21946 -0.020533  0.084007  0.087675  ... -0.492358  0.445483  0.295951   \n",
       "21947 -2.097418 -1.625089  1.460639  ...  0.321422 -0.618311 -0.525942   \n",
       "\n",
       "            930       931       932       933       934       935       936  \n",
       "0     -0.195748  0.352685  0.403829  0.277811  0.330617 -0.981467  0.679548  \n",
       "1     -1.150616  0.137138  0.875934  0.165417 -0.475121 -1.175414 -0.739302  \n",
       "2     -0.691935 -0.550415  0.581527  0.051508  0.303663  0.545320 -0.118935  \n",
       "3     -1.010033  0.952495  2.045002  1.416006 -0.397999 -0.616688  0.142138  \n",
       "4      0.088108 -0.381305  0.687056 -0.857168  0.024852 -0.463927  0.182058  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "21943 -0.352074 -0.098632  0.090319 -0.409949  1.080525 -0.232746  0.303632  \n",
       "21944 -1.002997 -1.273071  0.963687  0.770501  0.796243 -0.197498 -1.433636  \n",
       "21945  0.404705 -0.663696  0.914806  0.134995 -1.697094  0.385734  0.509971  \n",
       "21946  0.073333  1.863752  0.101821 -2.197211  0.720895 -0.241714 -0.673445  \n",
       "21947  0.268679  0.387513 -0.048869 -0.231102  1.013645  0.226134 -0.864912  \n",
       "\n",
       "[21948 rows x 939 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02676,
     "end_time": "2020-10-12T13:27:04.332310",
     "exception": false,
     "start_time": "2020-10-12T13:27:04.305550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "papermill": {
     "duration": 0.040414,
     "end_time": "2020-10-12T13:27:04.400117",
     "exception": false,
     "start_time": "2020-10-12T13:27:04.359703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            L.Input(len(train.columns)),\n",
    "            L.BatchNormalization(),\n",
    "            tfa.layers.WeightNormalization(L.Dense(2048, activation=\"relu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.4),\n",
    "            tfa.layers.WeightNormalization(L.Dense(2048, activation=\"relu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.4),\n",
    "            tfa.layers.WeightNormalization(L.Dense(206, activation=\"sigmoid\")),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n",
    "        loss=\"binary_crossentropy\",\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet for Tensorflow for MultiLabel\n",
    "\n",
    "https://www.kaggle.com/gogo827jz/moa-stacked-tabnet-baseline-tensorflow-2-0#Model-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_keras_custom_object(cls):\n",
    "    tf.keras.utils.get_custom_objects()[cls.__name__] = cls\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glu(x, n_units=None):\n",
    "    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n",
    "    if n_units is None:\n",
    "        n_units = tf.shape(x)[-1] // 2\n",
    "\n",
    "    return x[..., :n_units] * tf.nn.sigmoid(x[..., n_units:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/activations/sparsemax.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@register_keras_custom_object\n",
    "@tf.function\n",
    "def sparsemax(logits, axis):\n",
    "    \"\"\"Sparsemax activation function [1].\n",
    "    For each batch `i` and class `j` we have\n",
    "      $$sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0)$$\n",
    "    [1]: https://arxiv.org/abs/1602.02068\n",
    "    Args:\n",
    "        logits: Input tensor.\n",
    "        axis: Integer, axis along which the sparsemax operation is applied.\n",
    "    Returns:\n",
    "        Tensor, output of sparsemax transformation. Has the same type and\n",
    "        shape as `logits`.\n",
    "    Raises:\n",
    "        ValueError: In case `dim(logits) == 1`.\n",
    "    \"\"\"\n",
    "    logits = tf.convert_to_tensor(logits, name=\"logits\")\n",
    "\n",
    "    # We need its original shape for shape inference.\n",
    "    shape = logits.get_shape()\n",
    "    rank = shape.rank\n",
    "    is_last_axis = (axis == -1) or (axis == rank - 1)\n",
    "\n",
    "    if is_last_axis:\n",
    "        output = _compute_2d_sparsemax(logits)\n",
    "        output.set_shape(shape)\n",
    "        return output\n",
    "\n",
    "    # If dim is not the last dimension, we have to do a transpose so that we can\n",
    "    # still perform softmax on its last dimension.\n",
    "\n",
    "    # Swap logits' dimension of dim and its last dimension.\n",
    "    rank_op = tf.rank(logits)\n",
    "    axis_norm = axis % rank\n",
    "    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n",
    "\n",
    "    # Do the actual softmax on its last dimension.\n",
    "    output = _compute_2d_sparsemax(logits)\n",
    "    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n",
    "\n",
    "    # Make shape inference work since transpose may erase its static shape.\n",
    "    output.set_shape(shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _swap_axis(logits, dim_index, last_index, **kwargs):\n",
    "    return tf.transpose(\n",
    "        logits,\n",
    "        tf.concat(\n",
    "            [\n",
    "                tf.range(dim_index),\n",
    "                [last_index],\n",
    "                tf.range(dim_index + 1, last_index),\n",
    "                [dim_index],\n",
    "            ],\n",
    "            0,\n",
    "        ),\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_2d_sparsemax(logits):\n",
    "    \"\"\"Performs the sparsemax operation when axis=-1.\"\"\"\n",
    "    shape_op = tf.shape(logits)\n",
    "    obs = tf.math.reduce_prod(shape_op[:-1])\n",
    "    dims = shape_op[-1]\n",
    "\n",
    "    # In the paper, they call the logits z.\n",
    "    # The mean(logits) can be substracted from logits to make the algorithm\n",
    "    # more numerically stable. the instability in this algorithm comes mostly\n",
    "    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n",
    "    # to zero. However, in practise the numerical instability issues are very\n",
    "    # minor and substacting the mean causes extra issues with inf and nan\n",
    "    # input.\n",
    "    # Reshape to [obs, dims] as it is almost free and means the remanining\n",
    "    # code doesn't need to worry about the rank.\n",
    "    z = tf.reshape(logits, [obs, dims])\n",
    "\n",
    "    # sort z\n",
    "    z_sorted, _ = tf.nn.top_k(z, k=dims)\n",
    "\n",
    "    # calculate k(z)\n",
    "    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n",
    "    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n",
    "    z_check = 1 + k * z_sorted > z_cumsum\n",
    "    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n",
    "    # (index + 1) of the last `1` is the same as just summing the number of 1.\n",
    "    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n",
    "\n",
    "    # calculate tau(z)\n",
    "    # If there are inf values or all values are -inf, the k_z will be zero,\n",
    "    # this is mathematically invalid and will also cause the gather_nd to fail.\n",
    "    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n",
    "    # fixed later (see p_safe) by returning p = nan. This results in the same\n",
    "    # behavior as softmax.\n",
    "    k_z_safe = tf.math.maximum(k_z, 1)\n",
    "    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n",
    "    tau_sum = tf.gather_nd(z_cumsum, indices)\n",
    "    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n",
    "\n",
    "    # calculate p\n",
    "    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n",
    "    # If k_z = 0 or if z = nan, then the input is invalid\n",
    "    p_safe = tf.where(\n",
    "        tf.expand_dims(\n",
    "            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n",
    "        p,\n",
    "    )\n",
    "\n",
    "    # Reshape back to original size\n",
    "    p_safe = tf.reshape(p_safe, shape_op)\n",
    "    return p_safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/normalizations.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@register_keras_custom_object\n",
    "class GroupNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"Group normalization layer.\n",
    "    Group Normalization divides the channels into groups and computes\n",
    "    within each group the mean and variance for normalization.\n",
    "    Empirically, its accuracy is more stable than batch norm in a wide\n",
    "    range of small batch sizes, if learning rate is adjusted linearly\n",
    "    with batch sizes.\n",
    "    Relation to Layer Normalization:\n",
    "    If the number of groups is set to 1, then this operation becomes identical\n",
    "    to Layer Normalization.\n",
    "    Relation to Instance Normalization:\n",
    "    If the number of groups is set to the\n",
    "    input dimension (number of groups is equal\n",
    "    to number of channels), then this operation becomes\n",
    "    identical to Instance Normalization.\n",
    "    Arguments\n",
    "        groups: Integer, the number of groups for Group Normalization.\n",
    "            Can be in the range [1, N] where N is the input dimension.\n",
    "            The input dimension must be divisible by the number of groups.\n",
    "        axis: Integer, the axis that should be normalized.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    Output shape\n",
    "        Same shape as input.\n",
    "    References\n",
    "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        groups: int = 2,\n",
    "        axis: int = -1,\n",
    "        epsilon: float = 1e-3,\n",
    "        center: bool = True,\n",
    "        scale: bool = True,\n",
    "        beta_initializer=\"zeros\",\n",
    "        gamma_initializer=\"ones\",\n",
    "        beta_regularizer=None,\n",
    "        gamma_regularizer=None,\n",
    "        beta_constraint=None,\n",
    "        gamma_constraint=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.groups = groups\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n",
    "        self._check_axis()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self._check_if_input_shape_is_none(input_shape)\n",
    "        self._set_number_of_groups_for_instance_norm(input_shape)\n",
    "        self._check_size_of_dimensions(input_shape)\n",
    "        self._create_input_spec(input_shape)\n",
    "\n",
    "        self._add_gamma_weight(input_shape)\n",
    "        self._add_beta_weight(input_shape)\n",
    "        self.built = True\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Training=none is just for compat with batchnorm signature call\n",
    "        input_shape = tf.keras.backend.int_shape(inputs)\n",
    "        tensor_input_shape = tf.shape(inputs)\n",
    "\n",
    "        reshaped_inputs, group_shape = self._reshape_into_groups(inputs, input_shape, tensor_input_shape)\n",
    "\n",
    "        normalized_inputs = self._apply_normalization(reshaped_inputs, input_shape)\n",
    "\n",
    "        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"groups\": self.groups,\n",
    "            \"axis\": self.axis,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"center\": self.center,\n",
    "            \"scale\": self.scale,\n",
    "            \"beta_initializer\": tf.keras.initializers.serialize(self.beta_initializer),\n",
    "            \"gamma_initializer\": tf.keras.initializers.serialize(self.gamma_initializer),\n",
    "            \"beta_regularizer\": tf.keras.regularizers.serialize(self.beta_regularizer),\n",
    "            \"gamma_regularizer\": tf.keras.regularizers.serialize(self.gamma_regularizer),\n",
    "            \"beta_constraint\": tf.keras.constraints.serialize(self.beta_constraint),\n",
    "            \"gamma_constraint\": tf.keras.constraints.serialize(self.gamma_constraint),\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n",
    "\n",
    "        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n",
    "        group_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        group_shape.insert(self.axis, self.groups)\n",
    "        group_shape = tf.stack(group_shape)\n",
    "        reshaped_inputs = tf.reshape(inputs, group_shape)\n",
    "        return reshaped_inputs, group_shape\n",
    "\n",
    "    def _apply_normalization(self, reshaped_inputs, input_shape):\n",
    "\n",
    "        group_shape = tf.keras.backend.int_shape(reshaped_inputs)\n",
    "        group_reduction_axes = list(range(1, len(group_shape)))\n",
    "        axis = -2 if self.axis == -1 else self.axis - 1\n",
    "        group_reduction_axes.pop(axis)\n",
    "\n",
    "        mean, variance = tf.nn.moments(reshaped_inputs, group_reduction_axes, keepdims=True)\n",
    "\n",
    "        gamma, beta = self._get_reshaped_weights(input_shape)\n",
    "        normalized_inputs = tf.nn.batch_normalization(\n",
    "            reshaped_inputs,\n",
    "            mean=mean,\n",
    "            variance=variance,\n",
    "            scale=gamma,\n",
    "            offset=beta,\n",
    "            variance_epsilon=self.epsilon,\n",
    "        )\n",
    "        return normalized_inputs\n",
    "\n",
    "    def _get_reshaped_weights(self, input_shape):\n",
    "        broadcast_shape = self._create_broadcast_shape(input_shape)\n",
    "        gamma = None\n",
    "        beta = None\n",
    "        if self.scale:\n",
    "            gamma = tf.reshape(self.gamma, broadcast_shape)\n",
    "\n",
    "        if self.center:\n",
    "            beta = tf.reshape(self.beta, broadcast_shape)\n",
    "        return gamma, beta\n",
    "\n",
    "    def _check_if_input_shape_is_none(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim is None:\n",
    "            raise ValueError(\n",
    "                \"Axis \" + str(self.axis) + \" of \"\n",
    "                \"input tensor should have a defined dimension \"\n",
    "                \"but the layer received an input with shape \" + str(input_shape) + \".\"\n",
    "            )\n",
    "\n",
    "    def _set_number_of_groups_for_instance_norm(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "\n",
    "        if self.groups == -1:\n",
    "            self.groups = dim\n",
    "\n",
    "    def _check_size_of_dimensions(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim < self.groups:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") cannot be \"\n",
    "                \"more than the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") must be a \"\n",
    "                \"multiple of the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "    def _check_axis(self):\n",
    "\n",
    "        if self.axis == 0:\n",
    "            raise ValueError(\n",
    "                \"You are trying to normalize your batch axis. Do you want to \"\n",
    "                \"use tf.layer.batch_normalization instead\"\n",
    "            )\n",
    "\n",
    "    def _create_input_spec(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=len(input_shape), axes={self.axis: dim})\n",
    "\n",
    "    def _add_gamma_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"gamma\",\n",
    "                initializer=self.gamma_initializer,\n",
    "                regularizer=self.gamma_regularizer,\n",
    "                constraint=self.gamma_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "    def _add_beta_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"beta\",\n",
    "                initializer=self.beta_initializer,\n",
    "                regularizer=self.beta_regularizer,\n",
    "                constraint=self.beta_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.beta = None\n",
    "\n",
    "    def _create_broadcast_shape(self, input_shape):\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        broadcast_shape.insert(self.axis, self.groups)\n",
    "        return broadcast_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformBlock(tf.keras.Model):\n",
    "    def __init__(self, features, norm_type, momentum=0.9, virtual_batch_size=None, groups=2, block_name=\"\", **kwargs):\n",
    "        super(TransformBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.features = features\n",
    "        self.norm_type = norm_type\n",
    "        self.momentum = momentum\n",
    "        self.groups = groups\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "\n",
    "        self.transform = tf.keras.layers.Dense(self.features, use_bias=False, name=f\"transformblock_dense_{block_name}\")\n",
    "\n",
    "        if norm_type == \"batch\":\n",
    "            self.bn = tf.keras.layers.BatchNormalization(\n",
    "                axis=-1,\n",
    "                momentum=momentum,\n",
    "                virtual_batch_size=virtual_batch_size,\n",
    "                name=f\"transformblock_bn_{block_name}\",\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.bn = GroupNormalization(axis=-1, groups=self.groups, name=f\"transformblock_gn_{block_name}\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.transform(inputs)\n",
    "        x = self.bn(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNet(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNet, self).__init__(**kwargs)\n",
    "\n",
    "        # Input checks\n",
    "        if feature_columns is not None:\n",
    "            if type(feature_columns) not in (list, tuple):\n",
    "                raise ValueError(\"`feature_columns` must be a list or a tuple.\")\n",
    "\n",
    "            if len(feature_columns) == 0:\n",
    "                raise ValueError(\"`feature_columns` must be contain at least 1 tf.feature_column !\")\n",
    "\n",
    "            if num_features is None:\n",
    "                num_features = len(feature_columns)\n",
    "            else:\n",
    "                num_features = int(num_features)\n",
    "\n",
    "        else:\n",
    "            if num_features is None:\n",
    "                raise ValueError(\"If `feature_columns` is None, then `num_features` cannot be None.\")\n",
    "\n",
    "        if num_decision_steps < 1:\n",
    "            raise ValueError(\"Num decision steps must be greater than 0.\")\n",
    "\n",
    "        if feature_dim < output_dim:\n",
    "            raise ValueError(\"To compute `features_for_coef`, feature_dim must be larger than output dim\")\n",
    "\n",
    "        feature_dim = int(feature_dim)\n",
    "        output_dim = int(output_dim)\n",
    "        num_decision_steps = int(num_decision_steps)\n",
    "        relaxation_factor = float(relaxation_factor)\n",
    "        sparsity_coefficient = float(sparsity_coefficient)\n",
    "        batch_momentum = float(batch_momentum)\n",
    "        num_groups = max(1, int(num_groups))\n",
    "        epsilon = float(epsilon)\n",
    "\n",
    "        if relaxation_factor < 0.0:\n",
    "            raise ValueError(\"`relaxation_factor` cannot be negative !\")\n",
    "\n",
    "        if sparsity_coefficient < 0.0:\n",
    "            raise ValueError(\"`sparsity_coefficient` cannot be negative !\")\n",
    "\n",
    "        if virtual_batch_size is not None:\n",
    "            virtual_batch_size = int(virtual_batch_size)\n",
    "\n",
    "        if norm_type not in [\"batch\", \"group\"]:\n",
    "            raise ValueError(\"`norm_type` must be either `batch` or `group`\")\n",
    "\n",
    "        self.feature_columns = feature_columns\n",
    "        self.num_features = num_features\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.num_decision_steps = num_decision_steps\n",
    "        self.relaxation_factor = relaxation_factor\n",
    "        self.sparsity_coefficient = sparsity_coefficient\n",
    "        self.norm_type = norm_type\n",
    "        self.batch_momentum = batch_momentum\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.num_groups = num_groups\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # if num_decision_steps > 1:\n",
    "        # features_for_coeff = feature_dim - output_dim\n",
    "        # print(f\"[TabNet]: {features_for_coeff} features will be used for decision steps.\")\n",
    "\n",
    "        if self.feature_columns is not None:\n",
    "            self.input_features = tf.keras.layers.DenseFeatures(feature_columns, trainable=True)\n",
    "\n",
    "            if self.norm_type == \"batch\":\n",
    "                self.input_bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=batch_momentum, name=\"input_bn\")\n",
    "            else:\n",
    "                self.input_bn = GroupNormalization(axis=-1, groups=self.num_groups, name=\"input_gn\")\n",
    "\n",
    "        else:\n",
    "            self.input_features = None\n",
    "            self.input_bn = None\n",
    "\n",
    "        self.transform_f1 = TransformBlock(\n",
    "            2 * self.feature_dim,\n",
    "            self.norm_type,\n",
    "            self.batch_momentum,\n",
    "            self.virtual_batch_size,\n",
    "            self.num_groups,\n",
    "            block_name=\"f1\",\n",
    "        )\n",
    "\n",
    "        self.transform_f2 = TransformBlock(\n",
    "            2 * self.feature_dim,\n",
    "            self.norm_type,\n",
    "            self.batch_momentum,\n",
    "            self.virtual_batch_size,\n",
    "            self.num_groups,\n",
    "            block_name=\"f2\",\n",
    "        )\n",
    "\n",
    "        self.transform_f3_list = [\n",
    "            TransformBlock(\n",
    "                2 * self.feature_dim,\n",
    "                self.norm_type,\n",
    "                self.batch_momentum,\n",
    "                self.virtual_batch_size,\n",
    "                self.num_groups,\n",
    "                block_name=f\"f3_{i}\",\n",
    "            )\n",
    "            for i in range(self.num_decision_steps)\n",
    "        ]\n",
    "\n",
    "        self.transform_f4_list = [\n",
    "            TransformBlock(\n",
    "                2 * self.feature_dim,\n",
    "                self.norm_type,\n",
    "                self.batch_momentum,\n",
    "                self.virtual_batch_size,\n",
    "                self.num_groups,\n",
    "                block_name=f\"f4_{i}\",\n",
    "            )\n",
    "            for i in range(self.num_decision_steps)\n",
    "        ]\n",
    "\n",
    "        self.transform_coef_list = [\n",
    "            TransformBlock(\n",
    "                self.num_features,\n",
    "                self.norm_type,\n",
    "                self.batch_momentum,\n",
    "                self.virtual_batch_size,\n",
    "                self.num_groups,\n",
    "                block_name=f\"coef_{i}\",\n",
    "            )\n",
    "            for i in range(self.num_decision_steps - 1)\n",
    "        ]\n",
    "\n",
    "        self._step_feature_selection_masks = None\n",
    "        self._step_aggregate_feature_selection_mask = None\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if self.input_features is not None:\n",
    "            features = self.input_features(inputs)\n",
    "            features = self.input_bn(features, training=training)\n",
    "\n",
    "        else:\n",
    "            features = inputs\n",
    "\n",
    "        batch_size = tf.shape(features)[0]\n",
    "        self._step_feature_selection_masks = []\n",
    "        self._step_aggregate_feature_selection_mask = None\n",
    "\n",
    "        # Initializes decision-step dependent variables.\n",
    "        output_aggregated = tf.zeros([batch_size, self.output_dim])\n",
    "        masked_features = features\n",
    "        mask_values = tf.zeros([batch_size, self.num_features])\n",
    "        aggregated_mask_values = tf.zeros([batch_size, self.num_features])\n",
    "        complementary_aggregated_mask_values = tf.ones([batch_size, self.num_features])\n",
    "\n",
    "        total_entropy = 0.0\n",
    "        entropy_loss = 0.0\n",
    "\n",
    "        for ni in range(self.num_decision_steps):\n",
    "            # Feature transformer with two shared and two decision step dependent\n",
    "            # blocks is used below.=\n",
    "            transform_f1 = self.transform_f1(masked_features, training=training)\n",
    "            transform_f1 = glu(transform_f1, self.feature_dim)\n",
    "\n",
    "            transform_f2 = self.transform_f2(transform_f1, training=training)\n",
    "            transform_f2 = (glu(transform_f2, self.feature_dim) + transform_f1) * tf.math.sqrt(0.5)\n",
    "\n",
    "            transform_f3 = self.transform_f3_list[ni](transform_f2, training=training)\n",
    "            transform_f3 = (glu(transform_f3, self.feature_dim) + transform_f2) * tf.math.sqrt(0.5)\n",
    "\n",
    "            transform_f4 = self.transform_f4_list[ni](transform_f3, training=training)\n",
    "            transform_f4 = (glu(transform_f4, self.feature_dim) + transform_f3) * tf.math.sqrt(0.5)\n",
    "\n",
    "            if ni > 0 or self.num_decision_steps == 1:\n",
    "                decision_out = tf.nn.relu(transform_f4[:, : self.output_dim])\n",
    "\n",
    "                # Decision aggregation.\n",
    "                output_aggregated += decision_out\n",
    "\n",
    "                # Aggregated masks are used for visualization of the\n",
    "                # feature importance attributes.\n",
    "                scale_agg = tf.reduce_sum(decision_out, axis=1, keepdims=True)\n",
    "\n",
    "                if self.num_decision_steps > 1:\n",
    "                    scale_agg = scale_agg / tf.cast(self.num_decision_steps - 1, tf.float32)\n",
    "\n",
    "                aggregated_mask_values += mask_values * scale_agg\n",
    "\n",
    "            features_for_coef = transform_f4[:, self.output_dim :]\n",
    "\n",
    "            if ni < (self.num_decision_steps - 1):\n",
    "                # Determines the feature masks via linear and nonlinear\n",
    "                # transformations, taking into account of aggregated feature use.\n",
    "                mask_values = self.transform_coef_list[ni](features_for_coef, training=training)\n",
    "                mask_values *= complementary_aggregated_mask_values\n",
    "                mask_values = sparsemax(mask_values, axis=-1)\n",
    "\n",
    "                # Relaxation factor controls the amount of reuse of features between\n",
    "                # different decision blocks and updated with the values of\n",
    "                # coefficients.\n",
    "                complementary_aggregated_mask_values *= self.relaxation_factor - mask_values\n",
    "\n",
    "                # Entropy is used to penalize the amount of sparsity in feature\n",
    "                # selection.\n",
    "                total_entropy += tf.reduce_mean(\n",
    "                    tf.reduce_sum(-mask_values * tf.math.log(mask_values + self.epsilon), axis=1)\n",
    "                ) / (tf.cast(self.num_decision_steps - 1, tf.float32))\n",
    "\n",
    "                # Add entropy loss\n",
    "                entropy_loss = total_entropy\n",
    "\n",
    "                # Feature selection.\n",
    "                masked_features = tf.multiply(mask_values, features)\n",
    "\n",
    "                # Visualization of the feature selection mask at decision step ni\n",
    "                # tf.summary.image(\n",
    "                #     \"Mask for step\" + str(ni),\n",
    "                #     tf.expand_dims(tf.expand_dims(mask_values, 0), 3),\n",
    "                #     max_outputs=1)\n",
    "                mask_at_step_i = tf.expand_dims(tf.expand_dims(mask_values, 0), 3)\n",
    "                self._step_feature_selection_masks.append(mask_at_step_i)\n",
    "\n",
    "            else:\n",
    "                # This branch is needed for correct compilation by tf.autograph\n",
    "                entropy_loss = 0.0\n",
    "\n",
    "        # Adds the loss automatically\n",
    "        self.add_loss(self.sparsity_coefficient * entropy_loss)\n",
    "\n",
    "        # Visualization of the aggregated feature importances\n",
    "        # tf.summary.image(\n",
    "        #     \"Aggregated mask\",\n",
    "        #     tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3),\n",
    "        #     max_outputs=1)\n",
    "\n",
    "        agg_mask = tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3)\n",
    "        self._step_aggregate_feature_selection_mask = agg_mask\n",
    "\n",
    "        return output_aggregated\n",
    "\n",
    "    @property\n",
    "    def feature_selection_masks(self):\n",
    "        return self._step_feature_selection_masks\n",
    "\n",
    "    @property\n",
    "    def aggregate_feature_selection_mask(self):\n",
    "        return self._step_aggregate_feature_selection_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_classes,\n",
    "        num_features=None,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=1,\n",
    "        epsilon=1e-5,\n",
    "        multi_label=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_classes: Number of classes.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'group' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNetClassifier, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.tabnet = TabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_features=num_features,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        if multi_label:\n",
    "\n",
    "            self.clf = tf.keras.layers.Dense(num_classes, activation=\"sigmoid\", use_bias=False, name=\"classifier\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.clf = tf.keras.layers.Dense(num_classes, activation=\"softmax\", use_bias=False, name=\"classifier\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.clf(self.activations)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def summary(self, *super_args, **super_kwargs):\n",
    "        super().summary(*super_args, **super_kwargs)\n",
    "        self.tabnet.summary(*super_args, **super_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetRegressor(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_regressors,\n",
    "        num_features=None,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=1,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_regressors: Number of regression variables.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'group' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNetRegressor, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_regressors = num_regressors\n",
    "\n",
    "        self.tabnet = TabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_features=num_features,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.regressor = tf.keras.layers.Dense(num_regressors, use_bias=False, name=\"regressor\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.regressor(self.activations)\n",
    "        return out\n",
    "\n",
    "    def summary(self, *super_args, **super_kwargs):\n",
    "        super().summary(*super_args, **super_kwargs)\n",
    "        self.tabnet.summary(*super_args, **super_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aliases\n",
    "TabNetClassification = TabNetClassifier\n",
    "TabNetRegression = TabNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedTabNet(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_layers=1,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_layers: Number of TabNets to stack together.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer. Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "                Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(StackedTabNet, self).__init__(**kwargs)\n",
    "\n",
    "        if num_layers < 1:\n",
    "            raise ValueError(\"`num_layers` cannot be less than 1\")\n",
    "\n",
    "        if type(feature_dim) not in [list, tuple]:\n",
    "            feature_dim = [feature_dim] * num_layers\n",
    "\n",
    "        if type(output_dim) not in [list, tuple]:\n",
    "            output_dim = [output_dim] * num_layers\n",
    "\n",
    "        if len(feature_dim) != num_layers:\n",
    "            raise ValueError(\"`feature_dim` must be a list of length `num_layers`\")\n",
    "\n",
    "        if len(output_dim) != num_layers:\n",
    "            raise ValueError(\"`output_dim` must be a list of length `num_layers`\")\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            TabNet(\n",
    "                feature_columns=feature_columns,\n",
    "                num_features=num_features,\n",
    "                feature_dim=feature_dim[0],\n",
    "                output_dim=output_dim[0],\n",
    "                num_decision_steps=num_decision_steps,\n",
    "                relaxation_factor=relaxation_factor,\n",
    "                sparsity_coefficient=sparsity_coefficient,\n",
    "                norm_type=norm_type,\n",
    "                batch_momentum=batch_momentum,\n",
    "                virtual_batch_size=virtual_batch_size,\n",
    "                num_groups=num_groups,\n",
    "                epsilon=epsilon,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for layer_idx in range(1, num_layers):\n",
    "            layers.append(\n",
    "                TabNet(\n",
    "                    feature_columns=None,\n",
    "                    num_features=output_dim[layer_idx - 1],\n",
    "                    feature_dim=feature_dim[layer_idx],\n",
    "                    output_dim=output_dim[layer_idx],\n",
    "                    num_decision_steps=num_decision_steps,\n",
    "                    relaxation_factor=relaxation_factor,\n",
    "                    sparsity_coefficient=sparsity_coefficient,\n",
    "                    norm_type=norm_type,\n",
    "                    batch_momentum=batch_momentum,\n",
    "                    virtual_batch_size=virtual_batch_size,\n",
    "                    num_groups=num_groups,\n",
    "                    epsilon=epsilon,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.tabnet_layers = layers\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.tabnet_layers[0](inputs, training=training)\n",
    "\n",
    "        for layer_idx in range(1, self.num_layers):\n",
    "            x = self.tabnet_layers[layer_idx](x, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def tabnets(self):\n",
    "        return self.tabnet_layers\n",
    "\n",
    "    @property\n",
    "    def feature_selection_masks(self):\n",
    "        return [tabnet.feature_selection_masks for tabnet in self.tabnet_layers]\n",
    "\n",
    "    @property\n",
    "    def aggregate_feature_selection_mask(self):\n",
    "        return [tabnet.aggregate_feature_selection_mask for tabnet in self.tabnet_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedTabNetClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_classes,\n",
    "        num_layers=1,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        multi_label=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_classes: Number of classes.\n",
    "            num_layers: Number of TabNets to stack together.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer. Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "                Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(StackedTabNetClassifier, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.stacked_tabnet = StackedTabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_layers=num_layers,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_features=num_features,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "        if multi_label:\n",
    "\n",
    "            self.clf = tf.keras.layers.Dense(num_classes, activation=\"sigmoid\", use_bias=False)\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.clf = tf.keras.layers.Dense(num_classes, activation=\"softmax\", use_bias=False)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.stacked_tabnet(inputs, training=training)\n",
    "        out = self.clf(self.activations)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedTabNetRegressor(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_regressors,\n",
    "        num_layers=1,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_regressors: Number of regressors.\n",
    "            num_layers: Number of TabNets to stack together.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer. Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "                Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(StackedTabNetRegressor, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_regressors = num_regressors\n",
    "\n",
    "        self.stacked_tabnet = StackedTabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_layers=num_layers,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_features=num_features,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "\n",
    "        self.regressor = tf.keras.layers.Dense(num_regressors, use_bias=False)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.regressor(self.activations)\n",
    "        return outl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026796,
     "end_time": "2020-10-12T13:27:04.454105",
     "exception": false,
     "start_time": "2020-10-12T13:27:04.427309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "papermill": {
     "duration": 1146.878587,
     "end_time": "2020-10-12T13:46:11.358530",
     "exception": false,
     "start_time": "2020-10-12T13:27:04.479943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Repeat: 0, Fold: 0 =====\n",
      "Epoch 1/50\n",
      "147/147 [==============================] - 4s 26ms/step - loss: 0.5209 - val_loss: 0.1630\n",
      "Epoch 2/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0826 - val_loss: 0.0400\n",
      "Epoch 3/50\n",
      "147/147 [==============================] - 3s 24ms/step - loss: 0.0319 - val_loss: 0.0253\n",
      "Epoch 4/50\n",
      "147/147 [==============================] - 3s 24ms/step - loss: 0.0241 - val_loss: 0.0215\n",
      "Epoch 5/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0216 - val_loss: 0.0200\n",
      "Epoch 6/50\n",
      "147/147 [==============================] - 3s 24ms/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 7/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0189 - val_loss: 0.0184\n",
      "Epoch 8/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0181 - val_loss: 0.0181\n",
      "Epoch 9/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0176 - val_loss: 0.0176\n",
      "Epoch 10/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0170 - val_loss: 0.0174\n",
      "Epoch 11/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0165 - val_loss: 0.0172\n",
      "Epoch 12/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0158 - val_loss: 0.0168\n",
      "Epoch 13/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0153 - val_loss: 0.0168\n",
      "Epoch 14/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0146 - val_loss: 0.0167\n",
      "Epoch 15/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0142 - val_loss: 0.0166\n",
      "Epoch 16/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0133 - val_loss: 0.0166\n",
      "Epoch 17/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0125 - val_loss: 0.0167\n",
      "Epoch 18/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0116\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0116 - val_loss: 0.0168\n",
      "Epoch 19/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0101 - val_loss: 0.0165\n",
      "Epoch 20/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0097\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0097 - val_loss: 0.0165\n",
      "Epoch 21/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0095 - val_loss: 0.0165\n",
      "Epoch 22/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0094\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0094 - val_loss: 0.0165\n",
      "Epoch 23/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0094 - val_loss: 0.0165\n",
      "Epoch 24/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0095 - val_loss: 0.0165\n",
      "Epoch 25/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0094 - val_loss: 0.0165\n",
      "Epoch 26/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0094 - val_loss: 0.0165\n",
      "Epoch 27/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0094 - val_loss: 0.0165\n",
      "Epoch 28/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0094\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0094 - val_loss: 0.0165\n",
      "Epoch 29/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0094 - val_loss: 0.0165\n",
      "Epoch 30/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0094\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0094 - val_loss: 0.0165\n",
      "Epoch 31/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0095Restoring model weights from the end of the best epoch.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0095 - val_loss: 0.0165\n",
      "Epoch 00031: early stopping\n",
      "\n",
      "===== Repeat: 0, Fold: 1 =====\n",
      "Epoch 1/50\n",
      "147/147 [==============================] - 4s 25ms/step - loss: 0.5165 - val_loss: 0.1490\n",
      "Epoch 2/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0826 - val_loss: 0.0369\n",
      "Epoch 3/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0324 - val_loss: 0.0248\n",
      "Epoch 4/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0242 - val_loss: 0.0213\n",
      "Epoch 5/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0213 - val_loss: 0.0201\n",
      "Epoch 6/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0199 - val_loss: 0.0190\n",
      "Epoch 7/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0189 - val_loss: 0.0184\n",
      "Epoch 8/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0183 - val_loss: 0.0179\n",
      "Epoch 9/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0177 - val_loss: 0.0176\n",
      "Epoch 10/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0171 - val_loss: 0.0173\n",
      "Epoch 11/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0164 - val_loss: 0.0171\n",
      "Epoch 12/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0159 - val_loss: 0.0169\n",
      "Epoch 13/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0153 - val_loss: 0.0167\n",
      "Epoch 14/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0146 - val_loss: 0.0166\n",
      "Epoch 15/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0139 - val_loss: 0.0165\n",
      "Epoch 16/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0132 - val_loss: 0.0164\n",
      "Epoch 17/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0124 - val_loss: 0.0166\n",
      "Epoch 18/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0115\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0115 - val_loss: 0.0167\n",
      "Epoch 19/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0099 - val_loss: 0.0164\n",
      "Epoch 20/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0095 - val_loss: 0.0164\n",
      "Epoch 21/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0093 - val_loss: 0.0164\n",
      "Epoch 22/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0092\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0092 - val_loss: 0.0164\n",
      "Epoch 23/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0092 - val_loss: 0.0164\n",
      "Epoch 24/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0092\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0092 - val_loss: 0.0164\n",
      "Epoch 00024: early stopping\n",
      "\n",
      "===== Repeat: 0, Fold: 2 =====\n",
      "Epoch 1/50\n",
      "147/147 [==============================] - 4s 25ms/step - loss: 0.5203 - val_loss: 0.1461\n",
      "Epoch 2/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0822 - val_loss: 0.0370\n",
      "Epoch 3/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0323 - val_loss: 0.0254\n",
      "Epoch 4/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0242 - val_loss: 0.0214\n",
      "Epoch 5/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0214 - val_loss: 0.0198\n",
      "Epoch 6/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 7/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0191 - val_loss: 0.0185\n",
      "Epoch 8/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0185 - val_loss: 0.0181\n",
      "Epoch 9/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0176 - val_loss: 0.0179\n",
      "Epoch 10/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0171 - val_loss: 0.0176\n",
      "Epoch 11/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0165 - val_loss: 0.0174\n",
      "Epoch 12/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0160 - val_loss: 0.0173\n",
      "Epoch 13/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0154 - val_loss: 0.0170\n",
      "Epoch 14/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0147 - val_loss: 0.0169\n",
      "Epoch 15/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0139\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0139 - val_loss: 0.0171\n",
      "Epoch 16/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0128 - val_loss: 0.0166\n",
      "Epoch 17/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0126 - val_loss: 0.0166\n",
      "Epoch 18/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0124\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0124 - val_loss: 0.0167\n",
      "Epoch 19/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0166\n",
      "Epoch 20/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0123\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0166\n",
      "Epoch 21/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0166\n",
      "Epoch 22/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0166\n",
      "Epoch 23/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0166\n",
      "Epoch 24/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0122 - val_loss: 0.0166\n",
      "Epoch 25/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0122Restoring model weights from the end of the best epoch.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0166\n",
      "Epoch 00025: early stopping\n",
      "\n",
      "===== Repeat: 0, Fold: 3 =====\n",
      "Epoch 1/50\n",
      "147/147 [==============================] - 4s 27ms/step - loss: 0.5207 - val_loss: 0.1627\n",
      "Epoch 2/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0827 - val_loss: 0.0376\n",
      "Epoch 3/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0322 - val_loss: 0.0257\n",
      "Epoch 4/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0244 - val_loss: 0.0216\n",
      "Epoch 5/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0215 - val_loss: 0.0198\n",
      "Epoch 6/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0199 - val_loss: 0.0190\n",
      "Epoch 7/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0191 - val_loss: 0.0184\n",
      "Epoch 8/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0183 - val_loss: 0.0178\n",
      "Epoch 9/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0176 - val_loss: 0.0178\n",
      "Epoch 10/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0171 - val_loss: 0.0172\n",
      "Epoch 11/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0164 - val_loss: 0.0170\n",
      "Epoch 12/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0159 - val_loss: 0.0169\n",
      "Epoch 13/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0153 - val_loss: 0.0167\n",
      "Epoch 14/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0147 - val_loss: 0.0170\n",
      "Epoch 15/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0140 - val_loss: 0.0167\n",
      "Epoch 16/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0128 - val_loss: 0.0164\n",
      "Epoch 17/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0126 - val_loss: 0.0164\n",
      "Epoch 18/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0124\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0124 - val_loss: 0.0164\n",
      "Epoch 19/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0164\n",
      "Epoch 20/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0164\n",
      "Epoch 21/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0121 - val_loss: 0.0164\n",
      "Epoch 22/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0164\n",
      "Epoch 23/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0164\n",
      "Epoch 24/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0121 - val_loss: 0.0164\n",
      "Epoch 25/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0164\n",
      "Epoch 26/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0121\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0121 - val_loss: 0.0164\n",
      "Epoch 27/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0164\n",
      "Epoch 28/50\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0164\n",
      "Epoch 29/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0164\n",
      "Epoch 30/50\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.0121\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0121 - val_loss: 0.0164\n",
      "Epoch 31/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0164\n",
      "Epoch 32/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0122- ETA: 1s  - ETA: 0s - loss: 0\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0164\n",
      "Epoch 33/50\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0121 - val_loss: 0.0164\n",
      "Epoch 34/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0122 - val_loss: 0.0164\n",
      "Epoch 00034: early stopping\n",
      "\n",
      "===== Repeat: 0, Fold: 4 =====\n",
      "Epoch 1/50\n",
      "147/147 [==============================] - 4s 24ms/step - loss: 0.5181 - val_loss: 0.1482\n",
      "Epoch 2/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0829 - val_loss: 0.0415\n",
      "Epoch 3/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0321 - val_loss: 0.0252\n",
      "Epoch 4/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0242 - val_loss: 0.0216\n",
      "Epoch 5/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0214 - val_loss: 0.0200\n",
      "Epoch 6/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0198 - val_loss: 0.0191\n",
      "Epoch 7/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0188 - val_loss: 0.0187\n",
      "Epoch 8/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0184 - val_loss: 0.0182\n",
      "Epoch 9/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0177 - val_loss: 0.0178\n",
      "Epoch 10/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0171 - val_loss: 0.0175\n",
      "Epoch 11/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0166 - val_loss: 0.0174\n",
      "Epoch 12/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0160 - val_loss: 0.0175\n",
      "Epoch 13/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0154 - val_loss: 0.0169\n",
      "Epoch 14/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0147 - val_loss: 0.0169\n",
      "Epoch 15/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0141\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0141 - val_loss: 0.0169\n",
      "Epoch 16/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0130 - val_loss: 0.0167\n",
      "Epoch 17/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0128 - val_loss: 0.0166\n",
      "Epoch 18/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0126\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0126 - val_loss: 0.0166\n",
      "Epoch 19/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0124 - val_loss: 0.0166\n",
      "Epoch 20/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0124\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0124 - val_loss: 0.0166\n",
      "Epoch 21/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0124 - val_loss: 0.0166\n",
      "Epoch 22/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0124\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0124 - val_loss: 0.0166\n",
      "Epoch 23/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0124 - val_loss: 0.0166\n",
      "Epoch 24/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0125\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0125 - val_loss: 0.0166\n",
      "Epoch 25/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0124 - val_loss: 0.0166\n",
      "Epoch 26/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0124\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0124 - val_loss: 0.0166\n",
      "Epoch 00026: early stopping\n",
      "\n",
      "===== Repeat: 0, Fold: 5 =====\n",
      "Epoch 1/50\n",
      "147/147 [==============================] - 4s 24ms/step - loss: 0.5191 - val_loss: 0.1431\n",
      "Epoch 2/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0822 - val_loss: 0.0386\n",
      "Epoch 3/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0320 - val_loss: 0.0252\n",
      "Epoch 4/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0244 - val_loss: 0.0217\n",
      "Epoch 5/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0215 - val_loss: 0.0199\n",
      "Epoch 6/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0199 - val_loss: 0.0190\n",
      "Epoch 7/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0189 - val_loss: 0.0184\n",
      "Epoch 8/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0182 - val_loss: 0.0179\n",
      "Epoch 9/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0177 - val_loss: 0.0178\n",
      "Epoch 10/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0171 - val_loss: 0.0173\n",
      "Epoch 11/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0166 - val_loss: 0.0172\n",
      "Epoch 12/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0159 - val_loss: 0.0169\n",
      "Epoch 13/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0160 - val_loss: 0.0170\n",
      "Epoch 14/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0159\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0159 - val_loss: 0.0169\n",
      "Epoch 15/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0148 - val_loss: 0.0166\n",
      "Epoch 16/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0146 - val_loss: 0.0166\n",
      "Epoch 17/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0144 - val_loss: 0.0165\n",
      "Epoch 18/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0143 - val_loss: 0.0165\n",
      "Epoch 19/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0141\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0141 - val_loss: 0.0165\n",
      "Epoch 20/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0140 - val_loss: 0.0165\n",
      "Epoch 21/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0140 - val_loss: 0.0165\n",
      "Epoch 22/50\n",
      "147/147 [==============================] - 3s 21ms/step - loss: 0.0139 - val_loss: 0.0165\n",
      "Epoch 23/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0139 - val_loss: 0.0165\n",
      "Epoch 24/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0139Restoring model weights from the end of the best epoch.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0139 - val_loss: 0.0165\n",
      "Epoch 00024: early stopping\n",
      "\n",
      "===== Repeat: 0, Fold: 6 =====\n",
      "Epoch 1/50\n",
      "147/147 [==============================] - 4s 25ms/step - loss: 0.5215 - val_loss: 0.1621\n",
      "Epoch 2/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0832 - val_loss: 0.0400\n",
      "Epoch 3/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0323 - val_loss: 0.0247\n",
      "Epoch 4/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0242 - val_loss: 0.0211\n",
      "Epoch 5/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0214 - val_loss: 0.0199\n",
      "Epoch 6/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0201 - val_loss: 0.0193\n",
      "Epoch 7/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0189 - val_loss: 0.0183\n",
      "Epoch 8/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0182 - val_loss: 0.0179\n",
      "Epoch 9/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0175 - val_loss: 0.0176\n",
      "Epoch 10/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0169 - val_loss: 0.0174\n",
      "Epoch 11/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0165 - val_loss: 0.0171\n",
      "Epoch 12/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0158 - val_loss: 0.0170\n",
      "Epoch 13/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0153 - val_loss: 0.0168\n",
      "Epoch 14/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0146 - val_loss: 0.0168\n",
      "Epoch 15/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0140 - val_loss: 0.0167\n",
      "Epoch 16/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0131 - val_loss: 0.0167\n",
      "Epoch 17/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0124\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0124 - val_loss: 0.0169\n",
      "Epoch 18/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0111 - val_loss: 0.0166\n",
      "Epoch 19/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0107 - val_loss: 0.0165\n",
      "Epoch 20/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0105\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0105 - val_loss: 0.0166\n",
      "Epoch 21/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0103 - val_loss: 0.0166\n",
      "Epoch 22/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0102\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0102 - val_loss: 0.0166\n",
      "Epoch 23/50\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0102 - val_loss: 0.0166\n",
      "Epoch 24/50\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.0102\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.0102 - val_loss: 0.0166\n",
      "Epoch 00024: early stopping\n"
     ]
    }
   ],
   "source": [
    "N_STARTS = 1\n",
    "N_SPLITS = 7\n",
    "\n",
    "res = target_df.copy()\n",
    "submit_df.loc[:, target_df.columns] = 0\n",
    "res.loc[:, target_df.columns] = 0\n",
    "\n",
    "for seed in range(N_STARTS):\n",
    "    for n, (tr, te) in enumerate(\n",
    "        MultilabelStratifiedKFold(n_splits=N_SPLITS, random_state=seed, shuffle=True).split(target_df, target_df)\n",
    "    ):\n",
    "        print(\"\")\n",
    "        print(f\"===== Repeat: {seed}, Fold: {n} =====\")\n",
    "\n",
    "        model = create_model()\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=2, verbose=1, mode=\"min\")\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=5,\n",
    "            mode=\"min\",\n",
    "            verbose=1,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "        model.fit(\n",
    "            train.values[tr],\n",
    "            target_df.values[tr],\n",
    "            validation_data=(train.values[te], target_df.values[te]),\n",
    "            epochs=50,\n",
    "            batch_size=128,\n",
    "            callbacks=[reduce_lr_loss, early_stopping],\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        test_predict = model.predict(test.values)\n",
    "        val_predict = model.predict(train.values[te])\n",
    "\n",
    "        submit_df.loc[:, target_df.columns] += test_predict\n",
    "        res.loc[te, target_df.columns] += val_predict\n",
    "\n",
    "submit_df.loc[:, target_df.columns] /= (n + 1) * N_STARTS\n",
    "res.loc[:, target_df.columns] /= N_STARTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.405709,
     "end_time": "2020-10-12T13:46:14.200654",
     "exception": false,
     "start_time": "2020-10-12T13:46:12.794945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV log_loss:  0.016486476250573326\n"
     ]
    }
   ],
   "source": [
    "target_cols = target_df.columns.values.tolist()\n",
    "\n",
    "y_true = target_df.values\n",
    "y_pred = res.values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target_df.shape[1]\n",
    "\n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.371054,
     "end_time": "2020-10-12T13:46:20.315294",
     "exception": false,
     "start_time": "2020-10-12T13:46:18.944240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 補正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "papermill": {
     "duration": 1.495036,
     "end_time": "2020-10-12T13:46:23.181347",
     "exception": false,
     "start_time": "2020-10-12T13:46:21.686311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit_df.loc[test_df[\"cp_type\"] == \"ctl_vehicle\", target_df.columns] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.416097,
     "end_time": "2020-10-12T13:46:26.067249",
     "exception": false,
     "start_time": "2020-10-12T13:46:24.651152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## t検定\n",
    "\n",
    "https://logics-of-blue.com/t-test/\n",
    "\n",
    "学習データとテストデータに有意な差があるか。\n",
    "\n",
    "p値が0.05を下回ると、有意な差があると言える。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "papermill": {
     "duration": 1.525553,
     "end_time": "2020-10-12T13:46:29.409934",
     "exception": false,
     "start_time": "2020-10-12T13:46:27.884381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ng-0 0.032543038217202086\\ng-1 0.019393109454476025\\ng-3 0.046721309178349664\\ng-22 6.587624616736739e-05\\ng-37 0.010911040292799237\\ng-48 0.030801708455925808\\ng-50 0.03531252898048682\\ng-52 0.028092336066332588\\ng-60 0.003985876456805001\\ng-72 0.03734120596180097\\ng-98 0.002596791303651995\\ng-100 0.008351686077946879\\ng-101 3.938347554051903e-05\\ng-105 0.0429096270660344\\ng-110 0.007002341005554218\\ng-119 0.0026784043039369777\\ng-120 0.04036243989718826\\ng-121 0.018277391157227284\\ng-134 0.024952122240263698\\ng-135 5.2762961694835665e-05\\ng-136 0.014781889784802717\\ng-139 0.032409758453034926\\ng-140 0.029888796018104586\\ng-145 0.04082269273028868\\ng-150 0.02741917010243746\\ng-152 0.029540491011946193\\ng-158 0.006359327029617653\\ng-165 0.015874095698013277\\ng-166 0.044642334463927895\\ng-168 0.04202225455856443\\ng-169 0.03499856643946845\\ng-174 0.018682872175197398\\ng-186 0.008947447524514207\\ng-193 0.027903058327269626\\ng-206 0.031924304036266206\\ng-211 0.0011897547708701196\\ng-215 0.023199568324813253\\ng-224 0.03730567820677584\\ng-235 0.03499601003997585\\ng-237 0.023174582508069416\\ng-240 0.04288696473409333\\ng-248 0.031831549496090336\\ng-257 0.0006119796151287299\\ng-267 0.04980450859091937\\ng-273 0.006297806993275883\\ng-284 0.04655082243178294\\ng-298 0.015033343937168856\\ng-322 0.01744507921578457\\ng-330 0.011666755951211137\\ng-332 0.0021244787311601766\\ng-343 0.03324481374310012\\ng-346 0.01355309132510231\\ng-354 0.0016517824313901726\\ng-360 0.009620025505946095\\ng-362 0.0324817698501945\\ng-365 0.012689368176776432\\ng-367 0.023754819659996564\\ng-378 0.04656608215292139\\ng-380 0.0017078489673201087\\ng-386 0.009849666373976277\\ng-390 0.03601825397540972\\ng-392 0.00014622041338638775\\ng-396 0.011558383816503901\\ng-400 0.00234417466908514\\ng-406 0.03060881366633618\\ng-410 0.003942306812742894\\ng-413 0.008357431453948386\\ng-418 0.002296074513896323\\ng-433 0.0028952219485856396\\ng-437 0.03037716243553023\\ng-439 0.012787195107598747\\ng-440 0.0009790120173915739\\ng-445 0.03942676587611877\\ng-451 0.03521936791875377\\ng-461 0.026308390115405485\\ng-473 0.017296065430318728\\ng-485 0.04687073341670321\\ng-486 0.013919900233126406\\ng-489 0.0010059622726513917\\ng-491 0.0029324527830305452\\ng-492 0.00048123771140560774\\ng-498 0.012101192831340432\\ng-503 0.0015522760171656617\\ng-518 0.0160779468086828\\ng-526 0.006223261683449866\\ng-528 0.01243903801888239\\ng-554 0.005526846061505236\\ng-568 0.03981859366092907\\ng-574 0.0009977307203055188\\ng-577 0.017459489508530397\\ng-586 0.024064208550529632\\ng-600 0.002201740921036897\\ng-619 0.016276470302135262\\ng-628 0.0013193596507959106\\ng-655 0.01974989210697146\\ng-656 0.0001817267948382975\\ng-664 0.005051744552255386\\ng-672 0.03407566901525629\\ng-675 0.011914459074866945\\ng-683 0.03929606627209026\\ng-700 0.0037311306453646255\\ng-702 0.0010125897831886554\\ng-708 0.012301558853143693\\ng-709 0.0398328066319844\\ng-719 0.04875188668684708\\ng-723 0.029252768384844995\\ng-725 0.026614623630592568\\ng-743 0.040185681769053454\\ng-744 0.036928324629596374\\ng-745 0.018719801905843145\\ng-757 0.04915580418862598\\ng-761 0.005939948820549039\\ng-767 0.011931339152979633\\ng-770 0.010495387864363378\\ng-771 0.02066501430630159\\nc-9 0.00676768179495497\\nc-10 0.027463106489330136\\nc-18 0.027953170579559945\\nc-22 0.012676157706292834\\nc-49 0.04890393979396845\\nc-70 0.03525132105881509\\nc-97 0.03324866397008827\\nc-98 0.03673769845220116\\n\\n合計: 123\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "g-0 0.032543038217202086\n",
    "g-1 0.019393109454476025\n",
    "g-3 0.046721309178349664\n",
    "g-22 6.587624616736739e-05\n",
    "g-37 0.010911040292799237\n",
    "g-48 0.030801708455925808\n",
    "g-50 0.03531252898048682\n",
    "g-52 0.028092336066332588\n",
    "g-60 0.003985876456805001\n",
    "g-72 0.03734120596180097\n",
    "g-98 0.002596791303651995\n",
    "g-100 0.008351686077946879\n",
    "g-101 3.938347554051903e-05\n",
    "g-105 0.0429096270660344\n",
    "g-110 0.007002341005554218\n",
    "g-119 0.0026784043039369777\n",
    "g-120 0.04036243989718826\n",
    "g-121 0.018277391157227284\n",
    "g-134 0.024952122240263698\n",
    "g-135 5.2762961694835665e-05\n",
    "g-136 0.014781889784802717\n",
    "g-139 0.032409758453034926\n",
    "g-140 0.029888796018104586\n",
    "g-145 0.04082269273028868\n",
    "g-150 0.02741917010243746\n",
    "g-152 0.029540491011946193\n",
    "g-158 0.006359327029617653\n",
    "g-165 0.015874095698013277\n",
    "g-166 0.044642334463927895\n",
    "g-168 0.04202225455856443\n",
    "g-169 0.03499856643946845\n",
    "g-174 0.018682872175197398\n",
    "g-186 0.008947447524514207\n",
    "g-193 0.027903058327269626\n",
    "g-206 0.031924304036266206\n",
    "g-211 0.0011897547708701196\n",
    "g-215 0.023199568324813253\n",
    "g-224 0.03730567820677584\n",
    "g-235 0.03499601003997585\n",
    "g-237 0.023174582508069416\n",
    "g-240 0.04288696473409333\n",
    "g-248 0.031831549496090336\n",
    "g-257 0.0006119796151287299\n",
    "g-267 0.04980450859091937\n",
    "g-273 0.006297806993275883\n",
    "g-284 0.04655082243178294\n",
    "g-298 0.015033343937168856\n",
    "g-322 0.01744507921578457\n",
    "g-330 0.011666755951211137\n",
    "g-332 0.0021244787311601766\n",
    "g-343 0.03324481374310012\n",
    "g-346 0.01355309132510231\n",
    "g-354 0.0016517824313901726\n",
    "g-360 0.009620025505946095\n",
    "g-362 0.0324817698501945\n",
    "g-365 0.012689368176776432\n",
    "g-367 0.023754819659996564\n",
    "g-378 0.04656608215292139\n",
    "g-380 0.0017078489673201087\n",
    "g-386 0.009849666373976277\n",
    "g-390 0.03601825397540972\n",
    "g-392 0.00014622041338638775\n",
    "g-396 0.011558383816503901\n",
    "g-400 0.00234417466908514\n",
    "g-406 0.03060881366633618\n",
    "g-410 0.003942306812742894\n",
    "g-413 0.008357431453948386\n",
    "g-418 0.002296074513896323\n",
    "g-433 0.0028952219485856396\n",
    "g-437 0.03037716243553023\n",
    "g-439 0.012787195107598747\n",
    "g-440 0.0009790120173915739\n",
    "g-445 0.03942676587611877\n",
    "g-451 0.03521936791875377\n",
    "g-461 0.026308390115405485\n",
    "g-473 0.017296065430318728\n",
    "g-485 0.04687073341670321\n",
    "g-486 0.013919900233126406\n",
    "g-489 0.0010059622726513917\n",
    "g-491 0.0029324527830305452\n",
    "g-492 0.00048123771140560774\n",
    "g-498 0.012101192831340432\n",
    "g-503 0.0015522760171656617\n",
    "g-518 0.0160779468086828\n",
    "g-526 0.006223261683449866\n",
    "g-528 0.01243903801888239\n",
    "g-554 0.005526846061505236\n",
    "g-568 0.03981859366092907\n",
    "g-574 0.0009977307203055188\n",
    "g-577 0.017459489508530397\n",
    "g-586 0.024064208550529632\n",
    "g-600 0.002201740921036897\n",
    "g-619 0.016276470302135262\n",
    "g-628 0.0013193596507959106\n",
    "g-655 0.01974989210697146\n",
    "g-656 0.0001817267948382975\n",
    "g-664 0.005051744552255386\n",
    "g-672 0.03407566901525629\n",
    "g-675 0.011914459074866945\n",
    "g-683 0.03929606627209026\n",
    "g-700 0.0037311306453646255\n",
    "g-702 0.0010125897831886554\n",
    "g-708 0.012301558853143693\n",
    "g-709 0.0398328066319844\n",
    "g-719 0.04875188668684708\n",
    "g-723 0.029252768384844995\n",
    "g-725 0.026614623630592568\n",
    "g-743 0.040185681769053454\n",
    "g-744 0.036928324629596374\n",
    "g-745 0.018719801905843145\n",
    "g-757 0.04915580418862598\n",
    "g-761 0.005939948820549039\n",
    "g-767 0.011931339152979633\n",
    "g-770 0.010495387864363378\n",
    "g-771 0.02066501430630159\n",
    "c-9 0.00676768179495497\n",
    "c-10 0.027463106489330136\n",
    "c-18 0.027953170579559945\n",
    "c-22 0.012676157706292834\n",
    "c-49 0.04890393979396845\n",
    "c-70 0.03525132105881509\n",
    "c-97 0.03324866397008827\n",
    "c-98 0.03673769845220116\n",
    "\n",
    "合計: 123\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "papermill": {
     "duration": 3.127281,
     "end_time": "2020-10-12T13:46:34.040607",
     "exception": false,
     "start_time": "2020-10-12T13:46:30.913326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "duration": 1196.786661,
   "end_time": "2020-10-12T13:46:37.344135",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-12T13:26:40.557474",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
