{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021802,
     "end_time": "2020-10-12T13:26:44.600780",
     "exception": false,
     "start_time": "2020-10-12T13:26:44.578978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MoA\n",
    "\n",
    "Fork from [here](https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.963258,
     "end_time": "2020-10-12T13:26:45.585220",
     "exception": false,
     "start_time": "2020-10-12T13:26:44.621962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 5.615106,
     "end_time": "2020-10-12T13:26:51.222425",
     "exception": false,
     "start_time": "2020-10-12T13:26:45.607319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022202,
     "end_time": "2020-10-12T13:26:51.266905",
     "exception": false,
     "start_time": "2020-10-12T13:26:51.244703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.031275,
     "end_time": "2020-10-12T13:26:51.320334",
     "exception": false,
     "start_time": "2020-10-12T13:26:51.289059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_seed(seed=2020):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "random_seed = 22\n",
    "fix_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.0313,
     "end_time": "2020-10-12T13:26:51.417651",
     "exception": false,
     "start_time": "2020-10-12T13:26:51.386351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [Fast Numpy Log Loss] https://www.kaggle.com/gogo827jz/optimise-blending-weights-4-5x-faster-log-loss\n",
    "def metric(y_true, y_pred):\n",
    "    loss = 0\n",
    "    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        loss += -np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n",
    "    return loss / y_pred.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021577,
     "end_time": "2020-10-12T13:26:51.461017",
     "exception": false,
     "start_time": "2020-10-12T13:26:51.439440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## データロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 6.163534,
     "end_time": "2020-10-12T13:26:57.646712",
     "exception": false,
     "start_time": "2020-10-12T13:26:51.483178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n",
    "# test_df = pd.read_csv(\"../input/lish-moa/test_features.csv\")\n",
    "# target_df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\n",
    "# non_target_df = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\n",
    "# submit_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.09993,
     "end_time": "2020-10-12T13:26:57.769419",
     "exception": false,
     "start_time": "2020-10-12T13:26:57.669489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train = train_df.copy()\n",
    "# test = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022232,
     "end_time": "2020-10-12T13:26:57.814942",
     "exception": false,
     "start_time": "2020-10-12T13:26:57.792710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.04466,
     "end_time": "2020-10-12T13:26:57.882000",
     "exception": false,
     "start_time": "2020-10-12T13:26:57.837340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train.loc[:, \"cp_dose\"] = train.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})\n",
    "# test.loc[:, \"cp_dose\"] = test.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022031,
     "end_time": "2020-10-12T13:26:57.926657",
     "exception": false,
     "start_time": "2020-10-12T13:26:57.904626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### cp_type が ctrl_vehicle なものは MoA を持たない\n",
    "\n",
    "ので、学習から除外する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.063162,
     "end_time": "2020-10-12T13:26:58.012052",
     "exception": false,
     "start_time": "2020-10-12T13:26:57.948890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# target_df = target_df.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "# train = train.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.046507,
     "end_time": "2020-10-12T13:26:58.081418",
     "exception": false,
     "start_time": "2020-10-12T13:26:58.034911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train = train.drop(\"cp_type\", axis=1)\n",
    "# test = test.drop(\"cp_type\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.032336,
     "end_time": "2020-10-12T13:26:58.136751",
     "exception": false,
     "start_time": "2020-10-12T13:26:58.104415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del train[\"sig_id\"]\n",
    "# del target_df[\"sig_id\"]\n",
    "# del test[\"sig_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023429,
     "end_time": "2020-10-12T13:26:58.183729",
     "exception": false,
     "start_time": "2020-10-12T13:26:58.160300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Rank Gauss\n",
    "\n",
    "https://www.kaggle.com/nayuts/moa-pytorch-nn-pca-rankgauss\n",
    "\n",
    "連続値を特定の範囲の閉域に押し込めて、分布の偏りを解消する方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 4.653824,
     "end_time": "2020-10-12T13:27:02.861515",
     "exception": false,
     "start_time": "2020-10-12T13:26:58.207691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# g_cols = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "# c_cols = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "#\n",
    "# for col in g_cols + c_cols:\n",
    "#    transformer = QuantileTransformer(n_quantiles=100, random_state=random_seed, output_distribution=\"normal\")\n",
    "#\n",
    "#    vec_len = len(train[col].values)\n",
    "#    vec_len_test = len(test[col].values)\n",
    "#\n",
    "#    raw_vec = train[col].values.reshape(vec_len, 1)\n",
    "#    transformer.fit(raw_vec)\n",
    "#\n",
    "#    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "#    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024422,
     "end_time": "2020-10-12T13:27:03.001749",
     "exception": false,
     "start_time": "2020-10-12T13:27:02.977327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PCA features (+ Existing features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 0.617329,
     "end_time": "2020-10-12T13:27:03.644535",
     "exception": false,
     "start_time": "2020-10-12T13:27:03.027206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## g-\n",
    "# n_comp = 600\n",
    "#\n",
    "# data = pd.concat([pd.DataFrame(train[g_cols]), pd.DataFrame(test[g_cols])])\n",
    "# data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[g_cols])\n",
    "# train2 = data2[: train.shape[0]]\n",
    "# test2 = data2[-test.shape[0] :]\n",
    "#\n",
    "# train2 = pd.DataFrame(train2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
    "# test2 = pd.DataFrame(test2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
    "#\n",
    "# train.drop(g_cols, axis=1, inplace=True)\n",
    "# test.drop(g_cols, axis=1, inplace=True)\n",
    "#\n",
    "# train = pd.concat((train, train2), axis=1)\n",
    "# test = pd.concat((test, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.119545,
     "end_time": "2020-10-12T13:27:03.788341",
     "exception": false,
     "start_time": "2020-10-12T13:27:03.668796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## c-\n",
    "# n_comp = 50\n",
    "#\n",
    "# data = pd.concat([pd.DataFrame(train[c_cols]), pd.DataFrame(test[c_cols])])\n",
    "# data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[c_cols])\n",
    "# train2 = data2[: train.shape[0]]\n",
    "# test2 = data2[-test.shape[0] :]\n",
    "#\n",
    "# train2 = pd.DataFrame(train2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
    "# test2 = pd.DataFrame(test2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
    "#\n",
    "# train.drop(c_cols, axis=1, inplace=True)\n",
    "# test.drop(c_cols, axis=1, inplace=True)\n",
    "#\n",
    "# train = pd.concat((train, train2), axis=1)\n",
    "# test = pd.concat((test, test2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026575,
     "end_time": "2020-10-12T13:27:03.927532",
     "exception": false,
     "start_time": "2020-10-12T13:27:03.900957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### feature Selection using Variance Encoding\n",
    "\n",
    "分散がしきい値以下の特徴量を捨てます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.324466,
     "end_time": "2020-10-12T13:27:04.277321",
     "exception": false,
     "start_time": "2020-10-12T13:27:03.952855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# var_thresh = VarianceThreshold(threshold=0.5)\n",
    "#\n",
    "# data = train.append(test)\n",
    "# data_transformed = var_thresh.fit_transform(data.iloc[:, 2:])\n",
    "#\n",
    "# train_transformed = data_transformed[: train.shape[0]]\n",
    "# test_transformed = data_transformed[-test.shape[0] :]\n",
    "#\n",
    "#\n",
    "# train = pd.DataFrame(train[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
    "# train = pd.concat([train, pd.DataFrame(train_transformed)], axis=1)\n",
    "#\n",
    "#\n",
    "# test = pd.DataFrame(test[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
    "# test = pd.concat([test, pd.DataFrame(test_transformed)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02676,
     "end_time": "2020-10-12T13:27:04.332310",
     "exception": false,
     "start_time": "2020-10-12T13:27:04.305550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.040414,
     "end_time": "2020-10-12T13:27:04.400117",
     "exception": false,
     "start_time": "2020-10-12T13:27:04.359703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            L.Input(len(train.columns)),\n",
    "            L.BatchNormalization(),\n",
    "            tfa.layers.WeightNormalization(L.Dense(2048, activation=\"relu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.4),\n",
    "            tfa.layers.WeightNormalization(L.Dense(2048, activation=\"relu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.4),\n",
    "            tfa.layers.WeightNormalization(L.Dense(206, activation=\"sigmoid\")),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n",
    "        loss=\"binary_crossentropy\",\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet for Tensorflow for MultiLabel\n",
    "\n",
    "https://www.kaggle.com/gogo827jz/moa-stacked-tabnet-baseline-tensorflow-2-0#Model-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_keras_custom_object(cls):\n",
    "    tf.keras.utils.get_custom_objects()[cls.__name__] = cls\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glu(x, n_units=None):\n",
    "    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n",
    "    if n_units is None:\n",
    "        n_units = tf.shape(x)[-1] // 2\n",
    "\n",
    "    return x[..., :n_units] * tf.nn.sigmoid(x[..., n_units:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/activations/sparsemax.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@register_keras_custom_object\n",
    "@tf.function\n",
    "def sparsemax(logits, axis):\n",
    "    \"\"\"Sparsemax activation function [1].\n",
    "    For each batch `i` and class `j` we have\n",
    "      $$sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0)$$\n",
    "    [1]: https://arxiv.org/abs/1602.02068\n",
    "    Args:\n",
    "        logits: Input tensor.\n",
    "        axis: Integer, axis along which the sparsemax operation is applied.\n",
    "    Returns:\n",
    "        Tensor, output of sparsemax transformation. Has the same type and\n",
    "        shape as `logits`.\n",
    "    Raises:\n",
    "        ValueError: In case `dim(logits) == 1`.\n",
    "    \"\"\"\n",
    "    logits = tf.convert_to_tensor(logits, name=\"logits\")\n",
    "\n",
    "    # We need its original shape for shape inference.\n",
    "    shape = logits.get_shape()\n",
    "    rank = shape.rank\n",
    "    is_last_axis = (axis == -1) or (axis == rank - 1)\n",
    "\n",
    "    if is_last_axis:\n",
    "        output = _compute_2d_sparsemax(logits)\n",
    "        output.set_shape(shape)\n",
    "        return output\n",
    "\n",
    "    # If dim is not the last dimension, we have to do a transpose so that we can\n",
    "    # still perform softmax on its last dimension.\n",
    "\n",
    "    # Swap logits' dimension of dim and its last dimension.\n",
    "    rank_op = tf.rank(logits)\n",
    "    axis_norm = axis % rank\n",
    "    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n",
    "\n",
    "    # Do the actual softmax on its last dimension.\n",
    "    output = _compute_2d_sparsemax(logits)\n",
    "    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n",
    "\n",
    "    # Make shape inference work since transpose may erase its static shape.\n",
    "    output.set_shape(shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _swap_axis(logits, dim_index, last_index, **kwargs):\n",
    "    return tf.transpose(\n",
    "        logits,\n",
    "        tf.concat(\n",
    "            [\n",
    "                tf.range(dim_index),\n",
    "                [last_index],\n",
    "                tf.range(dim_index + 1, last_index),\n",
    "                [dim_index],\n",
    "            ],\n",
    "            0,\n",
    "        ),\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_2d_sparsemax(logits):\n",
    "    \"\"\"Performs the sparsemax operation when axis=-1.\"\"\"\n",
    "    shape_op = tf.shape(logits)\n",
    "    obs = tf.math.reduce_prod(shape_op[:-1])\n",
    "    dims = shape_op[-1]\n",
    "\n",
    "    # In the paper, they call the logits z.\n",
    "    # The mean(logits) can be substracted from logits to make the algorithm\n",
    "    # more numerically stable. the instability in this algorithm comes mostly\n",
    "    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n",
    "    # to zero. However, in practise the numerical instability issues are very\n",
    "    # minor and substacting the mean causes extra issues with inf and nan\n",
    "    # input.\n",
    "    # Reshape to [obs, dims] as it is almost free and means the remanining\n",
    "    # code doesn't need to worry about the rank.\n",
    "    z = tf.reshape(logits, [obs, dims])\n",
    "\n",
    "    # sort z\n",
    "    z_sorted, _ = tf.nn.top_k(z, k=dims)\n",
    "\n",
    "    # calculate k(z)\n",
    "    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n",
    "    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n",
    "    z_check = 1 + k * z_sorted > z_cumsum\n",
    "    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n",
    "    # (index + 1) of the last `1` is the same as just summing the number of 1.\n",
    "    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n",
    "\n",
    "    # calculate tau(z)\n",
    "    # If there are inf values or all values are -inf, the k_z will be zero,\n",
    "    # this is mathematically invalid and will also cause the gather_nd to fail.\n",
    "    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n",
    "    # fixed later (see p_safe) by returning p = nan. This results in the same\n",
    "    # behavior as softmax.\n",
    "    k_z_safe = tf.math.maximum(k_z, 1)\n",
    "    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n",
    "    tau_sum = tf.gather_nd(z_cumsum, indices)\n",
    "    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n",
    "\n",
    "    # calculate p\n",
    "    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n",
    "    # If k_z = 0 or if z = nan, then the input is invalid\n",
    "    p_safe = tf.where(\n",
    "        tf.expand_dims(\n",
    "            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n",
    "        p,\n",
    "    )\n",
    "\n",
    "    # Reshape back to original size\n",
    "    p_safe = tf.reshape(p_safe, shape_op)\n",
    "    return p_safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/normalizations.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@register_keras_custom_object\n",
    "class GroupNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"Group normalization layer.\n",
    "    Group Normalization divides the channels into groups and computes\n",
    "    within each group the mean and variance for normalization.\n",
    "    Empirically, its accuracy is more stable than batch norm in a wide\n",
    "    range of small batch sizes, if learning rate is adjusted linearly\n",
    "    with batch sizes.\n",
    "    Relation to Layer Normalization:\n",
    "    If the number of groups is set to 1, then this operation becomes identical\n",
    "    to Layer Normalization.\n",
    "    Relation to Instance Normalization:\n",
    "    If the number of groups is set to the\n",
    "    input dimension (number of groups is equal\n",
    "    to number of channels), then this operation becomes\n",
    "    identical to Instance Normalization.\n",
    "    Arguments\n",
    "        groups: Integer, the number of groups for Group Normalization.\n",
    "            Can be in the range [1, N] where N is the input dimension.\n",
    "            The input dimension must be divisible by the number of groups.\n",
    "        axis: Integer, the axis that should be normalized.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    Output shape\n",
    "        Same shape as input.\n",
    "    References\n",
    "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        groups: int = 2,\n",
    "        axis: int = -1,\n",
    "        epsilon: float = 1e-3,\n",
    "        center: bool = True,\n",
    "        scale: bool = True,\n",
    "        beta_initializer=\"zeros\",\n",
    "        gamma_initializer=\"ones\",\n",
    "        beta_regularizer=None,\n",
    "        gamma_regularizer=None,\n",
    "        beta_constraint=None,\n",
    "        gamma_constraint=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.groups = groups\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n",
    "        self._check_axis()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self._check_if_input_shape_is_none(input_shape)\n",
    "        self._set_number_of_groups_for_instance_norm(input_shape)\n",
    "        self._check_size_of_dimensions(input_shape)\n",
    "        self._create_input_spec(input_shape)\n",
    "\n",
    "        self._add_gamma_weight(input_shape)\n",
    "        self._add_beta_weight(input_shape)\n",
    "        self.built = True\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Training=none is just for compat with batchnorm signature call\n",
    "        input_shape = tf.keras.backend.int_shape(inputs)\n",
    "        tensor_input_shape = tf.shape(inputs)\n",
    "\n",
    "        reshaped_inputs, group_shape = self._reshape_into_groups(inputs, input_shape, tensor_input_shape)\n",
    "\n",
    "        normalized_inputs = self._apply_normalization(reshaped_inputs, input_shape)\n",
    "\n",
    "        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"groups\": self.groups,\n",
    "            \"axis\": self.axis,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"center\": self.center,\n",
    "            \"scale\": self.scale,\n",
    "            \"beta_initializer\": tf.keras.initializers.serialize(self.beta_initializer),\n",
    "            \"gamma_initializer\": tf.keras.initializers.serialize(self.gamma_initializer),\n",
    "            \"beta_regularizer\": tf.keras.regularizers.serialize(self.beta_regularizer),\n",
    "            \"gamma_regularizer\": tf.keras.regularizers.serialize(self.gamma_regularizer),\n",
    "            \"beta_constraint\": tf.keras.constraints.serialize(self.beta_constraint),\n",
    "            \"gamma_constraint\": tf.keras.constraints.serialize(self.gamma_constraint),\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n",
    "\n",
    "        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n",
    "        group_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        group_shape.insert(self.axis, self.groups)\n",
    "        group_shape = tf.stack(group_shape)\n",
    "        reshaped_inputs = tf.reshape(inputs, group_shape)\n",
    "        return reshaped_inputs, group_shape\n",
    "\n",
    "    def _apply_normalization(self, reshaped_inputs, input_shape):\n",
    "\n",
    "        group_shape = tf.keras.backend.int_shape(reshaped_inputs)\n",
    "        group_reduction_axes = list(range(1, len(group_shape)))\n",
    "        axis = -2 if self.axis == -1 else self.axis - 1\n",
    "        group_reduction_axes.pop(axis)\n",
    "\n",
    "        mean, variance = tf.nn.moments(reshaped_inputs, group_reduction_axes, keepdims=True)\n",
    "\n",
    "        gamma, beta = self._get_reshaped_weights(input_shape)\n",
    "        normalized_inputs = tf.nn.batch_normalization(\n",
    "            reshaped_inputs,\n",
    "            mean=mean,\n",
    "            variance=variance,\n",
    "            scale=gamma,\n",
    "            offset=beta,\n",
    "            variance_epsilon=self.epsilon,\n",
    "        )\n",
    "        return normalized_inputs\n",
    "\n",
    "    def _get_reshaped_weights(self, input_shape):\n",
    "        broadcast_shape = self._create_broadcast_shape(input_shape)\n",
    "        gamma = None\n",
    "        beta = None\n",
    "        if self.scale:\n",
    "            gamma = tf.reshape(self.gamma, broadcast_shape)\n",
    "\n",
    "        if self.center:\n",
    "            beta = tf.reshape(self.beta, broadcast_shape)\n",
    "        return gamma, beta\n",
    "\n",
    "    def _check_if_input_shape_is_none(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim is None:\n",
    "            raise ValueError(\n",
    "                \"Axis \" + str(self.axis) + \" of \"\n",
    "                \"input tensor should have a defined dimension \"\n",
    "                \"but the layer received an input with shape \" + str(input_shape) + \".\"\n",
    "            )\n",
    "\n",
    "    def _set_number_of_groups_for_instance_norm(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "\n",
    "        if self.groups == -1:\n",
    "            self.groups = dim\n",
    "\n",
    "    def _check_size_of_dimensions(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim < self.groups:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") cannot be \"\n",
    "                \"more than the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") must be a \"\n",
    "                \"multiple of the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "    def _check_axis(self):\n",
    "\n",
    "        if self.axis == 0:\n",
    "            raise ValueError(\n",
    "                \"You are trying to normalize your batch axis. Do you want to \"\n",
    "                \"use tf.layer.batch_normalization instead\"\n",
    "            )\n",
    "\n",
    "    def _create_input_spec(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=len(input_shape), axes={self.axis: dim})\n",
    "\n",
    "    def _add_gamma_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"gamma\",\n",
    "                initializer=self.gamma_initializer,\n",
    "                regularizer=self.gamma_regularizer,\n",
    "                constraint=self.gamma_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "    def _add_beta_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"beta\",\n",
    "                initializer=self.beta_initializer,\n",
    "                regularizer=self.beta_regularizer,\n",
    "                constraint=self.beta_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.beta = None\n",
    "\n",
    "    def _create_broadcast_shape(self, input_shape):\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        broadcast_shape.insert(self.axis, self.groups)\n",
    "        return broadcast_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformBlock(tf.keras.Model):\n",
    "    def __init__(self, features, norm_type, momentum=0.9, virtual_batch_size=None, groups=2, block_name=\"\", **kwargs):\n",
    "        super(TransformBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.features = features\n",
    "        self.norm_type = norm_type\n",
    "        self.momentum = momentum\n",
    "        self.groups = groups\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "\n",
    "        self.transform = tf.keras.layers.Dense(self.features, use_bias=False, name=f\"transformblock_dense_{block_name}\")\n",
    "\n",
    "        if norm_type == \"batch\":\n",
    "            self.bn = tf.keras.layers.BatchNormalization(\n",
    "                axis=-1,\n",
    "                momentum=momentum,\n",
    "                virtual_batch_size=virtual_batch_size,\n",
    "                name=f\"transformblock_bn_{block_name}\",\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.bn = GroupNormalization(axis=-1, groups=self.groups, name=f\"transformblock_gn_{block_name}\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.transform(inputs)\n",
    "        x = self.bn(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNet(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNet, self).__init__(**kwargs)\n",
    "\n",
    "        # Input checks\n",
    "        if feature_columns is not None:\n",
    "            if type(feature_columns) not in (list, tuple):\n",
    "                raise ValueError(\"`feature_columns` must be a list or a tuple.\")\n",
    "\n",
    "            if len(feature_columns) == 0:\n",
    "                raise ValueError(\"`feature_columns` must be contain at least 1 tf.feature_column !\")\n",
    "\n",
    "            if num_features is None:\n",
    "                num_features = len(feature_columns)\n",
    "            else:\n",
    "                num_features = int(num_features)\n",
    "\n",
    "        else:\n",
    "            if num_features is None:\n",
    "                raise ValueError(\"If `feature_columns` is None, then `num_features` cannot be None.\")\n",
    "\n",
    "        if num_decision_steps < 1:\n",
    "            raise ValueError(\"Num decision steps must be greater than 0.\")\n",
    "\n",
    "        if feature_dim < output_dim:\n",
    "            raise ValueError(\"To compute `features_for_coef`, feature_dim must be larger than output dim\")\n",
    "\n",
    "        feature_dim = int(feature_dim)\n",
    "        output_dim = int(output_dim)\n",
    "        num_decision_steps = int(num_decision_steps)\n",
    "        relaxation_factor = float(relaxation_factor)\n",
    "        sparsity_coefficient = float(sparsity_coefficient)\n",
    "        batch_momentum = float(batch_momentum)\n",
    "        num_groups = max(1, int(num_groups))\n",
    "        epsilon = float(epsilon)\n",
    "\n",
    "        if relaxation_factor < 0.0:\n",
    "            raise ValueError(\"`relaxation_factor` cannot be negative !\")\n",
    "\n",
    "        if sparsity_coefficient < 0.0:\n",
    "            raise ValueError(\"`sparsity_coefficient` cannot be negative !\")\n",
    "\n",
    "        if virtual_batch_size is not None:\n",
    "            virtual_batch_size = int(virtual_batch_size)\n",
    "\n",
    "        if norm_type not in [\"batch\", \"group\"]:\n",
    "            raise ValueError(\"`norm_type` must be either `batch` or `group`\")\n",
    "\n",
    "        self.feature_columns = feature_columns\n",
    "        self.num_features = num_features\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.num_decision_steps = num_decision_steps\n",
    "        self.relaxation_factor = relaxation_factor\n",
    "        self.sparsity_coefficient = sparsity_coefficient\n",
    "        self.norm_type = norm_type\n",
    "        self.batch_momentum = batch_momentum\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.num_groups = num_groups\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # if num_decision_steps > 1:\n",
    "        # features_for_coeff = feature_dim - output_dim\n",
    "        # print(f\"[TabNet]: {features_for_coeff} features will be used for decision steps.\")\n",
    "\n",
    "        if self.feature_columns is not None:\n",
    "            self.input_features = tf.keras.layers.DenseFeatures(feature_columns, trainable=True)\n",
    "\n",
    "            if self.norm_type == \"batch\":\n",
    "                self.input_bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=batch_momentum, name=\"input_bn\")\n",
    "            else:\n",
    "                self.input_bn = GroupNormalization(axis=-1, groups=self.num_groups, name=\"input_gn\")\n",
    "\n",
    "        else:\n",
    "            self.input_features = None\n",
    "            self.input_bn = None\n",
    "\n",
    "        self.transform_f1 = TransformBlock(\n",
    "            2 * self.feature_dim,\n",
    "            self.norm_type,\n",
    "            self.batch_momentum,\n",
    "            self.virtual_batch_size,\n",
    "            self.num_groups,\n",
    "            block_name=\"f1\",\n",
    "        )\n",
    "\n",
    "        self.transform_f2 = TransformBlock(\n",
    "            2 * self.feature_dim,\n",
    "            self.norm_type,\n",
    "            self.batch_momentum,\n",
    "            self.virtual_batch_size,\n",
    "            self.num_groups,\n",
    "            block_name=\"f2\",\n",
    "        )\n",
    "\n",
    "        self.transform_f3_list = [\n",
    "            TransformBlock(\n",
    "                2 * self.feature_dim,\n",
    "                self.norm_type,\n",
    "                self.batch_momentum,\n",
    "                self.virtual_batch_size,\n",
    "                self.num_groups,\n",
    "                block_name=f\"f3_{i}\",\n",
    "            )\n",
    "            for i in range(self.num_decision_steps)\n",
    "        ]\n",
    "\n",
    "        self.transform_f4_list = [\n",
    "            TransformBlock(\n",
    "                2 * self.feature_dim,\n",
    "                self.norm_type,\n",
    "                self.batch_momentum,\n",
    "                self.virtual_batch_size,\n",
    "                self.num_groups,\n",
    "                block_name=f\"f4_{i}\",\n",
    "            )\n",
    "            for i in range(self.num_decision_steps)\n",
    "        ]\n",
    "\n",
    "        self.transform_coef_list = [\n",
    "            TransformBlock(\n",
    "                self.num_features,\n",
    "                self.norm_type,\n",
    "                self.batch_momentum,\n",
    "                self.virtual_batch_size,\n",
    "                self.num_groups,\n",
    "                block_name=f\"coef_{i}\",\n",
    "            )\n",
    "            for i in range(self.num_decision_steps - 1)\n",
    "        ]\n",
    "\n",
    "        self._step_feature_selection_masks = None\n",
    "        self._step_aggregate_feature_selection_mask = None\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if self.input_features is not None:\n",
    "            features = self.input_features(inputs)\n",
    "            features = self.input_bn(features, training=training)\n",
    "\n",
    "        else:\n",
    "            features = inputs\n",
    "\n",
    "        batch_size = tf.shape(features)[0]\n",
    "        self._step_feature_selection_masks = []\n",
    "        self._step_aggregate_feature_selection_mask = None\n",
    "\n",
    "        # Initializes decision-step dependent variables.\n",
    "        output_aggregated = tf.zeros([batch_size, self.output_dim])\n",
    "        masked_features = features\n",
    "        mask_values = tf.zeros([batch_size, self.num_features])\n",
    "        aggregated_mask_values = tf.zeros([batch_size, self.num_features])\n",
    "        complementary_aggregated_mask_values = tf.ones([batch_size, self.num_features])\n",
    "\n",
    "        total_entropy = 0.0\n",
    "        entropy_loss = 0.0\n",
    "\n",
    "        for ni in range(self.num_decision_steps):\n",
    "            # Feature transformer with two shared and two decision step dependent\n",
    "            # blocks is used below.=\n",
    "            transform_f1 = self.transform_f1(masked_features, training=training)\n",
    "            transform_f1 = glu(transform_f1, self.feature_dim)\n",
    "\n",
    "            transform_f2 = self.transform_f2(transform_f1, training=training)\n",
    "            transform_f2 = (glu(transform_f2, self.feature_dim) + transform_f1) * tf.math.sqrt(0.5)\n",
    "\n",
    "            transform_f3 = self.transform_f3_list[ni](transform_f2, training=training)\n",
    "            transform_f3 = (glu(transform_f3, self.feature_dim) + transform_f2) * tf.math.sqrt(0.5)\n",
    "\n",
    "            transform_f4 = self.transform_f4_list[ni](transform_f3, training=training)\n",
    "            transform_f4 = (glu(transform_f4, self.feature_dim) + transform_f3) * tf.math.sqrt(0.5)\n",
    "\n",
    "            if ni > 0 or self.num_decision_steps == 1:\n",
    "                decision_out = tf.nn.relu(transform_f4[:, : self.output_dim])\n",
    "\n",
    "                # Decision aggregation.\n",
    "                output_aggregated += decision_out\n",
    "\n",
    "                # Aggregated masks are used for visualization of the\n",
    "                # feature importance attributes.\n",
    "                scale_agg = tf.reduce_sum(decision_out, axis=1, keepdims=True)\n",
    "\n",
    "                if self.num_decision_steps > 1:\n",
    "                    scale_agg = scale_agg / tf.cast(self.num_decision_steps - 1, tf.float32)\n",
    "\n",
    "                aggregated_mask_values += mask_values * scale_agg\n",
    "\n",
    "            features_for_coef = transform_f4[:, self.output_dim :]\n",
    "\n",
    "            if ni < (self.num_decision_steps - 1):\n",
    "                # Determines the feature masks via linear and nonlinear\n",
    "                # transformations, taking into account of aggregated feature use.\n",
    "                mask_values = self.transform_coef_list[ni](features_for_coef, training=training)\n",
    "                mask_values *= complementary_aggregated_mask_values\n",
    "                mask_values = sparsemax(mask_values, axis=-1)\n",
    "\n",
    "                # Relaxation factor controls the amount of reuse of features between\n",
    "                # different decision blocks and updated with the values of\n",
    "                # coefficients.\n",
    "                complementary_aggregated_mask_values *= self.relaxation_factor - mask_values\n",
    "\n",
    "                # Entropy is used to penalize the amount of sparsity in feature\n",
    "                # selection.\n",
    "                total_entropy += tf.reduce_mean(\n",
    "                    tf.reduce_sum(-mask_values * tf.math.log(mask_values + self.epsilon), axis=1)\n",
    "                ) / (tf.cast(self.num_decision_steps - 1, tf.float32))\n",
    "\n",
    "                # Add entropy loss\n",
    "                entropy_loss = total_entropy\n",
    "\n",
    "                # Feature selection.\n",
    "                masked_features = tf.multiply(mask_values, features)\n",
    "\n",
    "                # Visualization of the feature selection mask at decision step ni\n",
    "                # tf.summary.image(\n",
    "                #     \"Mask for step\" + str(ni),\n",
    "                #     tf.expand_dims(tf.expand_dims(mask_values, 0), 3),\n",
    "                #     max_outputs=1)\n",
    "                mask_at_step_i = tf.expand_dims(tf.expand_dims(mask_values, 0), 3)\n",
    "                self._step_feature_selection_masks.append(mask_at_step_i)\n",
    "\n",
    "            else:\n",
    "                # This branch is needed for correct compilation by tf.autograph\n",
    "                entropy_loss = 0.0\n",
    "\n",
    "        # Adds the loss automatically\n",
    "        self.add_loss(self.sparsity_coefficient * entropy_loss)\n",
    "\n",
    "        # Visualization of the aggregated feature importances\n",
    "        # tf.summary.image(\n",
    "        #     \"Aggregated mask\",\n",
    "        #     tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3),\n",
    "        #     max_outputs=1)\n",
    "\n",
    "        agg_mask = tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3)\n",
    "        self._step_aggregate_feature_selection_mask = agg_mask\n",
    "\n",
    "        return output_aggregated\n",
    "\n",
    "    @property\n",
    "    def feature_selection_masks(self):\n",
    "        return self._step_feature_selection_masks\n",
    "\n",
    "    @property\n",
    "    def aggregate_feature_selection_mask(self):\n",
    "        return self._step_aggregate_feature_selection_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_classes,\n",
    "        num_features=None,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=1,\n",
    "        epsilon=1e-5,\n",
    "        multi_label=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_classes: Number of classes.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'group' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNetClassifier, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.tabnet = TabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_features=num_features,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        if multi_label:\n",
    "\n",
    "            self.clf = tf.keras.layers.Dense(num_classes, activation=\"sigmoid\", use_bias=False, name=\"classifier\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.clf = tf.keras.layers.Dense(num_classes, activation=\"softmax\", use_bias=False, name=\"classifier\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.clf(self.activations)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def summary(self, *super_args, **super_kwargs):\n",
    "        super().summary(*super_args, **super_kwargs)\n",
    "        self.tabnet.summary(*super_args, **super_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetRegressor(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_regressors,\n",
    "        num_features=None,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=1,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_regressors: Number of regression variables.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'group' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNetRegressor, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_regressors = num_regressors\n",
    "\n",
    "        self.tabnet = TabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_features=num_features,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.regressor = tf.keras.layers.Dense(num_regressors, use_bias=False, name=\"regressor\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.regressor(self.activations)\n",
    "        return out\n",
    "\n",
    "    def summary(self, *super_args, **super_kwargs):\n",
    "        super().summary(*super_args, **super_kwargs)\n",
    "        self.tabnet.summary(*super_args, **super_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aliases\n",
    "TabNetClassification = TabNetClassifier\n",
    "TabNetRegression = TabNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedTabNet(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_layers=1,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_layers: Number of TabNets to stack together.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer. Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "                Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(StackedTabNet, self).__init__(**kwargs)\n",
    "\n",
    "        if num_layers < 1:\n",
    "            raise ValueError(\"`num_layers` cannot be less than 1\")\n",
    "\n",
    "        if type(feature_dim) not in [list, tuple]:\n",
    "            feature_dim = [feature_dim] * num_layers\n",
    "\n",
    "        if type(output_dim) not in [list, tuple]:\n",
    "            output_dim = [output_dim] * num_layers\n",
    "\n",
    "        if len(feature_dim) != num_layers:\n",
    "            raise ValueError(\"`feature_dim` must be a list of length `num_layers`\")\n",
    "\n",
    "        if len(output_dim) != num_layers:\n",
    "            raise ValueError(\"`output_dim` must be a list of length `num_layers`\")\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            TabNet(\n",
    "                feature_columns=feature_columns,\n",
    "                num_features=num_features,\n",
    "                feature_dim=feature_dim[0],\n",
    "                output_dim=output_dim[0],\n",
    "                num_decision_steps=num_decision_steps,\n",
    "                relaxation_factor=relaxation_factor,\n",
    "                sparsity_coefficient=sparsity_coefficient,\n",
    "                norm_type=norm_type,\n",
    "                batch_momentum=batch_momentum,\n",
    "                virtual_batch_size=virtual_batch_size,\n",
    "                num_groups=num_groups,\n",
    "                epsilon=epsilon,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for layer_idx in range(1, num_layers):\n",
    "            layers.append(\n",
    "                TabNet(\n",
    "                    feature_columns=None,\n",
    "                    num_features=output_dim[layer_idx - 1],\n",
    "                    feature_dim=feature_dim[layer_idx],\n",
    "                    output_dim=output_dim[layer_idx],\n",
    "                    num_decision_steps=num_decision_steps,\n",
    "                    relaxation_factor=relaxation_factor,\n",
    "                    sparsity_coefficient=sparsity_coefficient,\n",
    "                    norm_type=norm_type,\n",
    "                    batch_momentum=batch_momentum,\n",
    "                    virtual_batch_size=virtual_batch_size,\n",
    "                    num_groups=num_groups,\n",
    "                    epsilon=epsilon,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.tabnet_layers = layers\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.tabnet_layers[0](inputs, training=training)\n",
    "\n",
    "        for layer_idx in range(1, self.num_layers):\n",
    "            x = self.tabnet_layers[layer_idx](x, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def tabnets(self):\n",
    "        return self.tabnet_layers\n",
    "\n",
    "    @property\n",
    "    def feature_selection_masks(self):\n",
    "        return [tabnet.feature_selection_masks for tabnet in self.tabnet_layers]\n",
    "\n",
    "    @property\n",
    "    def aggregate_feature_selection_mask(self):\n",
    "        return [tabnet.aggregate_feature_selection_mask for tabnet in self.tabnet_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedTabNetClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_classes,\n",
    "        num_layers=1,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        multi_label=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_classes: Number of classes.\n",
    "            num_layers: Number of TabNets to stack together.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer. Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "                Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(StackedTabNetClassifier, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.stacked_tabnet = StackedTabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_layers=num_layers,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_features=num_features,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "        if multi_label:\n",
    "\n",
    "            self.clf = tf.keras.layers.Dense(num_classes, activation=\"sigmoid\", use_bias=False)\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.clf = tf.keras.layers.Dense(num_classes, activation=\"softmax\", use_bias=False)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.stacked_tabnet(inputs, training=training)\n",
    "        out = self.clf(self.activations)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedTabNetRegressor(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_regressors,\n",
    "        num_layers=1,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_regressors: Number of regressors.\n",
    "            num_layers: Number of TabNets to stack together.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer. Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "                Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(StackedTabNetRegressor, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_regressors = num_regressors\n",
    "\n",
    "        self.stacked_tabnet = StackedTabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_layers=num_layers,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_features=num_features,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "\n",
    "        self.regressor = tf.keras.layers.Dense(num_regressors, use_bias=False)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.regressor(self.activations)\n",
    "        return outl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026796,
     "end_time": "2020-10-12T13:27:04.454105",
     "exception": false,
     "start_time": "2020-10-12T13:27:04.427309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "papermill": {
     "duration": 1146.878587,
     "end_time": "2020-10-12T13:46:11.358530",
     "exception": false,
     "start_time": "2020-10-12T13:27:04.479943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_STARTS = 1\n",
    "N_SPLITS = 10\n",
    "\n",
    "class Objective:\n",
    "    def __init__(self):\n",
    "        self.best_submit_df = None\n",
    "        self._submit_df = None\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        use_existing_features = trial.suggest_int(\"use_existing_features\", 0, 1)\n",
    "        num_g_cols = trial.suggest_int(\"num_g_cols\", 100, 771)\n",
    "        num_c_cols = trial.suggest_int(\"num_c_cols\", 10, 99)\n",
    "        variance_threshold = trial.suggest_float(\"variance_threshold\", 0.0, 1.0)\n",
    "\n",
    "        train_df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n",
    "        test_df = pd.read_csv(\"../input/lish-moa/test_features.csv\")\n",
    "        target_df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\n",
    "        submit_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n",
    "\n",
    "        train = train_df.copy()\n",
    "        test = test_df.copy()\n",
    "\n",
    "        # encoding\n",
    "        train.loc[:, \"cp_dose\"] = train.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})\n",
    "        test.loc[:, \"cp_dose\"] = test.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})\n",
    "\n",
    "        # ctl_vehicle 除外\n",
    "        target_df = target_df.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "        train = train.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "\n",
    "        train = train.drop(\"cp_type\", axis=1)\n",
    "        test = test.drop(\"cp_type\", axis=1)\n",
    "\n",
    "        del train[\"sig_id\"]\n",
    "        del target_df[\"sig_id\"]\n",
    "        del test[\"sig_id\"]\n",
    "\n",
    "        g_cols = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "        c_cols = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "\n",
    "        # Rank Gauss\n",
    "        for col in g_cols + c_cols:\n",
    "            transformer = QuantileTransformer(n_quantiles=100, random_state=random_seed, output_distribution=\"normal\")\n",
    "\n",
    "            vec_len = len(train[col].values)\n",
    "            vec_len_test = len(test[col].values)\n",
    "\n",
    "            raw_vec = train[col].values.reshape(vec_len, 1)\n",
    "            transformer.fit(raw_vec)\n",
    "\n",
    "            train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "            test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n",
    "\n",
    "        # PCA\n",
    "        # g-\n",
    "        n_comp = num_g_cols\n",
    "\n",
    "        data = pd.concat([pd.DataFrame(train[g_cols]), pd.DataFrame(test[g_cols])])\n",
    "        data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[g_cols])\n",
    "        train2 = data2[: train.shape[0]]\n",
    "        test2 = data2[-test.shape[0] :]\n",
    "\n",
    "        train2 = pd.DataFrame(train2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
    "        test2 = pd.DataFrame(test2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
    "\n",
    "        if use_existing_features == 0:\n",
    "            train.drop(g_cols, axis=1, inplace=True)\n",
    "            test.drop(g_cols, axis=1, inplace=True)\n",
    "\n",
    "        train = pd.concat((train, train2), axis=1)\n",
    "        test = pd.concat((test, test2), axis=1)\n",
    "\n",
    "        # c-\n",
    "        n_comp = num_c_cols\n",
    "\n",
    "        data = pd.concat([pd.DataFrame(train[c_cols]), pd.DataFrame(test[c_cols])])\n",
    "        data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[c_cols])\n",
    "        train2 = data2[: train.shape[0]]\n",
    "        test2 = data2[-test.shape[0] :]\n",
    "\n",
    "        train2 = pd.DataFrame(train2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
    "        test2 = pd.DataFrame(test2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
    "\n",
    "        if use_existing_features == 0:\n",
    "            train.drop(c_cols, axis=1, inplace=True)\n",
    "            test.drop(c_cols, axis=1, inplace=True)\n",
    "\n",
    "        train = pd.concat((train, train2), axis=1)\n",
    "        test = pd.concat((test, test2), axis=1)\n",
    "\n",
    "        # Variance Encoding\n",
    "        var_thresh = VarianceThreshold(threshold=variance_threshold)\n",
    "\n",
    "        data = train.append(test)\n",
    "        data_transformed = var_thresh.fit_transform(data.iloc[:, 2:])\n",
    "\n",
    "        train_transformed = data_transformed[: train.shape[0]]\n",
    "        test_transformed = data_transformed[-test.shape[0] :]\n",
    "\n",
    "        train = pd.DataFrame(train[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
    "        train = pd.concat([train, pd.DataFrame(train_transformed)], axis=1)\n",
    "\n",
    "        test = pd.DataFrame(test[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
    "        test = pd.concat([test, pd.DataFrame(test_transformed)], axis=1)\n",
    "\n",
    "        res = target_df.copy()\n",
    "        submit_df.loc[:, target_df.columns] = 0\n",
    "        res.loc[:, target_df.columns] = 0\n",
    "\n",
    "        for seed in range(N_STARTS):\n",
    "            for n, (tr, te) in enumerate(\n",
    "                MultilabelStratifiedKFold(n_splits=N_SPLITS, random_state=seed, shuffle=True).split(\n",
    "                    target_df, target_df\n",
    "                )\n",
    "            ):\n",
    "                start_time = time()\n",
    "\n",
    "                # model = create_model()\n",
    "                model = StackedTabNetClassifier(\n",
    "                    feature_columns=None,\n",
    "                    num_classes=206,\n",
    "                    num_layers=2,\n",
    "                    feature_dim=128,\n",
    "                    output_dim=64,\n",
    "                    num_features=len(train.columns),\n",
    "                    num_decision_steps=1,\n",
    "                    relaxation_factor=1.5,\n",
    "                    sparsity_coefficient=1e-5,\n",
    "                    batch_momentum=0.98,\n",
    "                    virtual_batch_size=None,\n",
    "                    norm_type=\"group\",\n",
    "                    num_groups=-1,\n",
    "                    multi_label=True,\n",
    "                )\n",
    "\n",
    "                model.compile(\n",
    "                    optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(1e-3), sync_period=10),\n",
    "                    loss=\"binary_crossentropy\",\n",
    "                )\n",
    "\n",
    "                reduce_lr_loss = ReduceLROnPlateau(\n",
    "                    monitor=\"val_loss\", factor=0.1, patience=3, verbose=0, min_delta=1e-4, mode=\"min\"\n",
    "                )\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    patience=10,\n",
    "                    mode=\"min\",\n",
    "                    verbose=0,\n",
    "                    min_delta=1e-4,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "                model.fit(\n",
    "                    train.values[tr],\n",
    "                    target_df.values[tr],\n",
    "                    validation_data=(train.values[te], target_df.values[te]),\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                    callbacks=[reduce_lr_loss, early_stopping],\n",
    "                    verbose=0,\n",
    "                )\n",
    "\n",
    "                test_predict = model.predict(test.values)\n",
    "                val_predict = model.predict(train.values[te])\n",
    "\n",
    "                submit_df.loc[:, target_df.columns] += test_predict / (N_SPLITS * N_STARTS)\n",
    "                res.loc[te, target_df.columns] += val_predict / N_STARTS\n",
    "\n",
    "                fold_score = metric(target_df.loc[te].values, val_predict)\n",
    "\n",
    "                print(\n",
    "                    f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] TabNet: Seed {seed}, Fold {n}:\",\n",
    "                    fold_score,\n",
    "                )\n",
    "\n",
    "                K.clear_session()\n",
    "                del model\n",
    "                x = gc.collect()\n",
    "\n",
    "        self._submit_df = submit_df\n",
    "        score = metric(target_df.values, res.values)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def callback(self, study, trial):\n",
    "        if study.best_trial == trial:\n",
    "            self.best_submit_df = self._submit_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:02] TabNet: Seed 0, Fold 0: 0.01757906165393488\n",
      "[01:01] TabNet: Seed 0, Fold 1: 0.01752760893410638\n",
      "[01:00] TabNet: Seed 0, Fold 2: 0.018071846274350396\n",
      "[01:00] TabNet: Seed 0, Fold 3: 0.017659019682542392\n",
      "[01:00] TabNet: Seed 0, Fold 4: 0.017646695727423913\n",
      "[01:00] TabNet: Seed 0, Fold 5: 0.017408326743300975\n",
      "[01:00] TabNet: Seed 0, Fold 6: 0.017395727243024902\n",
      "[00:55] TabNet: Seed 0, Fold 7: 0.017685169707646324\n",
      "[01:00] TabNet: Seed 0, Fold 8: 0.017516096167437677\n",
      "[00:57] TabNet: Seed 0, Fold 9: 0.017703998435106994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 01:54:56,059] Trial 0 finished with value: 0.017619374860349438 and parameters: {'use_existing_features': 1, 'num_g_cols': 766, 'num_c_cols': 79, 'variance_threshold': 0.976660465819299}. Best is trial 0 with value: 0.017619374860349438.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:05] TabNet: Seed 0, Fold 0: 0.017420513414620586\n",
      "[00:59] TabNet: Seed 0, Fold 1: 0.017488383723993146\n",
      "[00:58] TabNet: Seed 0, Fold 2: 0.01757770638346513\n",
      "[01:01] TabNet: Seed 0, Fold 3: 0.01754060148701097\n",
      "[01:05] TabNet: Seed 0, Fold 4: 0.01761600294238051\n",
      "[01:05] TabNet: Seed 0, Fold 5: 0.01725024472909532\n",
      "[01:05] TabNet: Seed 0, Fold 6: 0.017366048287218782\n",
      "[01:04] TabNet: Seed 0, Fold 7: 0.017393433465548547\n",
      "[01:04] TabNet: Seed 0, Fold 8: 0.01740640025010772\n",
      "[01:00] TabNet: Seed 0, Fold 9: 0.01771406547206126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 02:05:44,903] Trial 1 finished with value: 0.0174773554332808 and parameters: {'use_existing_features': 0, 'num_g_cols': 312, 'num_c_cols': 96, 'variance_threshold': 0.8974948405168504}. Best is trial 1 with value: 0.0174773554332808.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:55] TabNet: Seed 0, Fold 0: 0.017920254025031893\n",
      "[00:56] TabNet: Seed 0, Fold 1: 0.017912652060465644\n",
      "[00:56] TabNet: Seed 0, Fold 2: 0.017839703775886805\n",
      "[00:53] TabNet: Seed 0, Fold 3: 0.017791700945814545\n",
      "[00:56] TabNet: Seed 0, Fold 4: 0.017909494343827638\n",
      "[00:56] TabNet: Seed 0, Fold 5: 0.01780102050788499\n",
      "[00:56] TabNet: Seed 0, Fold 6: 0.017822215794637785\n",
      "[00:55] TabNet: Seed 0, Fold 7: 0.01784229335271834\n",
      "[00:52] TabNet: Seed 0, Fold 8: 0.018148956233132483\n",
      "[00:56] TabNet: Seed 0, Fold 9: 0.017972077757826506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 02:15:21,542] Trial 2 finished with value: 0.017896044571424564 and parameters: {'use_existing_features': 1, 'num_g_cols': 399, 'num_c_cols': 70, 'variance_threshold': 0.5637920464243285}. Best is trial 1 with value: 0.0174773554332808.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52] TabNet: Seed 0, Fold 0: 0.017595236768348486\n",
      "[00:56] TabNet: Seed 0, Fold 1: 0.017845423463885086\n",
      "[00:56] TabNet: Seed 0, Fold 2: 0.018120214126068932\n",
      "[00:56] TabNet: Seed 0, Fold 3: 0.017851445587652576\n",
      "[00:55] TabNet: Seed 0, Fold 4: 0.017986536941944773\n",
      "[00:55] TabNet: Seed 0, Fold 5: 0.017662726160410317\n",
      "[00:56] TabNet: Seed 0, Fold 6: 0.017926129415662663\n",
      "[00:52] TabNet: Seed 0, Fold 7: 0.017564888753965585\n",
      "[00:52] TabNet: Seed 0, Fold 8: 0.01783435242435652\n",
      "[00:52] TabNet: Seed 0, Fold 9: 0.018055086443916773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 02:24:49,180] Trial 3 finished with value: 0.017844208542373494 and parameters: {'use_existing_features': 1, 'num_g_cols': 516, 'num_c_cols': 95, 'variance_threshold': 0.9567970992030014}. Best is trial 1 with value: 0.0174773554332808.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:01] TabNet: Seed 0, Fold 0: 0.01747589639200546\n",
      "[01:03] TabNet: Seed 0, Fold 1: 0.017544672182199357\n",
      "[01:04] TabNet: Seed 0, Fold 2: 0.017905760456572738\n",
      "[01:05] TabNet: Seed 0, Fold 3: 0.017597926791273033\n",
      "[01:04] TabNet: Seed 0, Fold 4: 0.017796233756024762\n",
      "[01:02] TabNet: Seed 0, Fold 5: 0.017475094258883578\n",
      "[01:05] TabNet: Seed 0, Fold 6: 0.01716112144190745\n",
      "[00:58] TabNet: Seed 0, Fold 7: 0.017550329458150542\n",
      "[00:59] TabNet: Seed 0, Fold 8: 0.017565811025220143\n",
      "[01:04] TabNet: Seed 0, Fold 9: 0.01736990071304723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 02:35:39,397] Trial 4 finished with value: 0.017544295252831004 and parameters: {'use_existing_features': 0, 'num_g_cols': 609, 'num_c_cols': 78, 'variance_threshold': 0.8300550937129119}. Best is trial 1 with value: 0.0174773554332808.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:56] TabNet: Seed 0, Fold 0: 0.01759117228187424\n",
      "[00:55] TabNet: Seed 0, Fold 1: 0.01791068002416688\n",
      "[00:55] TabNet: Seed 0, Fold 2: 0.018222429566520655\n",
      "[01:00] TabNet: Seed 0, Fold 3: 0.01777318157705382\n",
      "[00:57] TabNet: Seed 0, Fold 4: 0.018048138178774032\n",
      "[00:56] TabNet: Seed 0, Fold 5: 0.017692694249441192\n",
      "[00:55] TabNet: Seed 0, Fold 6: 0.017945744624978634\n",
      "[00:56] TabNet: Seed 0, Fold 7: 0.017796408427692003\n",
      "[00:56] TabNet: Seed 0, Fold 8: 0.018048446350607237\n",
      "[00:54] TabNet: Seed 0, Fold 9: 0.017899676233727197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 02:45:22,666] Trial 5 finished with value: 0.01789286385009693 and parameters: {'use_existing_features': 0, 'num_g_cols': 653, 'num_c_cols': 36, 'variance_threshold': 0.3315485134328974}. Best is trial 1 with value: 0.0174773554332808.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:55] TabNet: Seed 0, Fold 0: 0.017889885049027738\n",
      "[00:56] TabNet: Seed 0, Fold 1: 0.01810846702873537\n",
      "[00:56] TabNet: Seed 0, Fold 2: 0.01802129817390431\n",
      "[00:56] TabNet: Seed 0, Fold 3: 0.017780317802851556\n",
      "[00:55] TabNet: Seed 0, Fold 4: 0.01774121379578965\n",
      "[00:55] TabNet: Seed 0, Fold 5: 0.01760972509680993\n",
      "[00:52] TabNet: Seed 0, Fold 6: 0.017904362662344776\n",
      "[00:56] TabNet: Seed 0, Fold 7: 0.017832507012030152\n",
      "[00:53] TabNet: Seed 0, Fold 8: 0.018010991656734875\n",
      "[00:57] TabNet: Seed 0, Fold 9: 0.017749792772172326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 02:54:57,905] Trial 6 finished with value: 0.017864865916515447 and parameters: {'use_existing_features': 1, 'num_g_cols': 362, 'num_c_cols': 45, 'variance_threshold': 0.18037459907777476}. Best is trial 1 with value: 0.0174773554332808.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:00] TabNet: Seed 0, Fold 0: 0.017399134743275698\n",
      "[00:58] TabNet: Seed 0, Fold 1: 0.01774720510698259\n",
      "[01:00] TabNet: Seed 0, Fold 2: 0.017973936779461755\n",
      "[01:01] TabNet: Seed 0, Fold 3: 0.01755705518558637\n",
      "[01:00] TabNet: Seed 0, Fold 4: 0.017811842268031664\n",
      "[01:00] TabNet: Seed 0, Fold 5: 0.017795282722108945\n",
      "[01:00] TabNet: Seed 0, Fold 6: 0.017684954509159804\n",
      "[00:54] TabNet: Seed 0, Fold 7: 0.01750664524399759\n",
      "[00:54] TabNet: Seed 0, Fold 8: 0.01785940080789502\n",
      "[01:00] TabNet: Seed 0, Fold 9: 0.01778879651483084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 03:05:08,585] Trial 7 finished with value: 0.017712422862901093 and parameters: {'use_existing_features': 0, 'num_g_cols': 482, 'num_c_cols': 48, 'variance_threshold': 0.4889820585586365}. Best is trial 1 with value: 0.0174773554332808.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:59] TabNet: Seed 0, Fold 0: 0.017640121107800763\n",
      "[01:00] TabNet: Seed 0, Fold 1: 0.017647384975898395\n",
      "[01:06] TabNet: Seed 0, Fold 2: 0.018022659659619727\n",
      "[00:59] TabNet: Seed 0, Fold 3: 0.017717776753468563\n",
      "[00:59] TabNet: Seed 0, Fold 4: 0.017671130929109696\n",
      "[00:59] TabNet: Seed 0, Fold 5: 0.017547410392512286\n",
      "[01:00] TabNet: Seed 0, Fold 6: 0.017670816935801602\n",
      "[00:59] TabNet: Seed 0, Fold 7: 0.017662905203994866\n",
      "[00:54] TabNet: Seed 0, Fold 8: 0.01773447187749529\n",
      "[00:59] TabNet: Seed 0, Fold 9: 0.01745342485818413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 03:15:29,528] Trial 8 finished with value: 0.0176768164389013 and parameters: {'use_existing_features': 0, 'num_g_cols': 537, 'num_c_cols': 41, 'variance_threshold': 0.5699300489063038}. Best is trial 1 with value: 0.0174773554332808.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:01] TabNet: Seed 0, Fold 0: 0.017454231437071203\n",
      "[01:00] TabNet: Seed 0, Fold 1: 0.01746354264369123\n",
      "[01:00] TabNet: Seed 0, Fold 2: 0.0180345537632345\n",
      "[01:06] TabNet: Seed 0, Fold 3: 0.01759203060096614\n",
      "[01:06] TabNet: Seed 0, Fold 4: 0.017642642071087556\n",
      "[01:05] TabNet: Seed 0, Fold 5: 0.017349149054996803\n",
      "[00:59] TabNet: Seed 0, Fold 6: 0.017644541045295663\n",
      "[00:58] TabNet: Seed 0, Fold 7: 0.017602981489158107\n",
      "[01:00] TabNet: Seed 0, Fold 8: 0.017727448437284923\n",
      "[01:03] TabNet: Seed 0, Fold 9: 0.017728860120968075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 03:26:10,549] Trial 9 finished with value: 0.017624009643171228 and parameters: {'use_existing_features': 0, 'num_g_cols': 674, 'num_c_cols': 52, 'variance_threshold': 0.7145957965605945}. Best is trial 1 with value: 0.0174773554332808.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:12] TabNet: Seed 0, Fold 0: 0.017394695928867814\n",
      "[01:06] TabNet: Seed 0, Fold 1: 0.017648389658332663\n",
      "[01:04] TabNet: Seed 0, Fold 2: 0.017576535194810426\n",
      "[01:04] TabNet: Seed 0, Fold 3: 0.017451696694845576\n",
      "[01:05] TabNet: Seed 0, Fold 4: 0.017581791154959572\n",
      "[01:06] TabNet: Seed 0, Fold 5: 0.01738599448731367\n",
      "[01:05] TabNet: Seed 0, Fold 6: 0.0173771789739227\n",
      "[01:11] TabNet: Seed 0, Fold 7: 0.017426582224524652\n",
      "[01:00] TabNet: Seed 0, Fold 8: 0.017466426700537684\n",
      "[01:05] TabNet: Seed 0, Fold 9: 0.0175704228077111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 03:37:29,663] Trial 10 finished with value: 0.017487981076141464 and parameters: {'use_existing_features': 0, 'num_g_cols': 133, 'num_c_cols': 90, 'variance_threshold': 0.7543647013610926}. Best is trial 1 with value: 0.0174773554332808.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:00] TabNet: Seed 0, Fold 0: 0.017469043804863464\n",
      "[01:04] TabNet: Seed 0, Fold 1: 0.017616383446723977\n",
      "[01:06] TabNet: Seed 0, Fold 2: 0.0176745478522002\n",
      "[01:03] TabNet: Seed 0, Fold 3: 0.017562990149229803\n",
      "[00:59] TabNet: Seed 0, Fold 4: 0.017644334964800373\n",
      "[01:04] TabNet: Seed 0, Fold 5: 0.017264508438581096\n",
      "[01:06] TabNet: Seed 0, Fold 6: 0.017511893699530937\n",
      "[01:06] TabNet: Seed 0, Fold 7: 0.01722736034888967\n",
      "[01:05] TabNet: Seed 0, Fold 8: 0.017532771022006264\n",
      "[01:06] TabNet: Seed 0, Fold 9: 0.017458315271769265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 03:48:30,112] Trial 11 finished with value: 0.017496224746440935 and parameters: {'use_existing_features': 0, 'num_g_cols': 161, 'num_c_cols': 10, 'variance_threshold': 0.8018806726177694}. Best is trial 1 with value: 0.0174773554332808.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:07] TabNet: Seed 0, Fold 0: 0.01726944807694952\n",
      "[01:06] TabNet: Seed 0, Fold 1: 0.017205216969590507\n",
      "[01:05] TabNet: Seed 0, Fold 2: 0.01750899621098975\n",
      "[01:10] TabNet: Seed 0, Fold 3: 0.017380322444263255\n",
      "[01:05] TabNet: Seed 0, Fold 4: 0.0175032836348767\n",
      "[01:06] TabNet: Seed 0, Fold 5: 0.017328431711191035\n",
      "[01:06] TabNet: Seed 0, Fold 6: 0.017192401891614646\n",
      "[01:09] TabNet: Seed 0, Fold 7: 0.017131257129309428\n",
      "[01:11] TabNet: Seed 0, Fold 8: 0.017295152646540062\n",
      "[01:06] TabNet: Seed 0, Fold 9: 0.017457046590883856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 04:00:03,322] Trial 12 finished with value: 0.017327161803306276 and parameters: {'use_existing_features': 0, 'num_g_cols': 103, 'num_c_cols': 99, 'variance_threshold': 0.9996082435895635}. Best is trial 12 with value: 0.017327161803306276.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:07] TabNet: Seed 0, Fold 0: 0.017406365311158873\n",
      "[01:00] TabNet: Seed 0, Fold 1: 0.01760623057231604\n",
      "[01:11] TabNet: Seed 0, Fold 2: 0.017371637847276255\n",
      "[01:00] TabNet: Seed 0, Fold 3: 0.017512348004489378\n",
      "[01:06] TabNet: Seed 0, Fold 4: 0.0174349435761243\n",
      "[01:09] TabNet: Seed 0, Fold 5: 0.017189121924692248\n",
      "[01:06] TabNet: Seed 0, Fold 6: 0.017139654587170372\n",
      "[01:11] TabNet: Seed 0, Fold 7: 0.017316482742630028\n",
      "[01:06] TabNet: Seed 0, Fold 8: 0.017431627772171887\n",
      "[01:06] TabNet: Seed 0, Fold 9: 0.01741889628893099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 04:11:27,566] Trial 13 finished with value: 0.017382750746130254 and parameters: {'use_existing_features': 0, 'num_g_cols': 257, 'num_c_cols': 97, 'variance_threshold': 0.9810049125643869}. Best is trial 12 with value: 0.017327161803306276.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:07] TabNet: Seed 0, Fold 0: 0.01735869253377658\n",
      "[01:10] TabNet: Seed 0, Fold 1: 0.017752810390326283\n",
      "[01:04] TabNet: Seed 0, Fold 2: 0.017669870862700388\n",
      "[01:11] TabNet: Seed 0, Fold 3: 0.017489801451520157\n",
      "[01:06] TabNet: Seed 0, Fold 4: 0.017439545105874\n",
      "[01:06] TabNet: Seed 0, Fold 5: 0.01728393691586685\n",
      "[01:05] TabNet: Seed 0, Fold 6: 0.01747762697969731\n",
      "[01:04] TabNet: Seed 0, Fold 7: 0.017438748125695104\n",
      "[01:05] TabNet: Seed 0, Fold 8: 0.01741273022495576\n",
      "[01:06] TabNet: Seed 0, Fold 9: 0.017353657311969924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 04:22:53,448] Trial 14 finished with value: 0.017467749910028786 and parameters: {'use_existing_features': 0, 'num_g_cols': 244, 'num_c_cols': 65, 'variance_threshold': 0.9963823449183751}. Best is trial 12 with value: 0.017327161803306276.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:02] TabNet: Seed 0, Fold 0: 0.017517514173206558\n",
      "[01:04] TabNet: Seed 0, Fold 1: 0.01744030681292735\n",
      "[01:00] TabNet: Seed 0, Fold 2: 0.017724628761857705\n",
      "[00:59] TabNet: Seed 0, Fold 3: 0.017373706607543154\n",
      "[01:05] TabNet: Seed 0, Fold 4: 0.017469541617447\n",
      "[01:04] TabNet: Seed 0, Fold 5: 0.017431284532985265\n",
      "[01:06] TabNet: Seed 0, Fold 6: 0.0175324502767318\n",
      "[01:05] TabNet: Seed 0, Fold 7: 0.01743649756736275\n",
      "[01:00] TabNet: Seed 0, Fold 8: 0.017597742899677405\n",
      "[01:06] TabNet: Seed 0, Fold 9: 0.017626627453174604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 04:33:47,853] Trial 15 finished with value: 0.017515033080897243 and parameters: {'use_existing_features': 0, 'num_g_cols': 226, 'num_c_cols': 86, 'variance_threshold': 0.6669033915134567}. Best is trial 12 with value: 0.017327161803306276.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:01] TabNet: Seed 0, Fold 0: 0.017305773191337106\n",
      "[01:00] TabNet: Seed 0, Fold 1: 0.017348743232388705\n",
      "[01:06] TabNet: Seed 0, Fold 2: 0.017514379319212126\n",
      "[01:00] TabNet: Seed 0, Fold 3: 0.017410510067879234\n",
      "[01:05] TabNet: Seed 0, Fold 4: 0.017530872089227982\n",
      "[01:04] TabNet: Seed 0, Fold 5: 0.017225854994740705\n",
      "[01:00] TabNet: Seed 0, Fold 6: 0.01769615354731776\n",
      "[01:05] TabNet: Seed 0, Fold 7: 0.017358241216499624\n",
      "[01:05] TabNet: Seed 0, Fold 8: 0.017501170875887612\n",
      "[01:00] TabNet: Seed 0, Fold 9: 0.017471851554478893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 04:44:35,609] Trial 16 finished with value: 0.0174363527611978 and parameters: {'use_existing_features': 0, 'num_g_cols': 105, 'num_c_cols': 99, 'variance_threshold': 0.11603793447771693}. Best is trial 12 with value: 0.017327161803306276.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:01] TabNet: Seed 0, Fold 0: 0.017492977268953297\n",
      "[01:01] TabNet: Seed 0, Fold 1: 0.01755029125858537\n",
      "[01:05] TabNet: Seed 0, Fold 2: 0.017535625248472696\n",
      "[01:00] TabNet: Seed 0, Fold 3: 0.017465183735363352\n",
      "[01:06] TabNet: Seed 0, Fold 4: 0.017471690263646124\n",
      "[00:59] TabNet: Seed 0, Fold 5: 0.017435907436357784\n",
      "[01:06] TabNet: Seed 0, Fold 6: 0.017402340144782893\n",
      "[01:00] TabNet: Seed 0, Fold 7: 0.017442864857589575\n",
      "[01:06] TabNet: Seed 0, Fold 8: 0.017512309087875898\n",
      "[01:05] TabNet: Seed 0, Fold 9: 0.017499869340783968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 04:55:27,281] Trial 17 finished with value: 0.017480911479618826 and parameters: {'use_existing_features': 0, 'num_g_cols': 245, 'num_c_cols': 25, 'variance_threshold': 0.8860892069633682}. Best is trial 12 with value: 0.017327161803306276.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:00] TabNet: Seed 0, Fold 0: 0.01742475430500862\n",
      "[00:59] TabNet: Seed 0, Fold 1: 0.01748089095964688\n",
      "[01:01] TabNet: Seed 0, Fold 2: 0.01744797102412869\n",
      "[01:05] TabNet: Seed 0, Fold 3: 0.01713495066743928\n",
      "[01:06] TabNet: Seed 0, Fold 4: 0.017773837048227333\n",
      "[01:00] TabNet: Seed 0, Fold 5: 0.017177846907382128\n",
      "[00:59] TabNet: Seed 0, Fold 6: 0.017630273226786685\n",
      "[00:59] TabNet: Seed 0, Fold 7: 0.01751233727073034\n",
      "[01:05] TabNet: Seed 0, Fold 8: 0.017536535779503693\n",
      "[01:01] TabNet: Seed 0, Fold 9: 0.017598744512481425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 05:06:04,135] Trial 18 finished with value: 0.01747182035171014 and parameters: {'use_existing_features': 0, 'num_g_cols': 179, 'num_c_cols': 63, 'variance_threshold': 0.39913465404686654}. Best is trial 12 with value: 0.017327161803306276.\n",
      "/home/ubuntu/work/tensorflow-gpu/.venv/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:57] TabNet: Seed 0, Fold 0: 0.017643595666627095\n",
      "[00:56] TabNet: Seed 0, Fold 1: 0.017839851796784736\n",
      "[00:56] TabNet: Seed 0, Fold 2: 0.01812992860749519\n",
      "[00:56] TabNet: Seed 0, Fold 3: 0.017764190773669006\n",
      "[00:56] TabNet: Seed 0, Fold 4: 0.01768032295081006\n",
      "[00:56] TabNet: Seed 0, Fold 5: 0.017611706774815217\n",
      "[00:56] TabNet: Seed 0, Fold 6: 0.017702939139905262\n",
      "[00:56] TabNet: Seed 0, Fold 7: 0.017760825945543512\n",
      "[00:56] TabNet: Seed 0, Fold 8: 0.017819746141297707\n",
      "[00:53] TabNet: Seed 0, Fold 9: 0.01799897227465094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-14 05:15:45,681] Trial 19 finished with value: 0.017795220555865028 and parameters: {'use_existing_features': 1, 'num_g_cols': 288, 'num_c_cols': 83, 'variance_threshold': 0.643198084810188}. Best is trial 12 with value: 0.017327161803306276.\n"
     ]
    }
   ],
   "source": [
    "objective = Objective()\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=20, callbacks=[objective.callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.405709,
     "end_time": "2020-10-12T13:46:14.200654",
     "exception": false,
     "start_time": "2020-10-12T13:46:12.794945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.017327161803306276\n",
      "  Params: \n",
      "    use_existing_features: 0\n",
      "    num_g_cols: 103\n",
      "    num_c_cols: 99\n",
      "    variance_threshold: 0.9996082435895635\n"
     ]
    }
   ],
   "source": [
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('num_g_cols', 0.659198475777005),\n",
       "             ('variance_threshold', 0.16505603484007159),\n",
       "             ('use_existing_features', 0.14021386189225393),\n",
       "             ('num_c_cols', 0.03553162749066957)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna.importance.get_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 補正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "duration": 1196.786661,
   "end_time": "2020-10-12T13:46:37.344135",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-12T13:26:40.557474",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
