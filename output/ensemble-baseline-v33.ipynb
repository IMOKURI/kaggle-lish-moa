{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.055663,
     "end_time": "2020-10-26T03:44:53.614003",
     "exception": false,
     "start_time": "2020-10-26T03:44:53.558340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Strategy\n",
    "\n",
    "- Preprocessing\n",
    "    - RankGauss\n",
    "    - PCA + Existing Features\n",
    "    - Variance Encoding\n",
    "- Model\n",
    "    - Normal Neural Network\n",
    "    - Split Neural Network\n",
    "    - ~~NODE (Neural Oblivious Decision Ensembles)~~\n",
    "    - TabNet\n",
    "    - Multi input ResNet\n",
    "    - ~~Kernel Ridge Regression - Platt Scaling ~~\n",
    "    - ~~SVC~~\n",
    "- Learning\n",
    "    - Pre-train with non-scored label\n",
    "    - Optimizer: AdamW with weight_decay\n",
    "    - Label smoothing\n",
    "- Prediction\n",
    "    - Ensemble above ~~with weight optimization~~\n",
    "    - With clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.05589,
     "end_time": "2020-10-26T03:44:53.727594",
     "exception": false,
     "start_time": "2020-10-26T03:44:53.671704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:44:53.847819Z",
     "iopub.status.busy": "2020-10-26T03:44:53.846820Z",
     "iopub.status.idle": "2020-10-26T03:44:53.850041Z",
     "shell.execute_reply": "2020-10-26T03:44:53.849458Z"
    },
    "papermill": {
     "duration": 0.064994,
     "end_time": "2020-10-26T03:44:53.850159",
     "exception": false,
     "start_time": "2020-10-26T03:44:53.785165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:44:53.967563Z",
     "iopub.status.busy": "2020-10-26T03:44:53.966799Z",
     "iopub.status.idle": "2020-10-26T03:44:55.082049Z",
     "shell.execute_reply": "2020-10-26T03:44:55.080516Z"
    },
    "papermill": {
     "duration": 1.176193,
     "end_time": "2020-10-26T03:44:55.082183",
     "exception": false,
     "start_time": "2020-10-26T03:44:53.905990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "sys.path.append(\"../input/autograd\")\n",
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:44:55.210753Z",
     "iopub.status.busy": "2020-10-26T03:44:55.209784Z",
     "iopub.status.idle": "2020-10-26T03:45:02.145414Z",
     "shell.execute_reply": "2020-10-26T03:45:02.144821Z"
    },
    "papermill": {
     "duration": 7.005631,
     "end_time": "2020-10-26T03:45:02.145538",
     "exception": false,
     "start_time": "2020-10-26T03:44:55.139907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "from typing import Optional\n",
    "\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# import optuna\n",
    "from scipy.optimize import fsolve, minimize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:02.281728Z",
     "iopub.status.busy": "2020-10-26T03:45:02.280838Z",
     "iopub.status.idle": "2020-10-26T03:45:02.285897Z",
     "shell.execute_reply": "2020-10-26T03:45:02.285331Z"
    },
    "papermill": {
     "duration": 0.081114,
     "end_time": "2020-10-26T03:45:02.286014",
     "exception": false,
     "start_time": "2020-10-26T03:45:02.204900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerated Linear Algebra enabled\n"
     ]
    }
   ],
   "source": [
    "MIXED_PRECISION = False\n",
    "XLA_ACCELERATE = True\n",
    "\n",
    "if MIXED_PRECISION:\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "    if tpu:\n",
    "        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_bfloat16\")\n",
    "    else:\n",
    "        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "    mixed_precision.set_policy(policy)\n",
    "    print(\"Mixed precision enabled\")\n",
    "\n",
    "if XLA_ACCELERATE:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print(\"Accelerated Linear Algebra enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058345,
     "end_time": "2020-10-26T03:45:02.404913",
     "exception": false,
     "start_time": "2020-10-26T03:45:02.346568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:02.527581Z",
     "iopub.status.busy": "2020-10-26T03:45:02.526766Z",
     "iopub.status.idle": "2020-10-26T03:45:02.530610Z",
     "shell.execute_reply": "2020-10-26T03:45:02.530076Z"
    },
    "papermill": {
     "duration": 0.067933,
     "end_time": "2020-10-26T03:45:02.530706",
     "exception": false,
     "start_time": "2020-10-26T03:45:02.462773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_seed(seed=2020):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "random_seed = 22\n",
    "fix_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:02.654456Z",
     "iopub.status.busy": "2020-10-26T03:45:02.653560Z",
     "iopub.status.idle": "2020-10-26T03:45:02.656869Z",
     "shell.execute_reply": "2020-10-26T03:45:02.656311Z"
    },
    "papermill": {
     "duration": 0.068223,
     "end_time": "2020-10-26T03:45:02.656979",
     "exception": false,
     "start_time": "2020-10-26T03:45:02.588756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/lish-moa/discussion/189857#1043953\n",
    "\n",
    "# Prediction Clipping Thresholds\n",
    "p_min = 0.001\n",
    "p_max = 0.999\n",
    "\n",
    "# Evaluation Metric with clipping and no label smoothing\n",
    "def logloss(y_true, y_pred):\n",
    "    # y_pred = tf.clip_by_value(y_pred, p_min, p_max)\n",
    "    return -K.mean(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:02.784837Z",
     "iopub.status.busy": "2020-10-26T03:45:02.784061Z",
     "iopub.status.idle": "2020-10-26T03:45:02.787245Z",
     "shell.execute_reply": "2020-10-26T03:45:02.786702Z"
    },
    "papermill": {
     "duration": 0.069374,
     "end_time": "2020-10-26T03:45:02.787391",
     "exception": false,
     "start_time": "2020-10-26T03:45:02.718017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [Fast Numpy Log Loss] https://www.kaggle.com/gogo827jz/optimise-blending-weights-4-5x-faster-log-loss\n",
    "def metric(y_true, y_pred):\n",
    "    loss = 0\n",
    "    y_pred_clip = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        loss += -np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n",
    "    return loss / y_pred.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:02.909288Z",
     "iopub.status.busy": "2020-10-26T03:45:02.908309Z",
     "iopub.status.idle": "2020-10-26T03:45:02.911480Z",
     "shell.execute_reply": "2020-10-26T03:45:02.910888Z"
    },
    "papermill": {
     "duration": 0.066105,
     "end_time": "2020-10-26T03:45:02.911579",
     "exception": false,
     "start_time": "2020-10-26T03:45:02.845474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def blend(size, weights, oof):\n",
    "    blend_ = np.zeros(size)\n",
    "    for i, key in enumerate(oof.keys()):\n",
    "        blend_ += weights[i] * oof[key].values\n",
    "    return blend_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057681,
     "end_time": "2020-10-26T03:45:03.027763",
     "exception": false,
     "start_time": "2020-10-26T03:45:02.970082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:03.152330Z",
     "iopub.status.busy": "2020-10-26T03:45:03.151707Z",
     "iopub.status.idle": "2020-10-26T03:45:11.276284Z",
     "shell.execute_reply": "2020-10-26T03:45:11.275078Z"
    },
    "papermill": {
     "duration": 8.190848,
     "end_time": "2020-10-26T03:45:11.276459",
     "exception": false,
     "start_time": "2020-10-26T03:45:03.085611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n",
    "test_df = pd.read_csv(\"../input/lish-moa/test_features.csv\")\n",
    "target_df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\n",
    "non_target_df = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\n",
    "submit_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:11.402457Z",
     "iopub.status.busy": "2020-10-26T03:45:11.401151Z",
     "iopub.status.idle": "2020-10-26T03:45:11.474950Z",
     "shell.execute_reply": "2020-10-26T03:45:11.474372Z"
    },
    "papermill": {
     "duration": 0.138432,
     "end_time": "2020-10-26T03:45:11.475058",
     "exception": false,
     "start_time": "2020-10-26T03:45:11.336626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train_df.copy()\n",
    "test = test_df.copy()\n",
    "ss = submit_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057743,
     "end_time": "2020-10-26T03:45:11.591335",
     "exception": false,
     "start_time": "2020-10-26T03:45:11.533592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:11.730186Z",
     "iopub.status.busy": "2020-10-26T03:45:11.723026Z",
     "iopub.status.idle": "2020-10-26T03:45:11.742704Z",
     "shell.execute_reply": "2020-10-26T03:45:11.742121Z"
    },
    "papermill": {
     "duration": 0.091966,
     "end_time": "2020-10-26T03:45:11.742808",
     "exception": false,
     "start_time": "2020-10-26T03:45:11.650842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.loc[:, \"cp_dose\"] = train.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})\n",
    "test.loc[:, \"cp_dose\"] = test.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})\n",
    "\n",
    "train.loc[:, \"cp_time\"] = train.loc[:, \"cp_time\"].map({24: 0, 48: 1, 72: 2})\n",
    "test.loc[:, \"cp_time\"] = test.loc[:, \"cp_time\"].map({24: 0, 48: 1, 72: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059281,
     "end_time": "2020-10-26T03:45:11.861209",
     "exception": false,
     "start_time": "2020-10-26T03:45:11.801928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## cp_type が ctrl_vehicle なものは MoA を持たない\n",
    "\n",
    "ので、学習から除外する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:11.990683Z",
     "iopub.status.busy": "2020-10-26T03:45:11.989609Z",
     "iopub.status.idle": "2020-10-26T03:45:12.201008Z",
     "shell.execute_reply": "2020-10-26T03:45:12.200419Z"
    },
    "papermill": {
     "duration": 0.281183,
     "end_time": "2020-10-26T03:45:12.201131",
     "exception": false,
     "start_time": "2020-10-26T03:45:11.919948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_df = target_df.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "non_target_df = non_target_df.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "train = train.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:12.388678Z",
     "iopub.status.busy": "2020-10-26T03:45:12.329957Z",
     "iopub.status.idle": "2020-10-26T03:45:12.392418Z",
     "shell.execute_reply": "2020-10-26T03:45:12.391831Z"
    },
    "papermill": {
     "duration": 0.130504,
     "end_time": "2020-10-26T03:45:12.392538",
     "exception": false,
     "start_time": "2020-10-26T03:45:12.262034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.drop(\"cp_type\", axis=1)\n",
    "test = test.drop(\"cp_type\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:12.516125Z",
     "iopub.status.busy": "2020-10-26T03:45:12.515214Z",
     "iopub.status.idle": "2020-10-26T03:45:12.521244Z",
     "shell.execute_reply": "2020-10-26T03:45:12.521771Z"
    },
    "papermill": {
     "duration": 0.07138,
     "end_time": "2020-10-26T03:45:12.521889",
     "exception": false,
     "start_time": "2020-10-26T03:45:12.450509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del train[\"sig_id\"]\n",
    "del target_df[\"sig_id\"]\n",
    "del non_target_df[\"sig_id\"]\n",
    "del test[\"sig_id\"]\n",
    "del ss[\"sig_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:12.641285Z",
     "iopub.status.busy": "2020-10-26T03:45:12.640293Z",
     "iopub.status.idle": "2020-10-26T03:45:12.644816Z",
     "shell.execute_reply": "2020-10-26T03:45:12.644253Z"
    },
    "papermill": {
     "duration": 0.065137,
     "end_time": "2020-10-26T03:45:12.644936",
     "exception": false,
     "start_time": "2020-10-26T03:45:12.579799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058414,
     "end_time": "2020-10-26T03:45:12.763823",
     "exception": false,
     "start_time": "2020-10-26T03:45:12.705409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## feature Selection using Variance Encoding\n",
    "\n",
    "分散がしきい値以下の特徴量を捨てます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:12.892902Z",
     "iopub.status.busy": "2020-10-26T03:45:12.891819Z",
     "iopub.status.idle": "2020-10-26T03:45:13.264664Z",
     "shell.execute_reply": "2020-10-26T03:45:13.263658Z"
    },
    "papermill": {
     "duration": 0.4416,
     "end_time": "2020-10-26T03:45:13.264790",
     "exception": false,
     "start_time": "2020-10-26T03:45:12.823190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "var_threshold = 0.7\n",
    "\n",
    "data = train.append(test)\n",
    "ve_columns = (data.iloc[:, 2:].var() >= var_threshold).values\n",
    "ve_data = data.iloc[:, 2:].loc[:, ve_columns]\n",
    "\n",
    "ve_train = ve_data[: train.shape[0]]\n",
    "ve_test = ve_data[-test.shape[0] :]\n",
    "\n",
    "\n",
    "train = pd.DataFrame(train[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
    "train = pd.concat([train, ve_train], axis=1)\n",
    "\n",
    "\n",
    "test = pd.DataFrame(test[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
    "test = pd.concat([test, ve_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:13.396024Z",
     "iopub.status.busy": "2020-10-26T03:45:13.395143Z",
     "iopub.status.idle": "2020-10-26T03:45:13.434919Z",
     "shell.execute_reply": "2020-10-26T03:45:13.435550Z"
    },
    "papermill": {
     "duration": 0.109015,
     "end_time": "2020-10-26T03:45:13.435691",
     "exception": false,
     "start_time": "2020-10-26T03:45:13.326676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>0.5548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.3372</td>\n",
       "      <td>-0.4047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>-0.1321</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>-0.1498</td>\n",
       "      <td>-0.8789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1608</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>-0.2239</td>\n",
       "      <td>-0.2431</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>-0.1166</td>\n",
       "      <td>-0.1777</td>\n",
       "      <td>-0.7480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.3538</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.3377</td>\n",
       "      <td>-0.4753</td>\n",
       "      <td>-0.2504</td>\n",
       "      <td>-0.7415</td>\n",
       "      <td>0.8413</td>\n",
       "      <td>-0.4259</td>\n",
       "      <td>0.2434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>-0.4726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>-0.2252</td>\n",
       "      <td>-0.5565</td>\n",
       "      <td>0.5112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.4299</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>-1.2420</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>3.0790</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 839 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time  cp_dose     g-0     g-2     g-3     g-4     g-5     g-6  \\\n",
       "0            0        0  1.0620 -0.2479 -0.6208 -0.1944 -1.0120 -1.0220   \n",
       "1            2        0  0.0743  0.2991  0.0604  1.0190  0.5207  0.2341   \n",
       "2            1        0  0.6280  1.5540 -0.0764 -0.0323  1.2390  0.1715   \n",
       "3            1        0 -0.5138 -0.2656  0.5288  4.0620 -0.8095 -1.9590   \n",
       "4            2        1 -0.3254  0.9700  0.6919  1.4180 -0.8244 -0.2800   \n",
       "...        ...      ...     ...     ...     ...     ...     ...     ...   \n",
       "21943        2        0  0.1608  0.2551 -0.2239 -0.2431  0.4256 -0.1166   \n",
       "21944        0        1  0.1394 -0.1112 -0.5080 -0.4713  0.7201  0.5773   \n",
       "21945        0        1 -1.3260 -0.3743  0.9905 -0.7178  0.6621 -0.2252   \n",
       "21946        0        0  0.6660  0.4392  0.2044  0.8531 -0.0343  0.0323   \n",
       "21947        2        0 -0.8598 -0.1361  0.7952 -0.3611 -3.6750 -1.2420   \n",
       "\n",
       "          g-7     g-8  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n",
       "0     -0.0326  0.5548  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n",
       "1      0.3372 -0.4047  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n",
       "2      0.2155  0.0065  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n",
       "3      0.1792 -0.1321  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880   \n",
       "4     -0.1498 -0.8789  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n",
       "...       ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "21943 -0.1777 -0.7480  ...  0.0789  0.3538  0.0558  0.3377 -0.4753 -0.2504   \n",
       "21944  0.3055 -0.4726  ...  0.1969  0.0262 -0.8121  0.3434  0.5372 -0.3246   \n",
       "21945 -0.5565  0.5112  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086 -0.9798   \n",
       "21946  0.0463  0.4299  ... -0.1105  0.4258 -0.2012  0.1506  1.5230  0.7101   \n",
       "21947  0.9146  3.0790  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860 -1.4160   \n",
       "\n",
       "         c-96    c-97    c-98    c-99  \n",
       "0     -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4      0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...  \n",
       "21943 -0.7415  0.8413 -0.4259  0.2434  \n",
       "21944  0.0631  0.9171  0.5258  0.4680  \n",
       "21945 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "21946  0.1732  0.7015 -0.6290  0.0740  \n",
       "21947 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[21948 rows x 839 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059286,
     "end_time": "2020-10-26T03:45:13.561507",
     "exception": false,
     "start_time": "2020-10-26T03:45:13.502221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Rank Gauss\n",
    "\n",
    "https://www.kaggle.com/nayuts/moa-pytorch-nn-pca-rankgauss\n",
    "\n",
    "連続値を特定の範囲の閉域に押し込めて、分布の偏りを解消する方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:13.696047Z",
     "iopub.status.busy": "2020-10-26T03:45:13.695123Z",
     "iopub.status.idle": "2020-10-26T03:45:22.680900Z",
     "shell.execute_reply": "2020-10-26T03:45:22.680230Z"
    },
    "papermill": {
     "duration": 9.059424,
     "end_time": "2020-10-26T03:45:22.681030",
     "exception": false,
     "start_time": "2020-10-26T03:45:13.621606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_cols = [col for col in train.columns if col.startswith(\"g-\")]\n",
    "c_cols = [col for col in train.columns if col.startswith(\"c-\")]\n",
    "\n",
    "for col in g_cols + c_cols:\n",
    "    transformer = QuantileTransformer(n_quantiles=100, random_state=random_seed, output_distribution=\"normal\")\n",
    "\n",
    "    vec_len = len(train[col].values)\n",
    "    vec_len_test = len(test[col].values)\n",
    "\n",
    "    raw_vec = train[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:22.811532Z",
     "iopub.status.busy": "2020-10-26T03:45:22.810519Z",
     "iopub.status.idle": "2020-10-26T03:45:22.844080Z",
     "shell.execute_reply": "2020-10-26T03:45:22.844701Z"
    },
    "papermill": {
     "duration": 0.101903,
     "end_time": "2020-10-26T03:45:22.844873",
     "exception": false,
     "start_time": "2020-10-26T03:45:22.742970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.111801</td>\n",
       "      <td>-0.433829</td>\n",
       "      <td>-0.971728</td>\n",
       "      <td>-0.286559</td>\n",
       "      <td>-1.011388</td>\n",
       "      <td>-1.357431</td>\n",
       "      <td>-0.041716</td>\n",
       "      <td>0.719019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435228</td>\n",
       "      <td>0.388106</td>\n",
       "      <td>1.297345</td>\n",
       "      <td>0.882752</td>\n",
       "      <td>-0.202495</td>\n",
       "      <td>1.052112</td>\n",
       "      <td>-0.472513</td>\n",
       "      <td>0.345458</td>\n",
       "      <td>0.591507</td>\n",
       "      <td>0.692516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105667</td>\n",
       "      <td>0.257486</td>\n",
       "      <td>0.086759</td>\n",
       "      <td>1.199685</td>\n",
       "      <td>0.691813</td>\n",
       "      <td>0.353695</td>\n",
       "      <td>0.558374</td>\n",
       "      <td>-0.497516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.491941</td>\n",
       "      <td>1.148246</td>\n",
       "      <td>0.728406</td>\n",
       "      <td>0.097171</td>\n",
       "      <td>0.454821</td>\n",
       "      <td>0.773468</td>\n",
       "      <td>0.233309</td>\n",
       "      <td>0.207813</td>\n",
       "      <td>0.964312</td>\n",
       "      <td>1.223121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767036</td>\n",
       "      <td>1.408911</td>\n",
       "      <td>-0.126492</td>\n",
       "      <td>-0.028694</td>\n",
       "      <td>1.490985</td>\n",
       "      <td>0.272541</td>\n",
       "      <td>0.359490</td>\n",
       "      <td>0.038331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.794302</td>\n",
       "      <td>-0.715229</td>\n",
       "      <td>0.962055</td>\n",
       "      <td>0.096127</td>\n",
       "      <td>-1.176291</td>\n",
       "      <td>-0.361225</td>\n",
       "      <td>-0.727620</td>\n",
       "      <td>-0.248613</td>\n",
       "      <td>-1.076346</td>\n",
       "      <td>1.142699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.755626</td>\n",
       "      <td>-0.455058</td>\n",
       "      <td>0.765972</td>\n",
       "      <td>2.343522</td>\n",
       "      <td>-0.852713</td>\n",
       "      <td>-2.316440</td>\n",
       "      <td>0.301512</td>\n",
       "      <td>-0.150669</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.381920</td>\n",
       "      <td>-0.730154</td>\n",
       "      <td>-1.612183</td>\n",
       "      <td>-1.211000</td>\n",
       "      <td>-0.911943</td>\n",
       "      <td>-1.191839</td>\n",
       "      <td>-1.286279</td>\n",
       "      <td>-0.943448</td>\n",
       "      <td>-0.439482</td>\n",
       "      <td>-0.881278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.468806</td>\n",
       "      <td>0.956769</td>\n",
       "      <td>0.975864</td>\n",
       "      <td>1.447729</td>\n",
       "      <td>-0.863807</td>\n",
       "      <td>-0.346926</td>\n",
       "      <td>-0.227072</td>\n",
       "      <td>-1.026456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045906</td>\n",
       "      <td>0.023813</td>\n",
       "      <td>1.057944</td>\n",
       "      <td>1.737007</td>\n",
       "      <td>0.844923</td>\n",
       "      <td>-0.344784</td>\n",
       "      <td>0.176914</td>\n",
       "      <td>0.457388</td>\n",
       "      <td>-0.428668</td>\n",
       "      <td>1.176713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225332</td>\n",
       "      <td>0.203082</td>\n",
       "      <td>-0.361744</td>\n",
       "      <td>-0.365285</td>\n",
       "      <td>0.574150</td>\n",
       "      <td>-0.118761</td>\n",
       "      <td>-0.271980</td>\n",
       "      <td>-0.900899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144679</td>\n",
       "      <td>0.531217</td>\n",
       "      <td>0.104134</td>\n",
       "      <td>0.547313</td>\n",
       "      <td>-0.547039</td>\n",
       "      <td>-0.278963</td>\n",
       "      <td>-0.822632</td>\n",
       "      <td>1.339243</td>\n",
       "      <td>-0.480911</td>\n",
       "      <td>0.419996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195938</td>\n",
       "      <td>-0.255311</td>\n",
       "      <td>-0.799750</td>\n",
       "      <td>-0.721089</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.779755</td>\n",
       "      <td>0.509043</td>\n",
       "      <td>-0.582884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303342</td>\n",
       "      <td>0.053359</td>\n",
       "      <td>-0.846121</td>\n",
       "      <td>0.555644</td>\n",
       "      <td>0.821718</td>\n",
       "      <td>-0.369418</td>\n",
       "      <td>0.114555</td>\n",
       "      <td>1.470674</td>\n",
       "      <td>0.826858</td>\n",
       "      <td>0.772835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.946077</td>\n",
       "      <td>-0.604782</td>\n",
       "      <td>1.298848</td>\n",
       "      <td>-1.057997</td>\n",
       "      <td>0.856044</td>\n",
       "      <td>-0.274794</td>\n",
       "      <td>-0.731116</td>\n",
       "      <td>0.670339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.655890</td>\n",
       "      <td>0.665706</td>\n",
       "      <td>0.084499</td>\n",
       "      <td>-0.350560</td>\n",
       "      <td>-0.865833</td>\n",
       "      <td>-1.005158</td>\n",
       "      <td>-0.245946</td>\n",
       "      <td>-0.123235</td>\n",
       "      <td>-0.310856</td>\n",
       "      <td>0.613841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.803911</td>\n",
       "      <td>0.418646</td>\n",
       "      <td>0.309494</td>\n",
       "      <td>1.068126</td>\n",
       "      <td>-0.020533</td>\n",
       "      <td>0.084007</td>\n",
       "      <td>0.087675</td>\n",
       "      <td>0.570980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105613</td>\n",
       "      <td>0.641177</td>\n",
       "      <td>-0.236051</td>\n",
       "      <td>0.272655</td>\n",
       "      <td>2.254169</td>\n",
       "      <td>1.132558</td>\n",
       "      <td>0.263331</td>\n",
       "      <td>1.109534</td>\n",
       "      <td>-0.658578</td>\n",
       "      <td>0.173880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.270703</td>\n",
       "      <td>-0.287824</td>\n",
       "      <td>1.089356</td>\n",
       "      <td>-0.554418</td>\n",
       "      <td>-2.097418</td>\n",
       "      <td>-1.625089</td>\n",
       "      <td>1.460639</td>\n",
       "      <td>2.245448</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.533823</td>\n",
       "      <td>-1.358581</td>\n",
       "      <td>-1.700548</td>\n",
       "      <td>-1.593004</td>\n",
       "      <td>-1.710786</td>\n",
       "      <td>-1.249711</td>\n",
       "      <td>-1.531398</td>\n",
       "      <td>-0.551125</td>\n",
       "      <td>-1.254363</td>\n",
       "      <td>-1.809253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 839 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time  cp_dose       g-0       g-2       g-3       g-4       g-5  \\\n",
       "0            0        0  1.111801 -0.433829 -0.971728 -0.286559 -1.011388   \n",
       "1            2        0  0.105667  0.257486  0.086759  1.199685  0.691813   \n",
       "2            1        0  0.767036  1.408911 -0.126492 -0.028694  1.490985   \n",
       "3            1        0 -0.755626 -0.455058  0.765972  2.343522 -0.852713   \n",
       "4            2        1 -0.468806  0.956769  0.975864  1.447729 -0.863807   \n",
       "...        ...      ...       ...       ...       ...       ...       ...   \n",
       "21943        2        0  0.225332  0.203082 -0.361744 -0.365285  0.574150   \n",
       "21944        0        1  0.195938 -0.255311 -0.799750 -0.721089  0.924354   \n",
       "21945        0        1 -1.946077 -0.604782  1.298848 -1.057997  0.856044   \n",
       "21946        0        0  0.803911  0.418646  0.309494  1.068126 -0.020533   \n",
       "21947        2        0 -1.270703 -0.287824  1.089356 -0.554418 -2.097418   \n",
       "\n",
       "            g-6       g-7       g-8  ...      c-90      c-91      c-92  \\\n",
       "0     -1.357431 -0.041716  0.719019  ...  0.435228  0.388106  1.297345   \n",
       "1      0.353695  0.558374 -0.497516  ... -0.491941  1.148246  0.728406   \n",
       "2      0.272541  0.359490  0.038331  ... -0.794302 -0.715229  0.962055   \n",
       "3     -2.316440  0.301512 -0.150669  ... -1.381920 -0.730154 -1.612183   \n",
       "4     -0.346926 -0.227072 -1.026456  ...  0.045906  0.023813  1.057944   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "21943 -0.118761 -0.271980 -0.900899  ...  0.144679  0.531217  0.104134   \n",
       "21944  0.779755  0.509043 -0.582884  ...  0.303342  0.053359 -0.846121   \n",
       "21945 -0.274794 -0.731116  0.670339  ...  0.655890  0.665706  0.084499   \n",
       "21946  0.084007  0.087675  0.570980  ... -0.105613  0.641177 -0.236051   \n",
       "21947 -1.625089  1.460639  2.245448  ... -1.533823 -1.358581 -1.700548   \n",
       "\n",
       "           c-93      c-94      c-95      c-96      c-97      c-98      c-99  \n",
       "0      0.882752 -0.202495  1.052112 -0.472513  0.345458  0.591507  0.692516  \n",
       "1      0.097171  0.454821  0.773468  0.233309  0.207813  0.964312  1.223121  \n",
       "2      0.096127 -1.176291 -0.361225 -0.727620 -0.248613 -1.076346  1.142699  \n",
       "3     -1.211000 -0.911943 -1.191839 -1.286279 -0.943448 -0.439482 -0.881278  \n",
       "4      1.737007  0.844923 -0.344784  0.176914  0.457388 -0.428668  1.176713  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "21943  0.547313 -0.547039 -0.278963 -0.822632  1.339243 -0.480911  0.419996  \n",
       "21944  0.555644  0.821718 -0.369418  0.114555  1.470674  0.826858  0.772835  \n",
       "21945 -0.350560 -0.865833 -1.005158 -0.245946 -0.123235 -0.310856  0.613841  \n",
       "21946  0.272655  2.254169  1.132558  0.263331  1.109534 -0.658578  0.173880  \n",
       "21947 -1.593004 -1.710786 -1.249711 -1.531398 -0.551125 -1.254363 -1.809253  \n",
       "\n",
       "[21948 rows x 839 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.064218,
     "end_time": "2020-10-26T03:45:22.977059",
     "exception": false,
     "start_time": "2020-10-26T03:45:22.912841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PCA features (+ Existing features)\n",
    "\n",
    "既存のカラムは残したほうがいいのだろうか？？\n",
    "→ このコンペでは残したほうがいい成績が出ている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:23.248728Z",
     "iopub.status.busy": "2020-10-26T03:45:23.232435Z",
     "iopub.status.idle": "2020-10-26T03:45:25.421203Z",
     "shell.execute_reply": "2020-10-26T03:45:25.420474Z"
    },
    "papermill": {
     "duration": 2.351345,
     "end_time": "2020-10-26T03:45:25.421318",
     "exception": false,
     "start_time": "2020-10-26T03:45:23.069973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# g-\n",
    "n_comp = 80\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train[g_cols]), pd.DataFrame(test[g_cols])])\n",
    "data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[g_cols])\n",
    "train2 = data2[: train.shape[0]]\n",
    "test2 = data2[-test.shape[0] :]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
    "\n",
    "train = pd.concat((train, train2), axis=1)\n",
    "test = pd.concat((test, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:25.557241Z",
     "iopub.status.busy": "2020-10-26T03:45:25.555945Z",
     "iopub.status.idle": "2020-10-26T03:45:25.887776Z",
     "shell.execute_reply": "2020-10-26T03:45:25.886099Z"
    },
    "papermill": {
     "duration": 0.404376,
     "end_time": "2020-10-26T03:45:25.887920",
     "exception": false,
     "start_time": "2020-10-26T03:45:25.483544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# c-\n",
    "n_comp = 10\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train[c_cols]), pd.DataFrame(test[c_cols])])\n",
    "data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[c_cols])\n",
    "train2 = data2[: train.shape[0]]\n",
    "test2 = data2[-test.shape[0] :]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
    "\n",
    "train = pd.concat((train, train2), axis=1)\n",
    "test = pd.concat((test, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:26.034352Z",
     "iopub.status.busy": "2020-10-26T03:45:26.033100Z",
     "iopub.status.idle": "2020-10-26T03:45:26.067335Z",
     "shell.execute_reply": "2020-10-26T03:45:26.067910Z"
    },
    "papermill": {
     "duration": 0.111039,
     "end_time": "2020-10-26T03:45:26.068047",
     "exception": false,
     "start_time": "2020-10-26T03:45:25.957008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_C-0</th>\n",
       "      <th>pca_C-1</th>\n",
       "      <th>pca_C-2</th>\n",
       "      <th>pca_C-3</th>\n",
       "      <th>pca_C-4</th>\n",
       "      <th>pca_C-5</th>\n",
       "      <th>pca_C-6</th>\n",
       "      <th>pca_C-7</th>\n",
       "      <th>pca_C-8</th>\n",
       "      <th>pca_C-9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.111801</td>\n",
       "      <td>-0.433829</td>\n",
       "      <td>-0.971728</td>\n",
       "      <td>-0.286559</td>\n",
       "      <td>-1.011388</td>\n",
       "      <td>-1.357431</td>\n",
       "      <td>-0.041716</td>\n",
       "      <td>0.719019</td>\n",
       "      <td>...</td>\n",
       "      <td>5.138660</td>\n",
       "      <td>1.614524</td>\n",
       "      <td>0.776236</td>\n",
       "      <td>1.556109</td>\n",
       "      <td>1.041572</td>\n",
       "      <td>-1.121239</td>\n",
       "      <td>-0.492145</td>\n",
       "      <td>0.331276</td>\n",
       "      <td>0.173820</td>\n",
       "      <td>-0.447606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105667</td>\n",
       "      <td>0.257486</td>\n",
       "      <td>0.086759</td>\n",
       "      <td>1.199685</td>\n",
       "      <td>0.691813</td>\n",
       "      <td>0.353695</td>\n",
       "      <td>0.558374</td>\n",
       "      <td>-0.497516</td>\n",
       "      <td>...</td>\n",
       "      <td>5.305413</td>\n",
       "      <td>-0.392204</td>\n",
       "      <td>-0.367946</td>\n",
       "      <td>0.947217</td>\n",
       "      <td>-0.389658</td>\n",
       "      <td>0.701852</td>\n",
       "      <td>-0.079006</td>\n",
       "      <td>0.952375</td>\n",
       "      <td>0.403545</td>\n",
       "      <td>-1.909597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767036</td>\n",
       "      <td>1.408911</td>\n",
       "      <td>-0.126492</td>\n",
       "      <td>-0.028694</td>\n",
       "      <td>1.490985</td>\n",
       "      <td>0.272541</td>\n",
       "      <td>0.359490</td>\n",
       "      <td>0.038331</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.125249</td>\n",
       "      <td>0.502525</td>\n",
       "      <td>0.196190</td>\n",
       "      <td>-0.205435</td>\n",
       "      <td>-0.111137</td>\n",
       "      <td>-0.617544</td>\n",
       "      <td>0.695756</td>\n",
       "      <td>0.287744</td>\n",
       "      <td>0.534502</td>\n",
       "      <td>-0.999676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.755626</td>\n",
       "      <td>-0.455058</td>\n",
       "      <td>0.765972</td>\n",
       "      <td>2.343522</td>\n",
       "      <td>-0.852713</td>\n",
       "      <td>-2.316440</td>\n",
       "      <td>0.301512</td>\n",
       "      <td>-0.150669</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.642308</td>\n",
       "      <td>1.374223</td>\n",
       "      <td>1.094509</td>\n",
       "      <td>-1.155044</td>\n",
       "      <td>-0.078245</td>\n",
       "      <td>-1.038674</td>\n",
       "      <td>-1.121736</td>\n",
       "      <td>1.328550</td>\n",
       "      <td>0.619391</td>\n",
       "      <td>-1.531043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.468806</td>\n",
       "      <td>0.956769</td>\n",
       "      <td>0.975864</td>\n",
       "      <td>1.447729</td>\n",
       "      <td>-0.863807</td>\n",
       "      <td>-0.346926</td>\n",
       "      <td>-0.227072</td>\n",
       "      <td>-1.026456</td>\n",
       "      <td>...</td>\n",
       "      <td>3.857797</td>\n",
       "      <td>0.748852</td>\n",
       "      <td>0.499835</td>\n",
       "      <td>-0.058012</td>\n",
       "      <td>0.173419</td>\n",
       "      <td>0.262766</td>\n",
       "      <td>0.358375</td>\n",
       "      <td>0.121263</td>\n",
       "      <td>-0.149875</td>\n",
       "      <td>-0.381182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225332</td>\n",
       "      <td>0.203082</td>\n",
       "      <td>-0.361744</td>\n",
       "      <td>-0.365285</td>\n",
       "      <td>0.574150</td>\n",
       "      <td>-0.118761</td>\n",
       "      <td>-0.271980</td>\n",
       "      <td>-0.900899</td>\n",
       "      <td>...</td>\n",
       "      <td>1.858519</td>\n",
       "      <td>-0.014185</td>\n",
       "      <td>-1.038244</td>\n",
       "      <td>-0.490035</td>\n",
       "      <td>0.515629</td>\n",
       "      <td>0.593294</td>\n",
       "      <td>-1.169701</td>\n",
       "      <td>-0.369979</td>\n",
       "      <td>0.531955</td>\n",
       "      <td>0.554555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195938</td>\n",
       "      <td>-0.255311</td>\n",
       "      <td>-0.799750</td>\n",
       "      <td>-0.721089</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.779755</td>\n",
       "      <td>0.509043</td>\n",
       "      <td>-0.582884</td>\n",
       "      <td>...</td>\n",
       "      <td>3.951934</td>\n",
       "      <td>-0.202384</td>\n",
       "      <td>-0.530170</td>\n",
       "      <td>-1.108320</td>\n",
       "      <td>-0.559824</td>\n",
       "      <td>0.684455</td>\n",
       "      <td>-0.309543</td>\n",
       "      <td>-0.205453</td>\n",
       "      <td>1.198600</td>\n",
       "      <td>-1.643753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.946077</td>\n",
       "      <td>-0.604782</td>\n",
       "      <td>1.298848</td>\n",
       "      <td>-1.057997</td>\n",
       "      <td>0.856044</td>\n",
       "      <td>-0.274794</td>\n",
       "      <td>-0.731116</td>\n",
       "      <td>0.670339</td>\n",
       "      <td>...</td>\n",
       "      <td>1.173410</td>\n",
       "      <td>1.293207</td>\n",
       "      <td>-0.798852</td>\n",
       "      <td>-0.412653</td>\n",
       "      <td>0.665116</td>\n",
       "      <td>1.503851</td>\n",
       "      <td>1.622795</td>\n",
       "      <td>-0.837935</td>\n",
       "      <td>0.325608</td>\n",
       "      <td>-0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.803911</td>\n",
       "      <td>0.418646</td>\n",
       "      <td>0.309494</td>\n",
       "      <td>1.068126</td>\n",
       "      <td>-0.020533</td>\n",
       "      <td>0.084007</td>\n",
       "      <td>0.087675</td>\n",
       "      <td>0.570980</td>\n",
       "      <td>...</td>\n",
       "      <td>6.080402</td>\n",
       "      <td>1.991641</td>\n",
       "      <td>0.560793</td>\n",
       "      <td>1.329499</td>\n",
       "      <td>-1.595791</td>\n",
       "      <td>0.576354</td>\n",
       "      <td>-0.619038</td>\n",
       "      <td>0.381564</td>\n",
       "      <td>0.417649</td>\n",
       "      <td>0.852836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.270703</td>\n",
       "      <td>-0.287824</td>\n",
       "      <td>1.089356</td>\n",
       "      <td>-0.554418</td>\n",
       "      <td>-2.097418</td>\n",
       "      <td>-1.625089</td>\n",
       "      <td>1.460639</td>\n",
       "      <td>2.245448</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.656636</td>\n",
       "      <td>0.155709</td>\n",
       "      <td>-0.096318</td>\n",
       "      <td>-1.181447</td>\n",
       "      <td>-0.039918</td>\n",
       "      <td>-0.182590</td>\n",
       "      <td>0.703315</td>\n",
       "      <td>0.348770</td>\n",
       "      <td>-0.264875</td>\n",
       "      <td>-0.157159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 929 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time  cp_dose       g-0       g-2       g-3       g-4       g-5  \\\n",
       "0            0        0  1.111801 -0.433829 -0.971728 -0.286559 -1.011388   \n",
       "1            2        0  0.105667  0.257486  0.086759  1.199685  0.691813   \n",
       "2            1        0  0.767036  1.408911 -0.126492 -0.028694  1.490985   \n",
       "3            1        0 -0.755626 -0.455058  0.765972  2.343522 -0.852713   \n",
       "4            2        1 -0.468806  0.956769  0.975864  1.447729 -0.863807   \n",
       "...        ...      ...       ...       ...       ...       ...       ...   \n",
       "21943        2        0  0.225332  0.203082 -0.361744 -0.365285  0.574150   \n",
       "21944        0        1  0.195938 -0.255311 -0.799750 -0.721089  0.924354   \n",
       "21945        0        1 -1.946077 -0.604782  1.298848 -1.057997  0.856044   \n",
       "21946        0        0  0.803911  0.418646  0.309494  1.068126 -0.020533   \n",
       "21947        2        0 -1.270703 -0.287824  1.089356 -0.554418 -2.097418   \n",
       "\n",
       "            g-6       g-7       g-8  ...    pca_C-0   pca_C-1   pca_C-2  \\\n",
       "0     -1.357431 -0.041716  0.719019  ...   5.138660  1.614524  0.776236   \n",
       "1      0.353695  0.558374 -0.497516  ...   5.305413 -0.392204 -0.367946   \n",
       "2      0.272541  0.359490  0.038331  ...  -1.125249  0.502525  0.196190   \n",
       "3     -2.316440  0.301512 -0.150669  ... -10.642308  1.374223  1.094509   \n",
       "4     -0.346926 -0.227072 -1.026456  ...   3.857797  0.748852  0.499835   \n",
       "...         ...       ...       ...  ...        ...       ...       ...   \n",
       "21943 -0.118761 -0.271980 -0.900899  ...   1.858519 -0.014185 -1.038244   \n",
       "21944  0.779755  0.509043 -0.582884  ...   3.951934 -0.202384 -0.530170   \n",
       "21945 -0.274794 -0.731116  0.670339  ...   1.173410  1.293207 -0.798852   \n",
       "21946  0.084007  0.087675  0.570980  ...   6.080402  1.991641  0.560793   \n",
       "21947 -1.625089  1.460639  2.245448  ... -13.656636  0.155709 -0.096318   \n",
       "\n",
       "        pca_C-3   pca_C-4   pca_C-5   pca_C-6   pca_C-7   pca_C-8   pca_C-9  \n",
       "0      1.556109  1.041572 -1.121239 -0.492145  0.331276  0.173820 -0.447606  \n",
       "1      0.947217 -0.389658  0.701852 -0.079006  0.952375  0.403545 -1.909597  \n",
       "2     -0.205435 -0.111137 -0.617544  0.695756  0.287744  0.534502 -0.999676  \n",
       "3     -1.155044 -0.078245 -1.038674 -1.121736  1.328550  0.619391 -1.531043  \n",
       "4     -0.058012  0.173419  0.262766  0.358375  0.121263 -0.149875 -0.381182  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "21943 -0.490035  0.515629  0.593294 -1.169701 -0.369979  0.531955  0.554555  \n",
       "21944 -1.108320 -0.559824  0.684455 -0.309543 -0.205453  1.198600 -1.643753  \n",
       "21945 -0.412653  0.665116  1.503851  1.622795 -0.837935  0.325608 -0.000272  \n",
       "21946  1.329499 -1.595791  0.576354 -0.619038  0.381564  0.417649  0.852836  \n",
       "21947 -1.181447 -0.039918 -0.182590  0.703315  0.348770 -0.264875 -0.157159  \n",
       "\n",
       "[21948 rows x 929 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.064374,
     "end_time": "2020-10-26T03:45:26.197711",
     "exception": false,
     "start_time": "2020-10-26T03:45:26.133337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Basic stats\n",
    "\n",
    "四則演算特徴量追加\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:26.337183Z",
     "iopub.status.busy": "2020-10-26T03:45:26.335825Z",
     "iopub.status.idle": "2020-10-26T03:45:31.507183Z",
     "shell.execute_reply": "2020-10-26T03:45:31.506290Z"
    },
    "papermill": {
     "duration": 5.245995,
     "end_time": "2020-10-26T03:45:31.507320",
     "exception": false,
     "start_time": "2020-10-26T03:45:26.261325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for stats in [\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]:\n",
    "    train[\"g_\" + stats] = getattr(train[g_cols], stats)(axis = 1)\n",
    "    train[\"c_\" + stats] = getattr(train[c_cols], stats)(axis = 1)\n",
    "    train[\"gc_\" + stats] = getattr(train[g_cols + c_cols], stats)(axis = 1)\n",
    "    \n",
    "    test[\"g_\" + stats] = getattr(test[g_cols], stats)(axis = 1)\n",
    "    test[\"c_\" + stats] = getattr(test[c_cols], stats)(axis = 1)\n",
    "    test[\"gc_\" + stats] = getattr(test[g_cols + c_cols], stats)(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:31.652686Z",
     "iopub.status.busy": "2020-10-26T03:45:31.651984Z",
     "iopub.status.idle": "2020-10-26T03:45:31.691847Z",
     "shell.execute_reply": "2020-10-26T03:45:31.692459Z"
    },
    "papermill": {
     "duration": 0.109932,
     "end_time": "2020-10-26T03:45:31.692630",
     "exception": false,
     "start_time": "2020-10-26T03:45:31.582698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>...</th>\n",
       "      <th>gc_mean</th>\n",
       "      <th>g_std</th>\n",
       "      <th>c_std</th>\n",
       "      <th>gc_std</th>\n",
       "      <th>g_kurt</th>\n",
       "      <th>c_kurt</th>\n",
       "      <th>gc_kurt</th>\n",
       "      <th>g_skew</th>\n",
       "      <th>c_skew</th>\n",
       "      <th>gc_skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.111801</td>\n",
       "      <td>-0.433829</td>\n",
       "      <td>-0.971728</td>\n",
       "      <td>-0.286559</td>\n",
       "      <td>-1.011388</td>\n",
       "      <td>-1.357431</td>\n",
       "      <td>-0.041716</td>\n",
       "      <td>0.719019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057614</td>\n",
       "      <td>0.865619</td>\n",
       "      <td>0.730496</td>\n",
       "      <td>0.866639</td>\n",
       "      <td>-0.222353</td>\n",
       "      <td>-0.320226</td>\n",
       "      <td>-0.222313</td>\n",
       "      <td>-0.006447</td>\n",
       "      <td>0.072483</td>\n",
       "      <td>-0.038894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105667</td>\n",
       "      <td>0.257486</td>\n",
       "      <td>0.086759</td>\n",
       "      <td>1.199685</td>\n",
       "      <td>0.691813</td>\n",
       "      <td>0.353695</td>\n",
       "      <td>0.558374</td>\n",
       "      <td>-0.497516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059153</td>\n",
       "      <td>0.852443</td>\n",
       "      <td>0.607776</td>\n",
       "      <td>0.844414</td>\n",
       "      <td>-0.179074</td>\n",
       "      <td>0.083991</td>\n",
       "      <td>-0.202692</td>\n",
       "      <td>0.051917</td>\n",
       "      <td>-0.168964</td>\n",
       "      <td>-0.040794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767036</td>\n",
       "      <td>1.408911</td>\n",
       "      <td>-0.126492</td>\n",
       "      <td>-0.028694</td>\n",
       "      <td>1.490985</td>\n",
       "      <td>0.272541</td>\n",
       "      <td>0.359490</td>\n",
       "      <td>0.038331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041415</td>\n",
       "      <td>0.940095</td>\n",
       "      <td>0.664359</td>\n",
       "      <td>0.911721</td>\n",
       "      <td>-0.348731</td>\n",
       "      <td>-0.188699</td>\n",
       "      <td>-0.274455</td>\n",
       "      <td>-0.030332</td>\n",
       "      <td>0.384022</td>\n",
       "      <td>0.005181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.755626</td>\n",
       "      <td>-0.455058</td>\n",
       "      <td>0.765972</td>\n",
       "      <td>2.343522</td>\n",
       "      <td>-0.852713</td>\n",
       "      <td>-2.316440</td>\n",
       "      <td>0.301512</td>\n",
       "      <td>-0.150669</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146895</td>\n",
       "      <td>1.077030</td>\n",
       "      <td>0.575324</td>\n",
       "      <td>1.083923</td>\n",
       "      <td>-0.915539</td>\n",
       "      <td>3.977177</td>\n",
       "      <td>-0.954259</td>\n",
       "      <td>0.089322</td>\n",
       "      <td>1.955443</td>\n",
       "      <td>0.255048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.468806</td>\n",
       "      <td>0.956769</td>\n",
       "      <td>0.975864</td>\n",
       "      <td>1.447729</td>\n",
       "      <td>-0.863807</td>\n",
       "      <td>-0.346926</td>\n",
       "      <td>-0.227072</td>\n",
       "      <td>-1.026456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023250</td>\n",
       "      <td>1.112844</td>\n",
       "      <td>0.676915</td>\n",
       "      <td>1.077820</td>\n",
       "      <td>-0.207397</td>\n",
       "      <td>-0.725125</td>\n",
       "      <td>-0.085380</td>\n",
       "      <td>-0.195793</td>\n",
       "      <td>0.076891</td>\n",
       "      <td>-0.263166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225332</td>\n",
       "      <td>0.203082</td>\n",
       "      <td>-0.361744</td>\n",
       "      <td>-0.365285</td>\n",
       "      <td>0.574150</td>\n",
       "      <td>-0.118761</td>\n",
       "      <td>-0.271980</td>\n",
       "      <td>-0.900899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084741</td>\n",
       "      <td>0.682694</td>\n",
       "      <td>0.696628</td>\n",
       "      <td>0.684662</td>\n",
       "      <td>-0.146175</td>\n",
       "      <td>-0.377779</td>\n",
       "      <td>-0.150038</td>\n",
       "      <td>-0.064515</td>\n",
       "      <td>0.419003</td>\n",
       "      <td>-0.003303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195938</td>\n",
       "      <td>-0.255311</td>\n",
       "      <td>-0.799750</td>\n",
       "      <td>-0.721089</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.779755</td>\n",
       "      <td>0.509043</td>\n",
       "      <td>-0.582884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053313</td>\n",
       "      <td>0.786501</td>\n",
       "      <td>0.729967</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.120454</td>\n",
       "      <td>-0.221594</td>\n",
       "      <td>0.162327</td>\n",
       "      <td>-0.200306</td>\n",
       "      <td>0.320651</td>\n",
       "      <td>-0.158667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.946077</td>\n",
       "      <td>-0.604782</td>\n",
       "      <td>1.298848</td>\n",
       "      <td>-1.057997</td>\n",
       "      <td>0.856044</td>\n",
       "      <td>-0.274794</td>\n",
       "      <td>-0.731116</td>\n",
       "      <td>0.670339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022455</td>\n",
       "      <td>0.812636</td>\n",
       "      <td>0.850337</td>\n",
       "      <td>0.817323</td>\n",
       "      <td>0.089973</td>\n",
       "      <td>-0.212426</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>-0.010357</td>\n",
       "      <td>0.457180</td>\n",
       "      <td>0.054264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.803911</td>\n",
       "      <td>0.418646</td>\n",
       "      <td>0.309494</td>\n",
       "      <td>1.068126</td>\n",
       "      <td>-0.020533</td>\n",
       "      <td>0.084007</td>\n",
       "      <td>0.087675</td>\n",
       "      <td>0.570980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143788</td>\n",
       "      <td>1.165174</td>\n",
       "      <td>0.800976</td>\n",
       "      <td>1.139215</td>\n",
       "      <td>-1.002151</td>\n",
       "      <td>-0.630398</td>\n",
       "      <td>-0.900820</td>\n",
       "      <td>-0.147141</td>\n",
       "      <td>0.107626</td>\n",
       "      <td>-0.204890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.270703</td>\n",
       "      <td>-0.287824</td>\n",
       "      <td>1.089356</td>\n",
       "      <td>-0.554418</td>\n",
       "      <td>-2.097418</td>\n",
       "      <td>-1.625089</td>\n",
       "      <td>1.460639</td>\n",
       "      <td>2.245448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194836</td>\n",
       "      <td>1.203292</td>\n",
       "      <td>0.347157</td>\n",
       "      <td>1.217116</td>\n",
       "      <td>-0.974193</td>\n",
       "      <td>6.118678</td>\n",
       "      <td>-1.048005</td>\n",
       "      <td>0.018163</td>\n",
       "      <td>2.104892</td>\n",
       "      <td>0.226085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 944 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time  cp_dose       g-0       g-2       g-3       g-4       g-5  \\\n",
       "0            0        0  1.111801 -0.433829 -0.971728 -0.286559 -1.011388   \n",
       "1            2        0  0.105667  0.257486  0.086759  1.199685  0.691813   \n",
       "2            1        0  0.767036  1.408911 -0.126492 -0.028694  1.490985   \n",
       "3            1        0 -0.755626 -0.455058  0.765972  2.343522 -0.852713   \n",
       "4            2        1 -0.468806  0.956769  0.975864  1.447729 -0.863807   \n",
       "...        ...      ...       ...       ...       ...       ...       ...   \n",
       "21943        2        0  0.225332  0.203082 -0.361744 -0.365285  0.574150   \n",
       "21944        0        1  0.195938 -0.255311 -0.799750 -0.721089  0.924354   \n",
       "21945        0        1 -1.946077 -0.604782  1.298848 -1.057997  0.856044   \n",
       "21946        0        0  0.803911  0.418646  0.309494  1.068126 -0.020533   \n",
       "21947        2        0 -1.270703 -0.287824  1.089356 -0.554418 -2.097418   \n",
       "\n",
       "            g-6       g-7       g-8  ...   gc_mean     g_std     c_std  \\\n",
       "0     -1.357431 -0.041716  0.719019  ...  0.057614  0.865619  0.730496   \n",
       "1      0.353695  0.558374 -0.497516  ...  0.059153  0.852443  0.607776   \n",
       "2      0.272541  0.359490  0.038331  ... -0.041415  0.940095  0.664359   \n",
       "3     -2.316440  0.301512 -0.150669  ... -0.146895  1.077030  0.575324   \n",
       "4     -0.346926 -0.227072 -1.026456  ...  0.023250  1.112844  0.676915   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "21943 -0.118761 -0.271980 -0.900899  ...  0.084741  0.682694  0.696628   \n",
       "21944  0.779755  0.509043 -0.582884  ...  0.053313  0.786501  0.729967   \n",
       "21945 -0.274794 -0.731116  0.670339  ...  0.022455  0.812636  0.850337   \n",
       "21946  0.084007  0.087675  0.570980  ...  0.143788  1.165174  0.800976   \n",
       "21947 -1.625089  1.460639  2.245448  ... -0.194836  1.203292  0.347157   \n",
       "\n",
       "         gc_std    g_kurt    c_kurt   gc_kurt    g_skew    c_skew   gc_skew  \n",
       "0      0.866639 -0.222353 -0.320226 -0.222313 -0.006447  0.072483 -0.038894  \n",
       "1      0.844414 -0.179074  0.083991 -0.202692  0.051917 -0.168964 -0.040794  \n",
       "2      0.911721 -0.348731 -0.188699 -0.274455 -0.030332  0.384022  0.005181  \n",
       "3      1.083923 -0.915539  3.977177 -0.954259  0.089322  1.955443  0.255048  \n",
       "4      1.077820 -0.207397 -0.725125 -0.085380 -0.195793  0.076891 -0.263166  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "21943  0.684662 -0.146175 -0.377779 -0.150038 -0.064515  0.419003 -0.003303  \n",
       "21944  0.788491  0.120454 -0.221594  0.162327 -0.200306  0.320651 -0.158667  \n",
       "21945  0.817323  0.089973 -0.212426  0.070046 -0.010357  0.457180  0.054264  \n",
       "21946  1.139215 -1.002151 -0.630398 -0.900820 -0.147141  0.107626 -0.204890  \n",
       "21947  1.217116 -0.974193  6.118678 -1.048005  0.018163  2.104892  0.226085  \n",
       "\n",
       "[21948 rows x 944 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:31.844152Z",
     "iopub.status.busy": "2020-10-26T03:45:31.842880Z",
     "iopub.status.idle": "2020-10-26T03:45:32.077097Z",
     "shell.execute_reply": "2020-10-26T03:45:32.076423Z"
    },
    "papermill": {
     "duration": 0.312266,
     "end_time": "2020-10-26T03:45:32.077235",
     "exception": false,
     "start_time": "2020-10-26T03:45:31.764969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pca = train.copy()\n",
    "test_pca = test.copy()\n",
    "\n",
    "train_pca.drop(g_cols, axis=1, inplace=True)\n",
    "test_pca.drop(g_cols, axis=1, inplace=True)\n",
    "\n",
    "train_pca.drop(c_cols, axis=1, inplace=True)\n",
    "test_pca.drop(c_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:32.227533Z",
     "iopub.status.busy": "2020-10-26T03:45:32.226543Z",
     "iopub.status.idle": "2020-10-26T03:45:32.263176Z",
     "shell.execute_reply": "2020-10-26T03:45:32.263953Z"
    },
    "papermill": {
     "duration": 0.116051,
     "end_time": "2020-10-26T03:45:32.264124",
     "exception": false,
     "start_time": "2020-10-26T03:45:32.148073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>pca_G-0</th>\n",
       "      <th>pca_G-1</th>\n",
       "      <th>pca_G-2</th>\n",
       "      <th>pca_G-3</th>\n",
       "      <th>pca_G-4</th>\n",
       "      <th>pca_G-5</th>\n",
       "      <th>pca_G-6</th>\n",
       "      <th>pca_G-7</th>\n",
       "      <th>...</th>\n",
       "      <th>gc_mean</th>\n",
       "      <th>g_std</th>\n",
       "      <th>c_std</th>\n",
       "      <th>gc_std</th>\n",
       "      <th>g_kurt</th>\n",
       "      <th>c_kurt</th>\n",
       "      <th>gc_kurt</th>\n",
       "      <th>g_skew</th>\n",
       "      <th>c_skew</th>\n",
       "      <th>gc_skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.803145</td>\n",
       "      <td>-6.318625</td>\n",
       "      <td>7.672392</td>\n",
       "      <td>-7.376133</td>\n",
       "      <td>4.607942</td>\n",
       "      <td>1.246979</td>\n",
       "      <td>3.471866</td>\n",
       "      <td>1.297148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057614</td>\n",
       "      <td>0.865619</td>\n",
       "      <td>0.730496</td>\n",
       "      <td>0.866639</td>\n",
       "      <td>-0.222353</td>\n",
       "      <td>-0.320226</td>\n",
       "      <td>-0.222313</td>\n",
       "      <td>-0.006447</td>\n",
       "      <td>0.072483</td>\n",
       "      <td>-0.038894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.054122</td>\n",
       "      <td>0.431677</td>\n",
       "      <td>-12.292988</td>\n",
       "      <td>4.753020</td>\n",
       "      <td>0.818093</td>\n",
       "      <td>0.085630</td>\n",
       "      <td>1.315260</td>\n",
       "      <td>-1.294847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059153</td>\n",
       "      <td>0.852443</td>\n",
       "      <td>0.607776</td>\n",
       "      <td>0.844414</td>\n",
       "      <td>-0.179074</td>\n",
       "      <td>0.083991</td>\n",
       "      <td>-0.202692</td>\n",
       "      <td>0.051917</td>\n",
       "      <td>-0.168964</td>\n",
       "      <td>-0.040794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.711322</td>\n",
       "      <td>8.874418</td>\n",
       "      <td>-1.903753</td>\n",
       "      <td>0.330580</td>\n",
       "      <td>0.633495</td>\n",
       "      <td>3.046700</td>\n",
       "      <td>-2.091665</td>\n",
       "      <td>3.629779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041415</td>\n",
       "      <td>0.940095</td>\n",
       "      <td>0.664359</td>\n",
       "      <td>0.911721</td>\n",
       "      <td>-0.348731</td>\n",
       "      <td>-0.188699</td>\n",
       "      <td>-0.274455</td>\n",
       "      <td>-0.030332</td>\n",
       "      <td>0.384022</td>\n",
       "      <td>0.005181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.953440</td>\n",
       "      <td>9.418785</td>\n",
       "      <td>0.074335</td>\n",
       "      <td>-4.867886</td>\n",
       "      <td>-7.236305</td>\n",
       "      <td>-2.899540</td>\n",
       "      <td>-1.928034</td>\n",
       "      <td>6.706572</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146895</td>\n",
       "      <td>1.077030</td>\n",
       "      <td>0.575324</td>\n",
       "      <td>1.083923</td>\n",
       "      <td>-0.915539</td>\n",
       "      <td>3.977177</td>\n",
       "      <td>-0.954259</td>\n",
       "      <td>0.089322</td>\n",
       "      <td>1.955443</td>\n",
       "      <td>0.255048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.860694</td>\n",
       "      <td>6.375191</td>\n",
       "      <td>-8.469342</td>\n",
       "      <td>-4.523437</td>\n",
       "      <td>-8.205764</td>\n",
       "      <td>-8.250662</td>\n",
       "      <td>-3.662988</td>\n",
       "      <td>-3.730783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023250</td>\n",
       "      <td>1.112844</td>\n",
       "      <td>0.676915</td>\n",
       "      <td>1.077820</td>\n",
       "      <td>-0.207397</td>\n",
       "      <td>-0.725125</td>\n",
       "      <td>-0.085380</td>\n",
       "      <td>-0.195793</td>\n",
       "      <td>0.076891</td>\n",
       "      <td>-0.263166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.671429</td>\n",
       "      <td>2.996342</td>\n",
       "      <td>2.277468</td>\n",
       "      <td>1.423697</td>\n",
       "      <td>-0.439910</td>\n",
       "      <td>1.659410</td>\n",
       "      <td>-3.136092</td>\n",
       "      <td>-1.076689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084741</td>\n",
       "      <td>0.682694</td>\n",
       "      <td>0.696628</td>\n",
       "      <td>0.684662</td>\n",
       "      <td>-0.146175</td>\n",
       "      <td>-0.377779</td>\n",
       "      <td>-0.150038</td>\n",
       "      <td>-0.064515</td>\n",
       "      <td>0.419003</td>\n",
       "      <td>-0.003303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.240921</td>\n",
       "      <td>5.974020</td>\n",
       "      <td>-0.937860</td>\n",
       "      <td>0.315171</td>\n",
       "      <td>1.250794</td>\n",
       "      <td>1.994066</td>\n",
       "      <td>2.336326</td>\n",
       "      <td>2.795934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053313</td>\n",
       "      <td>0.786501</td>\n",
       "      <td>0.729967</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.120454</td>\n",
       "      <td>-0.221594</td>\n",
       "      <td>0.162327</td>\n",
       "      <td>-0.200306</td>\n",
       "      <td>0.320651</td>\n",
       "      <td>-0.158667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.612794</td>\n",
       "      <td>-0.870594</td>\n",
       "      <td>4.202482</td>\n",
       "      <td>2.249155</td>\n",
       "      <td>0.144837</td>\n",
       "      <td>-3.366519</td>\n",
       "      <td>-2.021058</td>\n",
       "      <td>2.195657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022455</td>\n",
       "      <td>0.812636</td>\n",
       "      <td>0.850337</td>\n",
       "      <td>0.817323</td>\n",
       "      <td>0.089973</td>\n",
       "      <td>-0.212426</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>-0.010357</td>\n",
       "      <td>0.457180</td>\n",
       "      <td>0.054264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.208960</td>\n",
       "      <td>-14.826250</td>\n",
       "      <td>3.310835</td>\n",
       "      <td>-4.177547</td>\n",
       "      <td>-9.737452</td>\n",
       "      <td>6.406602</td>\n",
       "      <td>9.491045</td>\n",
       "      <td>-0.565421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143788</td>\n",
       "      <td>1.165174</td>\n",
       "      <td>0.800976</td>\n",
       "      <td>1.139215</td>\n",
       "      <td>-1.002151</td>\n",
       "      <td>-0.630398</td>\n",
       "      <td>-0.900820</td>\n",
       "      <td>-0.147141</td>\n",
       "      <td>0.107626</td>\n",
       "      <td>-0.204890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.680442</td>\n",
       "      <td>-2.566097</td>\n",
       "      <td>5.966806</td>\n",
       "      <td>-9.330424</td>\n",
       "      <td>7.187720</td>\n",
       "      <td>-1.153823</td>\n",
       "      <td>-2.235090</td>\n",
       "      <td>16.518001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194836</td>\n",
       "      <td>1.203292</td>\n",
       "      <td>0.347157</td>\n",
       "      <td>1.217116</td>\n",
       "      <td>-0.974193</td>\n",
       "      <td>6.118678</td>\n",
       "      <td>-1.048005</td>\n",
       "      <td>0.018163</td>\n",
       "      <td>2.104892</td>\n",
       "      <td>0.226085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time  cp_dose    pca_G-0    pca_G-1    pca_G-2   pca_G-3   pca_G-4  \\\n",
       "0            0        0  -5.803145  -6.318625   7.672392 -7.376133  4.607942   \n",
       "1            2        0  -5.054122   0.431677 -12.292988  4.753020  0.818093   \n",
       "2            1        0   0.711322   8.874418  -1.903753  0.330580  0.633495   \n",
       "3            1        0  10.953440   9.418785   0.074335 -4.867886 -7.236305   \n",
       "4            2        1  -6.860694   6.375191  -8.469342 -4.523437 -8.205764   \n",
       "...        ...      ...        ...        ...        ...       ...       ...   \n",
       "21943        2        0  -4.671429   2.996342   2.277468  1.423697 -0.439910   \n",
       "21944        0        1  -3.240921   5.974020  -0.937860  0.315171  1.250794   \n",
       "21945        0        1  -2.612794  -0.870594   4.202482  2.249155  0.144837   \n",
       "21946        0        0   4.208960 -14.826250   3.310835 -4.177547 -9.737452   \n",
       "21947        2        0   3.680442  -2.566097   5.966806 -9.330424  7.187720   \n",
       "\n",
       "        pca_G-5   pca_G-6    pca_G-7  ...   gc_mean     g_std     c_std  \\\n",
       "0      1.246979  3.471866   1.297148  ...  0.057614  0.865619  0.730496   \n",
       "1      0.085630  1.315260  -1.294847  ...  0.059153  0.852443  0.607776   \n",
       "2      3.046700 -2.091665   3.629779  ... -0.041415  0.940095  0.664359   \n",
       "3     -2.899540 -1.928034   6.706572  ... -0.146895  1.077030  0.575324   \n",
       "4     -8.250662 -3.662988  -3.730783  ...  0.023250  1.112844  0.676915   \n",
       "...         ...       ...        ...  ...       ...       ...       ...   \n",
       "21943  1.659410 -3.136092  -1.076689  ...  0.084741  0.682694  0.696628   \n",
       "21944  1.994066  2.336326   2.795934  ...  0.053313  0.786501  0.729967   \n",
       "21945 -3.366519 -2.021058   2.195657  ...  0.022455  0.812636  0.850337   \n",
       "21946  6.406602  9.491045  -0.565421  ...  0.143788  1.165174  0.800976   \n",
       "21947 -1.153823 -2.235090  16.518001  ... -0.194836  1.203292  0.347157   \n",
       "\n",
       "         gc_std    g_kurt    c_kurt   gc_kurt    g_skew    c_skew   gc_skew  \n",
       "0      0.866639 -0.222353 -0.320226 -0.222313 -0.006447  0.072483 -0.038894  \n",
       "1      0.844414 -0.179074  0.083991 -0.202692  0.051917 -0.168964 -0.040794  \n",
       "2      0.911721 -0.348731 -0.188699 -0.274455 -0.030332  0.384022  0.005181  \n",
       "3      1.083923 -0.915539  3.977177 -0.954259  0.089322  1.955443  0.255048  \n",
       "4      1.077820 -0.207397 -0.725125 -0.085380 -0.195793  0.076891 -0.263166  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "21943  0.684662 -0.146175 -0.377779 -0.150038 -0.064515  0.419003 -0.003303  \n",
       "21944  0.788491  0.120454 -0.221594  0.162327 -0.200306  0.320651 -0.158667  \n",
       "21945  0.817323  0.089973 -0.212426  0.070046 -0.010357  0.457180  0.054264  \n",
       "21946  1.139215 -1.002151 -0.630398 -0.900820 -0.147141  0.107626 -0.204890  \n",
       "21947  1.217116 -0.974193  6.118678 -1.048005  0.018163  2.104892  0.226085  \n",
       "\n",
       "[21948 rows x 107 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.110567,
     "end_time": "2020-10-26T03:45:32.448881",
     "exception": false,
     "start_time": "2020-10-26T03:45:32.338314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:32.591173Z",
     "iopub.status.busy": "2020-10-26T03:45:32.590151Z",
     "iopub.status.idle": "2020-10-26T03:45:32.592825Z",
     "shell.execute_reply": "2020-10-26T03:45:32.593915Z"
    },
    "papermill": {
     "duration": 0.077788,
     "end_time": "2020-10-26T03:45:32.594044",
     "exception": false,
     "start_time": "2020-10-26T03:45:32.516256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_simple_nn(num_col, output_dim):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            L.Input(num_col),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.2),\n",
    "            tfa.layers.WeightNormalization(L.Dense(2048, activation=\"relu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.5),\n",
    "            tfa.layers.WeightNormalization(L.Dense(1024, activation=\"relu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.5),\n",
    "            tfa.layers.WeightNormalization(L.Dense(output_dim, activation=\"sigmoid\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.066418,
     "end_time": "2020-10-26T03:45:32.725850",
     "exception": false,
     "start_time": "2020-10-26T03:45:32.659432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Model - Split Neural Network\n",
    "\n",
    "https://www.kaggle.com/gogo827jz/split-neural-network-approach-tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:32.877762Z",
     "iopub.status.busy": "2020-10-26T03:45:32.875822Z",
     "iopub.status.idle": "2020-10-26T03:45:32.878414Z",
     "shell.execute_reply": "2020-10-26T03:45:32.878909Z"
    },
    "papermill": {
     "duration": 0.087855,
     "end_time": "2020-10-26T03:45:32.879038",
     "exception": false,
     "start_time": "2020-10-26T03:45:32.791183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_split_nn(num_columns, hidden_units, dropout_rate, output_dim):\n",
    "\n",
    "    inp1 = L.Input(shape=(num_columns,))\n",
    "    x1 = L.BatchNormalization()(inp1)\n",
    "\n",
    "    for i, units in enumerate(hidden_units[0]):\n",
    "        x1 = tfa.layers.WeightNormalization(L.Dense(units, activation=\"elu\"))(x1)\n",
    "        x1 = L.Dropout(dropout_rate[0])(x1)\n",
    "        x1 = L.BatchNormalization()(x1)\n",
    "\n",
    "    inp2 = L.Input(shape=(num_columns,))\n",
    "    x2 = L.BatchNormalization()(inp2)\n",
    "\n",
    "    for i, units in enumerate(hidden_units[1]):\n",
    "        x2 = tfa.layers.WeightNormalization(L.Dense(units, activation=\"elu\"))(x2)\n",
    "        x2 = L.Dropout(dropout_rate[1])(x2)\n",
    "        x2 = L.BatchNormalization()(x2)\n",
    "\n",
    "    inp3 = L.Input(shape=(num_columns,))\n",
    "    x3 = L.BatchNormalization()(inp3)\n",
    "\n",
    "    for i, units in enumerate(hidden_units[2]):\n",
    "        x3 = tfa.layers.WeightNormalization(L.Dense(units, activation=\"elu\"))(x3)\n",
    "        x3 = L.Dropout(dropout_rate[2])(x3)\n",
    "        x3 = L.BatchNormalization()(x3)\n",
    "\n",
    "    x = L.Concatenate()([x1, x2, x3])\n",
    "    x = L.Dropout(dropout_rate[3])(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "\n",
    "    for units in hidden_units[3]:\n",
    "\n",
    "        x = tfa.layers.WeightNormalization(L.Dense(units, activation=\"elu\"))(x)\n",
    "        x = L.Dropout(dropout_rate[4])(x)\n",
    "        x = L.BatchNormalization()(x)\n",
    "\n",
    "    out = tfa.layers.WeightNormalization(L.Dense(output_dim, activation=\"sigmoid\"))(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[inp1, inp2, inp3], outputs=out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:33.019611Z",
     "iopub.status.busy": "2020-10-26T03:45:33.017698Z",
     "iopub.status.idle": "2020-10-26T03:45:33.020281Z",
     "shell.execute_reply": "2020-10-26T03:45:33.020912Z"
    },
    "papermill": {
     "duration": 0.075225,
     "end_time": "2020-10-26T03:45:33.021054",
     "exception": false,
     "start_time": "2020-10-26T03:45:32.945829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_split_nn(num_col, output_dim):\n",
    "    hidden_units = [[2048, 512], [1024, 1024], [1024, 512], [512, 512]]\n",
    "    dropout_rate = [0.4, 0.35, 0.3, 0.3, 0.2]\n",
    "    size = int(np.ceil(0.8 * num_col))\n",
    "\n",
    "    model = create_split_nn(size, hidden_units, dropout_rate, output_dim)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.065859,
     "end_time": "2020-10-26T03:45:33.153702",
     "exception": false,
     "start_time": "2020-10-26T03:45:33.087843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Model - Multi input ResNet\n",
    "\n",
    "https://www.kaggle.com/rahulsd91/moa-multi-input-resnet-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:33.308608Z",
     "iopub.status.busy": "2020-10-26T03:45:33.306916Z",
     "iopub.status.idle": "2020-10-26T03:45:33.309453Z",
     "shell.execute_reply": "2020-10-26T03:45:33.309956Z"
    },
    "papermill": {
     "duration": 0.08984,
     "end_time": "2020-10-26T03:45:33.310078",
     "exception": false,
     "start_time": "2020-10-26T03:45:33.220238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_resnet(n_features, n_features_2, n_labels):\n",
    "    input_1 = L.Input(shape=(n_features,), name=\"Input1\")\n",
    "    input_2 = L.Input(shape=(n_features_2,), name=\"Input2\")\n",
    "\n",
    "    head_1 = tf.keras.Sequential(\n",
    "        [\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.2),\n",
    "            tfa.layers.WeightNormalization(L.Dense(512, activation=\"elu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.2),\n",
    "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"elu\")),\n",
    "        ],\n",
    "        name=\"Head1\",\n",
    "    )\n",
    "\n",
    "    input_3 = head_1(input_1)\n",
    "    input_3_concat = L.Concatenate()([input_2, input_3])\n",
    "\n",
    "    head_2 = tf.keras.Sequential(\n",
    "        [\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.3),\n",
    "            tfa.layers.WeightNormalization(L.Dense(512, activation=\"relu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.2),\n",
    "            tfa.layers.WeightNormalization(L.Dense(512, activation=\"elu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.2),\n",
    "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"relu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.2),\n",
    "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"elu\")),\n",
    "        ],\n",
    "        name=\"Head2\",\n",
    "    )\n",
    "\n",
    "    input_4 = head_2(input_3_concat)\n",
    "    input_4_avg = L.Average()([input_3, input_4])\n",
    "\n",
    "    head_3 = tf.keras.Sequential(\n",
    "        [\n",
    "            L.BatchNormalization(),\n",
    "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"selu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dropout(0.2),\n",
    "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"selu\")),\n",
    "            L.BatchNormalization(),\n",
    "            L.Dense(n_labels, activation=\"sigmoid\"),\n",
    "        ],\n",
    "        name=\"Head3\",\n",
    "    )\n",
    "\n",
    "    output = head_3(input_4_avg)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.066919,
     "end_time": "2020-10-26T03:45:33.443510",
     "exception": false,
     "start_time": "2020-10-26T03:45:33.376591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Model - NODE\n",
    "\n",
    "Neural Oblivious Decision Ensembles\n",
    "\n",
    "https://www.kaggle.com/gogo827jz/moa-neural-oblivious-decision-ensembles-tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:33.583637Z",
     "iopub.status.busy": "2020-10-26T03:45:33.582701Z",
     "iopub.status.idle": "2020-10-26T03:45:33.585366Z",
     "shell.execute_reply": "2020-10-26T03:45:33.585890Z"
    },
    "papermill": {
     "duration": 0.074383,
     "end_time": "2020-10-26T03:45:33.586021",
     "exception": false,
     "start_time": "2020-10-26T03:45:33.511638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sparsemoid(inputs: tf.Tensor):\n",
    "    return tf.clip_by_value(0.5 * inputs + 0.5, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:33.725365Z",
     "iopub.status.busy": "2020-10-26T03:45:33.723520Z",
     "iopub.status.idle": "2020-10-26T03:45:33.726063Z",
     "shell.execute_reply": "2020-10-26T03:45:33.726587Z"
    },
    "papermill": {
     "duration": 0.076595,
     "end_time": "2020-10-26T03:45:33.726716",
     "exception": false,
     "start_time": "2020-10-26T03:45:33.650121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def identity(x: tf.Tensor):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:33.893255Z",
     "iopub.status.busy": "2020-10-26T03:45:33.873764Z",
     "iopub.status.idle": "2020-10-26T03:45:33.896236Z",
     "shell.execute_reply": "2020-10-26T03:45:33.895560Z"
    },
    "papermill": {
     "duration": 0.102775,
     "end_time": "2020-10-26T03:45:33.896338",
     "exception": false,
     "start_time": "2020-10-26T03:45:33.793563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ODST(L.Layer):\n",
    "    def __init__(self, n_trees: int = 3, depth: int = 4, units: int = 1, threshold_init_beta: float = 1.0):\n",
    "        super(ODST, self).__init__()\n",
    "        self.initialized = False\n",
    "        self.n_trees = n_trees\n",
    "        self.depth = depth\n",
    "        self.units = units\n",
    "        self.threshold_init_beta = threshold_init_beta\n",
    "\n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        feature_selection_logits_init = tf.zeros_initializer()\n",
    "        self.feature_selection_logits = tf.Variable(\n",
    "            initial_value=feature_selection_logits_init(\n",
    "                shape=(input_shape[-1], self.n_trees, self.depth), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=\"feature_selection_logits\",\n",
    "        )\n",
    "\n",
    "        feature_thresholds_init = tf.zeros_initializer()\n",
    "        self.feature_thresholds = tf.Variable(\n",
    "            initial_value=feature_thresholds_init(shape=(self.n_trees, self.depth), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "            name=\"feature_thresholds\",\n",
    "        )\n",
    "\n",
    "        log_temperatures_init = tf.ones_initializer()\n",
    "        self.log_temperatures = tf.Variable(\n",
    "            initial_value=log_temperatures_init(shape=(self.n_trees, self.depth), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "            name=\"log_temperatures\",\n",
    "        )\n",
    "\n",
    "        indices = K.arange(0, 2 ** self.depth, 1)\n",
    "        offsets = 2 ** K.arange(0, self.depth, 1)\n",
    "        bin_codes = tf.reshape(indices, (1, -1)) // tf.reshape(offsets, (-1, 1)) % 2\n",
    "        bin_codes_1hot = tf.stack([bin_codes, 1 - bin_codes], axis=-1)\n",
    "        self.bin_codes_1hot = tf.Variable(\n",
    "            initial_value=tf.cast(bin_codes_1hot, \"float32\"), trainable=False, name=\"bin_codes_1hot\"\n",
    "        )\n",
    "\n",
    "        response_init = tf.ones_initializer()\n",
    "        self.response = tf.Variable(\n",
    "            initial_value=response_init(shape=(self.n_trees, self.units, 2 ** self.depth), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "            name=\"response\",\n",
    "        )\n",
    "\n",
    "    def initialize(self, inputs):\n",
    "        feature_values = self.feature_values(inputs)\n",
    "\n",
    "        # intialize feature_thresholds\n",
    "        percentiles_q = 100 * tfp.distributions.Beta(self.threshold_init_beta, self.threshold_init_beta).sample(\n",
    "            [self.n_trees * self.depth]\n",
    "        )\n",
    "        flattened_feature_values = tf.map_fn(K.flatten, feature_values)\n",
    "        init_feature_thresholds = tf.linalg.diag_part(\n",
    "            tfp.stats.percentile(flattened_feature_values, percentiles_q, axis=0)\n",
    "        )\n",
    "\n",
    "        self.feature_thresholds.assign(tf.reshape(init_feature_thresholds, self.feature_thresholds.shape))\n",
    "\n",
    "        # intialize log_temperatures\n",
    "        self.log_temperatures.assign(\n",
    "            tfp.stats.percentile(tf.math.abs(feature_values - self.feature_thresholds), 50, axis=0)\n",
    "        )\n",
    "\n",
    "    def feature_values(self, inputs: tf.Tensor, training: bool = None):\n",
    "        feature_selectors = tfa.activations.sparsemax(self.feature_selection_logits)\n",
    "        # ^--[in_features, n_trees, depth]\n",
    "\n",
    "        feature_values = tf.einsum(\"bi,ind->bnd\", inputs, feature_selectors)\n",
    "        # ^--[batch_size, n_trees, depth]\n",
    "\n",
    "        return feature_values\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, training: bool = None):\n",
    "        if not self.initialized:\n",
    "            self.initialize(inputs)\n",
    "            self.initialized = True\n",
    "\n",
    "        feature_values = self.feature_values(inputs)\n",
    "\n",
    "        threshold_logits_a = (feature_values - self.feature_thresholds) * tf.math.exp(-self.log_temperatures)\n",
    "\n",
    "        threshold_logits_b = tf.stack([-threshold_logits_a, threshold_logits_a], axis=-1)\n",
    "        # ^--[batch_size, n_trees, depth, 2]\n",
    "\n",
    "        bins = sparsemoid(threshold_logits_b)\n",
    "        # ^--[batch_size, n_trees, depth, 2], approximately binary\n",
    "\n",
    "        bin_matches = tf.einsum(\"btds,dcs->btdc\", bins, self.bin_codes_1hot)\n",
    "        # ^--[batch_size, n_trees, depth, 2 ** depth]\n",
    "\n",
    "        response_weights = tf.math.reduce_prod(bin_matches, axis=-2)\n",
    "        # ^-- [batch_size, n_trees, 2 ** depth]\n",
    "\n",
    "        response = tf.einsum(\"bnd,ncd->bnc\", response_weights, self.response)\n",
    "        # ^-- [batch_size, n_trees, units]\n",
    "\n",
    "        return tf.reduce_sum(response, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:34.052179Z",
     "iopub.status.busy": "2020-10-26T03:45:34.051185Z",
     "iopub.status.idle": "2020-10-26T03:45:34.053637Z",
     "shell.execute_reply": "2020-10-26T03:45:34.054173Z"
    },
    "papermill": {
     "duration": 0.090779,
     "end_time": "2020-10-26T03:45:34.054288",
     "exception": false,
     "start_time": "2020-10-26T03:45:33.963509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NODE(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units: int = 1,\n",
    "        n_layers: int = 1,\n",
    "        output_dim=1,\n",
    "        dropout_rate=0.1,\n",
    "        link: tf.function = tf.identity,\n",
    "        n_trees: int = 3,\n",
    "        depth: int = 4,\n",
    "        threshold_init_beta: float = 1.0,\n",
    "        feature_column: Optional[L.DenseFeatures] = None,\n",
    "    ):\n",
    "        super(NODE, self).__init__()\n",
    "        self.units = units\n",
    "        self.n_layers = n_layers\n",
    "        self.n_trees = n_trees\n",
    "        self.depth = depth\n",
    "        self.units = units\n",
    "        self.threshold_init_beta = threshold_init_beta\n",
    "        self.feature_column = feature_column\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if feature_column is None:\n",
    "            self.feature = L.Lambda(identity)\n",
    "        else:\n",
    "            self.feature = feature_column\n",
    "\n",
    "        self.bn = [L.BatchNormalization() for _ in range(n_layers + 1)]\n",
    "        self.dropout = [L.Dropout(self.dropout_rate) for _ in range(n_layers + 1)]\n",
    "        self.ensemble = [\n",
    "            ODST(n_trees=n_trees, depth=depth, units=units, threshold_init_beta=threshold_init_beta)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "        self.last_layer = L.Dense(self.output_dim)\n",
    "\n",
    "        self.link = link\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        X_a = self.feature(inputs)\n",
    "        X_b = self.bn[0](X_a, training=training)\n",
    "        X_c = self.dropout[0](X_b, training=training)\n",
    "\n",
    "        X = defaultdict(dict)\n",
    "        X[0][0] = X_c\n",
    "        for i, tree in enumerate(self.ensemble):\n",
    "            X[i][1] = tf.concat([X[i][0], tree(X[i][0])], axis=1)\n",
    "            X[i][2] = self.bn[i + 1](X[i][1], training=training)\n",
    "            X[i + 1][0] = self.dropout[i + 1](X[i][2], training=training)\n",
    "\n",
    "        return self.link(self.last_layer(X[i + 1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:34.198907Z",
     "iopub.status.busy": "2020-10-26T03:45:34.197967Z",
     "iopub.status.idle": "2020-10-26T03:45:34.201261Z",
     "shell.execute_reply": "2020-10-26T03:45:34.200672Z"
    },
    "papermill": {
     "duration": 0.080596,
     "end_time": "2020-10-26T03:45:34.201365",
     "exception": false,
     "start_time": "2020-10-26T03:45:34.120769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_node(output_dim):\n",
    "    model = NODE(\n",
    "        n_layers=3,\n",
    "        units=128,\n",
    "        output_dim=output_dim,\n",
    "        dropout_rate=0.1,\n",
    "        depth=6,\n",
    "        n_trees=3,\n",
    "        link=tf.keras.activations.sigmoid,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.067171,
     "end_time": "2020-10-26T03:45:34.337055",
     "exception": false,
     "start_time": "2020-10-26T03:45:34.269884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Model - TabNet for MultiLabel\n",
    "\n",
    "https://www.kaggle.com/gogo827jz/moa-stacked-tabnet-baseline-tensorflow-2-0#Model-Functions\n",
    "\n",
    "TabNetとは。\n",
    "https://cloud.google.com/blog/ja/products/ai-machine-learning/ml-model-tabnet-is-easy-to-use-on-cloud-ai-platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:34.478515Z",
     "iopub.status.busy": "2020-10-26T03:45:34.477496Z",
     "iopub.status.idle": "2020-10-26T03:45:34.480113Z",
     "shell.execute_reply": "2020-10-26T03:45:34.480785Z"
    },
    "papermill": {
     "duration": 0.075282,
     "end_time": "2020-10-26T03:45:34.480921",
     "exception": false,
     "start_time": "2020-10-26T03:45:34.405639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def register_keras_custom_object(cls):\n",
    "    tf.keras.utils.get_custom_objects()[cls.__name__] = cls\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:34.625229Z",
     "iopub.status.busy": "2020-10-26T03:45:34.624420Z",
     "iopub.status.idle": "2020-10-26T03:45:34.627575Z",
     "shell.execute_reply": "2020-10-26T03:45:34.627081Z"
    },
    "papermill": {
     "duration": 0.07618,
     "end_time": "2020-10-26T03:45:34.627675",
     "exception": false,
     "start_time": "2020-10-26T03:45:34.551495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def glu(x, n_units=None):\n",
    "    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n",
    "    if n_units is None:\n",
    "        n_units = tf.shape(x)[-1] // 2\n",
    "\n",
    "    return x[..., :n_units] * tf.nn.sigmoid(x[..., n_units:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:34.775812Z",
     "iopub.status.busy": "2020-10-26T03:45:34.774905Z",
     "iopub.status.idle": "2020-10-26T03:45:34.778410Z",
     "shell.execute_reply": "2020-10-26T03:45:34.777873Z"
    },
    "papermill": {
     "duration": 0.081114,
     "end_time": "2020-10-26T03:45:34.778514",
     "exception": false,
     "start_time": "2020-10-26T03:45:34.697400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/activations/sparsemax.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@register_keras_custom_object\n",
    "@tf.function\n",
    "def sparsemax(logits, axis):\n",
    "    \"\"\"Sparsemax activation function [1].\n",
    "    For each batch `i` and class `j` we have\n",
    "      $$sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0)$$\n",
    "    [1]: https://arxiv.org/abs/1602.02068\n",
    "    Args:\n",
    "        logits: Input tensor.\n",
    "        axis: Integer, axis along which the sparsemax operation is applied.\n",
    "    Returns:\n",
    "        Tensor, output of sparsemax transformation. Has the same type and\n",
    "        shape as `logits`.\n",
    "    Raises:\n",
    "        ValueError: In case `dim(logits) == 1`.\n",
    "    \"\"\"\n",
    "    logits = tf.convert_to_tensor(logits, name=\"logits\")\n",
    "\n",
    "    # We need its original shape for shape inference.\n",
    "    shape = logits.get_shape()\n",
    "    rank = shape.rank\n",
    "    is_last_axis = (axis == -1) or (axis == rank - 1)\n",
    "\n",
    "    if is_last_axis:\n",
    "        output = _compute_2d_sparsemax(logits)\n",
    "        output.set_shape(shape)\n",
    "        return output\n",
    "\n",
    "    # If dim is not the last dimension, we have to do a transpose so that we can\n",
    "    # still perform softmax on its last dimension.\n",
    "\n",
    "    # Swap logits' dimension of dim and its last dimension.\n",
    "    rank_op = tf.rank(logits)\n",
    "    axis_norm = axis % rank\n",
    "    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n",
    "\n",
    "    # Do the actual softmax on its last dimension.\n",
    "    output = _compute_2d_sparsemax(logits)\n",
    "    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n",
    "\n",
    "    # Make shape inference work since transpose may erase its static shape.\n",
    "    output.set_shape(shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:34.922194Z",
     "iopub.status.busy": "2020-10-26T03:45:34.921232Z",
     "iopub.status.idle": "2020-10-26T03:45:34.924181Z",
     "shell.execute_reply": "2020-10-26T03:45:34.923684Z"
    },
    "papermill": {
     "duration": 0.078072,
     "end_time": "2020-10-26T03:45:34.924283",
     "exception": false,
     "start_time": "2020-10-26T03:45:34.846211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _swap_axis(logits, dim_index, last_index, **kwargs):\n",
    "    return tf.transpose(\n",
    "        logits,\n",
    "        tf.concat(\n",
    "            [\n",
    "                tf.range(dim_index),\n",
    "                [last_index],\n",
    "                tf.range(dim_index + 1, last_index),\n",
    "                [dim_index],\n",
    "            ],\n",
    "            0,\n",
    "        ),\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:35.077126Z",
     "iopub.status.busy": "2020-10-26T03:45:35.076193Z",
     "iopub.status.idle": "2020-10-26T03:45:35.079254Z",
     "shell.execute_reply": "2020-10-26T03:45:35.078760Z"
    },
    "papermill": {
     "duration": 0.087111,
     "end_time": "2020-10-26T03:45:35.079356",
     "exception": false,
     "start_time": "2020-10-26T03:45:34.992245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _compute_2d_sparsemax(logits):\n",
    "    \"\"\"Performs the sparsemax operation when axis=-1.\"\"\"\n",
    "    shape_op = tf.shape(logits)\n",
    "    obs = tf.math.reduce_prod(shape_op[:-1])\n",
    "    dims = shape_op[-1]\n",
    "\n",
    "    # In the paper, they call the logits z.\n",
    "    # The mean(logits) can be substracted from logits to make the algorithm\n",
    "    # more numerically stable. the instability in this algorithm comes mostly\n",
    "    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n",
    "    # to zero. However, in practise the numerical instability issues are very\n",
    "    # minor and substacting the mean causes extra issues with inf and nan\n",
    "    # input.\n",
    "    # Reshape to [obs, dims] as it is almost free and means the remanining\n",
    "    # code doesn't need to worry about the rank.\n",
    "    z = tf.reshape(logits, [obs, dims])\n",
    "\n",
    "    # sort z\n",
    "    z_sorted, _ = tf.nn.top_k(z, k=dims)\n",
    "\n",
    "    # calculate k(z)\n",
    "    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n",
    "    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n",
    "    z_check = 1 + k * z_sorted > z_cumsum\n",
    "    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n",
    "    # (index + 1) of the last `1` is the same as just summing the number of 1.\n",
    "    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n",
    "\n",
    "    # calculate tau(z)\n",
    "    # If there are inf values or all values are -inf, the k_z will be zero,\n",
    "    # this is mathematically invalid and will also cause the gather_nd to fail.\n",
    "    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n",
    "    # fixed later (see p_safe) by returning p = nan. This results in the same\n",
    "    # behavior as softmax.\n",
    "    k_z_safe = tf.math.maximum(k_z, 1)\n",
    "    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n",
    "    tau_sum = tf.gather_nd(z_cumsum, indices)\n",
    "    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n",
    "\n",
    "    # calculate p\n",
    "    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n",
    "    # If k_z = 0 or if z = nan, then the input is invalid\n",
    "    p_safe = tf.where(\n",
    "        tf.expand_dims(\n",
    "            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n",
    "        p,\n",
    "    )\n",
    "\n",
    "    # Reshape back to original size\n",
    "    p_safe = tf.reshape(p_safe, shape_op)\n",
    "    return p_safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:35.261533Z",
     "iopub.status.busy": "2020-10-26T03:45:35.250905Z",
     "iopub.status.idle": "2020-10-26T03:45:35.273634Z",
     "shell.execute_reply": "2020-10-26T03:45:35.273049Z"
    },
    "papermill": {
     "duration": 0.126889,
     "end_time": "2020-10-26T03:45:35.273743",
     "exception": false,
     "start_time": "2020-10-26T03:45:35.146854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/normalizations.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@register_keras_custom_object\n",
    "class GroupNormalization(L.Layer):\n",
    "    \"\"\"Group normalization layer.\n",
    "    Group Normalization divides the channels into groups and computes\n",
    "    within each group the mean and variance for normalization.\n",
    "    Empirically, its accuracy is more stable than batch norm in a wide\n",
    "    range of small batch sizes, if learning rate is adjusted linearly\n",
    "    with batch sizes.\n",
    "    Relation to Layer Normalization:\n",
    "    If the number of groups is set to 1, then this operation becomes identical\n",
    "    to Layer Normalization.\n",
    "    Relation to Instance Normalization:\n",
    "    If the number of groups is set to the\n",
    "    input dimension (number of groups is equal\n",
    "    to number of channels), then this operation becomes\n",
    "    identical to Instance Normalization.\n",
    "    Arguments\n",
    "        groups: Integer, the number of groups for Group Normalization.\n",
    "            Can be in the range [1, N] where N is the input dimension.\n",
    "            The input dimension must be divisible by the number of groups.\n",
    "        axis: Integer, the axis that should be normalized.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    Output shape\n",
    "        Same shape as input.\n",
    "    References\n",
    "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        groups: int = 2,\n",
    "        axis: int = -1,\n",
    "        epsilon: float = 1e-3,\n",
    "        center: bool = True,\n",
    "        scale: bool = True,\n",
    "        beta_initializer=\"zeros\",\n",
    "        gamma_initializer=\"ones\",\n",
    "        beta_regularizer=None,\n",
    "        gamma_regularizer=None,\n",
    "        beta_constraint=None,\n",
    "        gamma_constraint=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.groups = groups\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n",
    "        self._check_axis()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self._check_if_input_shape_is_none(input_shape)\n",
    "        self._set_number_of_groups_for_instance_norm(input_shape)\n",
    "        self._check_size_of_dimensions(input_shape)\n",
    "        self._create_input_spec(input_shape)\n",
    "\n",
    "        self._add_gamma_weight(input_shape)\n",
    "        self._add_beta_weight(input_shape)\n",
    "        self.built = True\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Training=none is just for compat with batchnorm signature call\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        tensor_input_shape = tf.shape(inputs)\n",
    "\n",
    "        reshaped_inputs, group_shape = self._reshape_into_groups(inputs, input_shape, tensor_input_shape)\n",
    "\n",
    "        normalized_inputs = self._apply_normalization(reshaped_inputs, input_shape)\n",
    "\n",
    "        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"groups\": self.groups,\n",
    "            \"axis\": self.axis,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"center\": self.center,\n",
    "            \"scale\": self.scale,\n",
    "            \"beta_initializer\": tf.keras.initializers.serialize(self.beta_initializer),\n",
    "            \"gamma_initializer\": tf.keras.initializers.serialize(self.gamma_initializer),\n",
    "            \"beta_regularizer\": tf.keras.regularizers.serialize(self.beta_regularizer),\n",
    "            \"gamma_regularizer\": tf.keras.regularizers.serialize(self.gamma_regularizer),\n",
    "            \"beta_constraint\": tf.keras.constraints.serialize(self.beta_constraint),\n",
    "            \"gamma_constraint\": tf.keras.constraints.serialize(self.gamma_constraint),\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n",
    "\n",
    "        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n",
    "        group_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        group_shape.insert(self.axis, self.groups)\n",
    "        group_shape = tf.stack(group_shape)\n",
    "        reshaped_inputs = tf.reshape(inputs, group_shape)\n",
    "        return reshaped_inputs, group_shape\n",
    "\n",
    "    def _apply_normalization(self, reshaped_inputs, input_shape):\n",
    "\n",
    "        group_shape = K.int_shape(reshaped_inputs)\n",
    "        group_reduction_axes = list(range(1, len(group_shape)))\n",
    "        axis = -2 if self.axis == -1 else self.axis - 1\n",
    "        group_reduction_axes.pop(axis)\n",
    "\n",
    "        mean, variance = tf.nn.moments(reshaped_inputs, group_reduction_axes, keepdims=True)\n",
    "\n",
    "        gamma, beta = self._get_reshaped_weights(input_shape)\n",
    "        normalized_inputs = tf.nn.batch_normalization(\n",
    "            reshaped_inputs,\n",
    "            mean=mean,\n",
    "            variance=variance,\n",
    "            scale=gamma,\n",
    "            offset=beta,\n",
    "            variance_epsilon=self.epsilon,\n",
    "        )\n",
    "        return normalized_inputs\n",
    "\n",
    "    def _get_reshaped_weights(self, input_shape):\n",
    "        broadcast_shape = self._create_broadcast_shape(input_shape)\n",
    "        gamma = None\n",
    "        beta = None\n",
    "        if self.scale:\n",
    "            gamma = tf.reshape(self.gamma, broadcast_shape)\n",
    "\n",
    "        if self.center:\n",
    "            beta = tf.reshape(self.beta, broadcast_shape)\n",
    "        return gamma, beta\n",
    "\n",
    "    def _check_if_input_shape_is_none(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim is None:\n",
    "            raise ValueError(\n",
    "                \"Axis \" + str(self.axis) + \" of \"\n",
    "                \"input tensor should have a defined dimension \"\n",
    "                \"but the layer received an input with shape \" + str(input_shape) + \".\"\n",
    "            )\n",
    "\n",
    "    def _set_number_of_groups_for_instance_norm(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "\n",
    "        if self.groups == -1:\n",
    "            self.groups = dim\n",
    "\n",
    "    def _check_size_of_dimensions(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim < self.groups:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") cannot be \"\n",
    "                \"more than the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") must be a \"\n",
    "                \"multiple of the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "    def _check_axis(self):\n",
    "\n",
    "        if self.axis == 0:\n",
    "            raise ValueError(\n",
    "                \"You are trying to normalize your batch axis. Do you want to \"\n",
    "                \"use tf.layer.batch_normalization instead\"\n",
    "            )\n",
    "\n",
    "    def _create_input_spec(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        self.input_spec = L.InputSpec(ndim=len(input_shape), axes={self.axis: dim})\n",
    "\n",
    "    def _add_gamma_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"gamma\",\n",
    "                initializer=self.gamma_initializer,\n",
    "                regularizer=self.gamma_regularizer,\n",
    "                constraint=self.gamma_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "    def _add_beta_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"beta\",\n",
    "                initializer=self.beta_initializer,\n",
    "                regularizer=self.beta_regularizer,\n",
    "                constraint=self.beta_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.beta = None\n",
    "\n",
    "    def _create_broadcast_shape(self, input_shape):\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        broadcast_shape.insert(self.axis, self.groups)\n",
    "        return broadcast_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:35.424750Z",
     "iopub.status.busy": "2020-10-26T03:45:35.423761Z",
     "iopub.status.idle": "2020-10-26T03:45:35.426964Z",
     "shell.execute_reply": "2020-10-26T03:45:35.426469Z"
    },
    "papermill": {
     "duration": 0.083905,
     "end_time": "2020-10-26T03:45:35.427083",
     "exception": false,
     "start_time": "2020-10-26T03:45:35.343178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformBlock(tf.keras.Model):\n",
    "    def __init__(self, features, norm_type, momentum=0.9, virtual_batch_size=None, groups=2, block_name=\"\", **kwargs):\n",
    "        super(TransformBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.features = features\n",
    "        self.norm_type = norm_type\n",
    "        self.momentum = momentum\n",
    "        self.groups = groups\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "\n",
    "        self.transform = L.Dense(self.features, use_bias=False, name=f\"transformblock_dense_{block_name}\")\n",
    "\n",
    "        if norm_type == \"batch\":\n",
    "            self.bn = L.BatchNormalization(\n",
    "                axis=-1,\n",
    "                momentum=momentum,\n",
    "                virtual_batch_size=virtual_batch_size,\n",
    "                name=f\"transformblock_bn_{block_name}\",\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.bn = GroupNormalization(axis=-1, groups=self.groups, name=f\"transformblock_gn_{block_name}\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.transform(inputs)\n",
    "        x = self.bn(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:35.620466Z",
     "iopub.status.busy": "2020-10-26T03:45:35.599488Z",
     "iopub.status.idle": "2020-10-26T03:45:35.646325Z",
     "shell.execute_reply": "2020-10-26T03:45:35.645761Z"
    },
    "papermill": {
     "duration": 0.145924,
     "end_time": "2020-10-26T03:45:35.646487",
     "exception": false,
     "start_time": "2020-10-26T03:45:35.500563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TabNet(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNet, self).__init__(**kwargs)\n",
    "\n",
    "        # Input checks\n",
    "        if feature_columns is not None:\n",
    "            if type(feature_columns) not in (list, tuple):\n",
    "                raise ValueError(\"`feature_columns` must be a list or a tuple.\")\n",
    "\n",
    "            if len(feature_columns) == 0:\n",
    "                raise ValueError(\"`feature_columns` must be contain at least 1 tf.feature_column !\")\n",
    "\n",
    "            if num_features is None:\n",
    "                num_features = len(feature_columns)\n",
    "            else:\n",
    "                num_features = int(num_features)\n",
    "\n",
    "        else:\n",
    "            if num_features is None:\n",
    "                raise ValueError(\"If `feature_columns` is None, then `num_features` cannot be None.\")\n",
    "\n",
    "        if num_decision_steps < 1:\n",
    "            raise ValueError(\"Num decision steps must be greater than 0.\")\n",
    "\n",
    "        if feature_dim < output_dim:\n",
    "            raise ValueError(\"To compute `features_for_coef`, feature_dim must be larger than output dim\")\n",
    "\n",
    "        feature_dim = int(feature_dim)\n",
    "        output_dim = int(output_dim)\n",
    "        num_decision_steps = int(num_decision_steps)\n",
    "        relaxation_factor = float(relaxation_factor)\n",
    "        sparsity_coefficient = float(sparsity_coefficient)\n",
    "        batch_momentum = float(batch_momentum)\n",
    "        num_groups = max(1, int(num_groups))\n",
    "        epsilon = float(epsilon)\n",
    "\n",
    "        if relaxation_factor < 0.0:\n",
    "            raise ValueError(\"`relaxation_factor` cannot be negative !\")\n",
    "\n",
    "        if sparsity_coefficient < 0.0:\n",
    "            raise ValueError(\"`sparsity_coefficient` cannot be negative !\")\n",
    "\n",
    "        if virtual_batch_size is not None:\n",
    "            virtual_batch_size = int(virtual_batch_size)\n",
    "\n",
    "        if norm_type not in [\"batch\", \"group\"]:\n",
    "            raise ValueError(\"`norm_type` must be either `batch` or `group`\")\n",
    "\n",
    "        self.feature_columns = feature_columns\n",
    "        self.num_features = num_features\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.num_decision_steps = num_decision_steps\n",
    "        self.relaxation_factor = relaxation_factor\n",
    "        self.sparsity_coefficient = sparsity_coefficient\n",
    "        self.norm_type = norm_type\n",
    "        self.batch_momentum = batch_momentum\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.num_groups = num_groups\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # if num_decision_steps > 1:\n",
    "        # features_for_coeff = feature_dim - output_dim\n",
    "        # print(f\"[TabNet]: {features_for_coeff} features will be used for decision steps.\")\n",
    "\n",
    "        if self.feature_columns is not None:\n",
    "            self.input_features = L.DenseFeatures(feature_columns, trainable=True)\n",
    "\n",
    "            if self.norm_type == \"batch\":\n",
    "                self.input_bn = L.BatchNormalization(axis=-1, momentum=batch_momentum, name=\"input_bn\")\n",
    "            else:\n",
    "                self.input_bn = GroupNormalization(axis=-1, groups=self.num_groups, name=\"input_gn\")\n",
    "\n",
    "        else:\n",
    "            self.input_features = None\n",
    "            self.input_bn = None\n",
    "\n",
    "        self.transform_f1 = TransformBlock(\n",
    "            2 * self.feature_dim,\n",
    "            self.norm_type,\n",
    "            self.batch_momentum,\n",
    "            self.virtual_batch_size,\n",
    "            self.num_groups,\n",
    "            block_name=\"f1\",\n",
    "        )\n",
    "\n",
    "        self.transform_f2 = TransformBlock(\n",
    "            2 * self.feature_dim,\n",
    "            self.norm_type,\n",
    "            self.batch_momentum,\n",
    "            self.virtual_batch_size,\n",
    "            self.num_groups,\n",
    "            block_name=\"f2\",\n",
    "        )\n",
    "\n",
    "        self.transform_f3_list = [\n",
    "            TransformBlock(\n",
    "                2 * self.feature_dim,\n",
    "                self.norm_type,\n",
    "                self.batch_momentum,\n",
    "                self.virtual_batch_size,\n",
    "                self.num_groups,\n",
    "                block_name=f\"f3_{i}\",\n",
    "            )\n",
    "            for i in range(self.num_decision_steps)\n",
    "        ]\n",
    "\n",
    "        self.transform_f4_list = [\n",
    "            TransformBlock(\n",
    "                2 * self.feature_dim,\n",
    "                self.norm_type,\n",
    "                self.batch_momentum,\n",
    "                self.virtual_batch_size,\n",
    "                self.num_groups,\n",
    "                block_name=f\"f4_{i}\",\n",
    "            )\n",
    "            for i in range(self.num_decision_steps)\n",
    "        ]\n",
    "\n",
    "        self.transform_coef_list = [\n",
    "            TransformBlock(\n",
    "                self.num_features,\n",
    "                self.norm_type,\n",
    "                self.batch_momentum,\n",
    "                self.virtual_batch_size,\n",
    "                self.num_groups,\n",
    "                block_name=f\"coef_{i}\",\n",
    "            )\n",
    "            for i in range(self.num_decision_steps - 1)\n",
    "        ]\n",
    "\n",
    "        self._step_feature_selection_masks = None\n",
    "        self._step_aggregate_feature_selection_mask = None\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if self.input_features is not None:\n",
    "            features = self.input_features(inputs)\n",
    "            features = self.input_bn(features, training=training)\n",
    "\n",
    "        else:\n",
    "            features = inputs\n",
    "\n",
    "        batch_size = tf.shape(features)[0]\n",
    "        self._step_feature_selection_masks = []\n",
    "        self._step_aggregate_feature_selection_mask = None\n",
    "\n",
    "        # Initializes decision-step dependent variables.\n",
    "        output_aggregated = tf.zeros([batch_size, self.output_dim])\n",
    "        masked_features = features\n",
    "        mask_values = tf.zeros([batch_size, self.num_features])\n",
    "        aggregated_mask_values = tf.zeros([batch_size, self.num_features])\n",
    "        complementary_aggregated_mask_values = tf.ones([batch_size, self.num_features])\n",
    "\n",
    "        total_entropy = 0.0\n",
    "        entropy_loss = 0.0\n",
    "\n",
    "        for ni in range(self.num_decision_steps):\n",
    "            # Feature transformer with two shared and two decision step dependent\n",
    "            # blocks is used below.=\n",
    "            transform_f1 = self.transform_f1(masked_features, training=training)\n",
    "            transform_f1 = glu(transform_f1, self.feature_dim)\n",
    "\n",
    "            transform_f2 = self.transform_f2(transform_f1, training=training)\n",
    "            transform_f2 = (glu(transform_f2, self.feature_dim) + transform_f1) * tf.math.sqrt(0.5)\n",
    "\n",
    "            transform_f3 = self.transform_f3_list[ni](transform_f2, training=training)\n",
    "            transform_f3 = (glu(transform_f3, self.feature_dim) + transform_f2) * tf.math.sqrt(0.5)\n",
    "\n",
    "            transform_f4 = self.transform_f4_list[ni](transform_f3, training=training)\n",
    "            transform_f4 = (glu(transform_f4, self.feature_dim) + transform_f3) * tf.math.sqrt(0.5)\n",
    "\n",
    "            if ni > 0 or self.num_decision_steps == 1:\n",
    "                decision_out = tf.nn.relu(transform_f4[:, : self.output_dim])\n",
    "\n",
    "                # Decision aggregation.\n",
    "                output_aggregated += decision_out\n",
    "\n",
    "                # Aggregated masks are used for visualization of the\n",
    "                # feature importance attributes.\n",
    "                scale_agg = tf.reduce_sum(decision_out, axis=1, keepdims=True)\n",
    "\n",
    "                if self.num_decision_steps > 1:\n",
    "                    scale_agg = scale_agg / tf.cast(self.num_decision_steps - 1, tf.float32)\n",
    "\n",
    "                aggregated_mask_values += mask_values * scale_agg\n",
    "\n",
    "            features_for_coef = transform_f4[:, self.output_dim :]\n",
    "\n",
    "            if ni < (self.num_decision_steps - 1):\n",
    "                # Determines the feature masks via linear and nonlinear\n",
    "                # transformations, taking into account of aggregated feature use.\n",
    "                mask_values = self.transform_coef_list[ni](features_for_coef, training=training)\n",
    "                mask_values *= complementary_aggregated_mask_values\n",
    "                mask_values = sparsemax(mask_values, axis=-1)\n",
    "\n",
    "                # Relaxation factor controls the amount of reuse of features between\n",
    "                # different decision blocks and updated with the values of\n",
    "                # coefficients.\n",
    "                complementary_aggregated_mask_values *= self.relaxation_factor - mask_values\n",
    "\n",
    "                # Entropy is used to penalize the amount of sparsity in feature\n",
    "                # selection.\n",
    "                total_entropy += tf.reduce_mean(\n",
    "                    tf.reduce_sum(-mask_values * tf.math.log(mask_values + self.epsilon), axis=1)\n",
    "                ) / (tf.cast(self.num_decision_steps - 1, tf.float32))\n",
    "\n",
    "                # Add entropy loss\n",
    "                entropy_loss = total_entropy\n",
    "\n",
    "                # Feature selection.\n",
    "                masked_features = tf.multiply(mask_values, features)\n",
    "\n",
    "                # Visualization of the feature selection mask at decision step ni\n",
    "                # tf.summary.image(\n",
    "                #     \"Mask for step\" + str(ni),\n",
    "                #     tf.expand_dims(tf.expand_dims(mask_values, 0), 3),\n",
    "                #     max_outputs=1)\n",
    "                mask_at_step_i = tf.expand_dims(tf.expand_dims(mask_values, 0), 3)\n",
    "                self._step_feature_selection_masks.append(mask_at_step_i)\n",
    "\n",
    "            else:\n",
    "                # This branch is needed for correct compilation by tf.autograph\n",
    "                entropy_loss = 0.0\n",
    "\n",
    "        # Adds the loss automatically\n",
    "        self.add_loss(self.sparsity_coefficient * entropy_loss)\n",
    "\n",
    "        # Visualization of the aggregated feature importances\n",
    "        # tf.summary.image(\n",
    "        #     \"Aggregated mask\",\n",
    "        #     tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3),\n",
    "        #     max_outputs=1)\n",
    "\n",
    "        agg_mask = tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3)\n",
    "        self._step_aggregate_feature_selection_mask = agg_mask\n",
    "\n",
    "        return output_aggregated\n",
    "\n",
    "    @property\n",
    "    def feature_selection_masks(self):\n",
    "        return self._step_feature_selection_masks\n",
    "\n",
    "    @property\n",
    "    def aggregate_feature_selection_mask(self):\n",
    "        return self._step_aggregate_feature_selection_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:35.813450Z",
     "iopub.status.busy": "2020-10-26T03:45:35.811274Z",
     "iopub.status.idle": "2020-10-26T03:45:35.814145Z",
     "shell.execute_reply": "2020-10-26T03:45:35.814699Z"
    },
    "papermill": {
     "duration": 0.094524,
     "end_time": "2020-10-26T03:45:35.814833",
     "exception": false,
     "start_time": "2020-10-26T03:45:35.720309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TabNetClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_classes,\n",
    "        num_features=None,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=1,\n",
    "        epsilon=1e-5,\n",
    "        multi_label=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_classes: Number of classes.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'group' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNetClassifier, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.tabnet = TabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_features=num_features,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        if multi_label:\n",
    "\n",
    "            self.clf = L.Dense(num_classes, activation=\"sigmoid\", use_bias=False, name=\"classifier\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.clf = L.Dense(num_classes, activation=\"softmax\", use_bias=False, name=\"classifier\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.clf(self.activations)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def summary(self, *super_args, **super_kwargs):\n",
    "        super().summary(*super_args, **super_kwargs)\n",
    "        self.tabnet.summary(*super_args, **super_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:35.968975Z",
     "iopub.status.busy": "2020-10-26T03:45:35.968020Z",
     "iopub.status.idle": "2020-10-26T03:45:35.971410Z",
     "shell.execute_reply": "2020-10-26T03:45:35.970857Z"
    },
    "papermill": {
     "duration": 0.08917,
     "end_time": "2020-10-26T03:45:35.971523",
     "exception": false,
     "start_time": "2020-10-26T03:45:35.882353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TabNetRegressor(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_regressors,\n",
    "        num_features=None,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=1,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_regressors: Number of regression variables.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'group' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNetRegressor, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_regressors = num_regressors\n",
    "\n",
    "        self.tabnet = TabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_features=num_features,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.regressor = L.Dense(num_regressors, use_bias=False, name=\"regressor\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.regressor(self.activations)\n",
    "        return out\n",
    "\n",
    "    def summary(self, *super_args, **super_kwargs):\n",
    "        super().summary(*super_args, **super_kwargs)\n",
    "        self.tabnet.summary(*super_args, **super_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:36.114820Z",
     "iopub.status.busy": "2020-10-26T03:45:36.113896Z",
     "iopub.status.idle": "2020-10-26T03:45:36.116473Z",
     "shell.execute_reply": "2020-10-26T03:45:36.116952Z"
    },
    "papermill": {
     "duration": 0.076019,
     "end_time": "2020-10-26T03:45:36.117118",
     "exception": false,
     "start_time": "2020-10-26T03:45:36.041099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aliases\n",
    "TabNetClassification = TabNetClassifier\n",
    "TabNetRegression = TabNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:36.279573Z",
     "iopub.status.busy": "2020-10-26T03:45:36.269036Z",
     "iopub.status.idle": "2020-10-26T03:45:36.285781Z",
     "shell.execute_reply": "2020-10-26T03:45:36.285212Z"
    },
    "papermill": {
     "duration": 0.100068,
     "end_time": "2020-10-26T03:45:36.285894",
     "exception": false,
     "start_time": "2020-10-26T03:45:36.185826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StackedTabNet(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_layers=1,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_layers: Number of TabNets to stack together.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer. Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "                Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(StackedTabNet, self).__init__(**kwargs)\n",
    "\n",
    "        if num_layers < 1:\n",
    "            raise ValueError(\"`num_layers` cannot be less than 1\")\n",
    "\n",
    "        if type(feature_dim) not in [list, tuple]:\n",
    "            feature_dim = [feature_dim] * num_layers\n",
    "\n",
    "        if type(output_dim) not in [list, tuple]:\n",
    "            output_dim = [output_dim] * num_layers\n",
    "\n",
    "        if len(feature_dim) != num_layers:\n",
    "            raise ValueError(\"`feature_dim` must be a list of length `num_layers`\")\n",
    "\n",
    "        if len(output_dim) != num_layers:\n",
    "            raise ValueError(\"`output_dim` must be a list of length `num_layers`\")\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            TabNet(\n",
    "                feature_columns=feature_columns,\n",
    "                num_features=num_features,\n",
    "                feature_dim=feature_dim[0],\n",
    "                output_dim=output_dim[0],\n",
    "                num_decision_steps=num_decision_steps,\n",
    "                relaxation_factor=relaxation_factor,\n",
    "                sparsity_coefficient=sparsity_coefficient,\n",
    "                norm_type=norm_type,\n",
    "                batch_momentum=batch_momentum,\n",
    "                virtual_batch_size=virtual_batch_size,\n",
    "                num_groups=num_groups,\n",
    "                epsilon=epsilon,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for layer_idx in range(1, num_layers):\n",
    "            layers.append(\n",
    "                TabNet(\n",
    "                    feature_columns=None,\n",
    "                    num_features=output_dim[layer_idx - 1],\n",
    "                    feature_dim=feature_dim[layer_idx],\n",
    "                    output_dim=output_dim[layer_idx],\n",
    "                    num_decision_steps=num_decision_steps,\n",
    "                    relaxation_factor=relaxation_factor,\n",
    "                    sparsity_coefficient=sparsity_coefficient,\n",
    "                    norm_type=norm_type,\n",
    "                    batch_momentum=batch_momentum,\n",
    "                    virtual_batch_size=virtual_batch_size,\n",
    "                    num_groups=num_groups,\n",
    "                    epsilon=epsilon,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.tabnet_layers = layers\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.tabnet_layers[0](inputs, training=training)\n",
    "\n",
    "        for layer_idx in range(1, self.num_layers):\n",
    "            x = self.tabnet_layers[layer_idx](x, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def tabnets(self):\n",
    "        return self.tabnet_layers\n",
    "\n",
    "    @property\n",
    "    def feature_selection_masks(self):\n",
    "        return [tabnet.feature_selection_masks for tabnet in self.tabnet_layers]\n",
    "\n",
    "    @property\n",
    "    def aggregate_feature_selection_mask(self):\n",
    "        return [tabnet.aggregate_feature_selection_mask for tabnet in self.tabnet_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:36.446152Z",
     "iopub.status.busy": "2020-10-26T03:45:36.445241Z",
     "iopub.status.idle": "2020-10-26T03:45:36.448575Z",
     "shell.execute_reply": "2020-10-26T03:45:36.448036Z"
    },
    "papermill": {
     "duration": 0.093455,
     "end_time": "2020-10-26T03:45:36.448682",
     "exception": false,
     "start_time": "2020-10-26T03:45:36.355227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StackedTabNetClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_classes,\n",
    "        num_layers=1,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        multi_label=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_classes: Number of classes.\n",
    "            num_layers: Number of TabNets to stack together.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer. Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "                Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(StackedTabNetClassifier, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.stacked_tabnet = StackedTabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_layers=num_layers,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_features=num_features,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "        if multi_label:\n",
    "\n",
    "            self.clf = L.Dense(num_classes, activation=\"sigmoid\", use_bias=False)\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.clf = L.Dense(num_classes, activation=\"softmax\", use_bias=False)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.stacked_tabnet(inputs, training=training)\n",
    "        out = self.clf(self.activations)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:36.607416Z",
     "iopub.status.busy": "2020-10-26T03:45:36.606002Z",
     "iopub.status.idle": "2020-10-26T03:45:36.608723Z",
     "shell.execute_reply": "2020-10-26T03:45:36.609344Z"
    },
    "papermill": {
     "duration": 0.089999,
     "end_time": "2020-10-26T03:45:36.609499",
     "exception": false,
     "start_time": "2020-10-26T03:45:36.519500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StackedTabNetRegressor(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_columns,\n",
    "        num_regressors,\n",
    "        num_layers=1,\n",
    "        feature_dim=64,\n",
    "        output_dim=64,\n",
    "        num_features=None,\n",
    "        num_decision_steps=5,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-5,\n",
    "        norm_type=\"group\",\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        num_groups=2,\n",
    "        epsilon=1e-5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_regressors: Number of regressors.\n",
    "            num_layers: Number of TabNets to stack together.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer. Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "                Can be either a single int, or a list of\n",
    "                integers. If a list, must be of same length as the number of layers.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(StackedTabNetRegressor, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_regressors = num_regressors\n",
    "\n",
    "        self.stacked_tabnet = StackedTabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_layers=num_layers,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_features=num_features,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            norm_type=norm_type,\n",
    "            batch_momentum=batch_momentum,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            num_groups=num_groups,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "\n",
    "        self.regressor = L.Dense(num_regressors, use_bias=False)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.regressor(self.activations)\n",
    "        return outl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:36.762112Z",
     "iopub.status.busy": "2020-10-26T03:45:36.761100Z",
     "iopub.status.idle": "2020-10-26T03:45:36.764087Z",
     "shell.execute_reply": "2020-10-26T03:45:36.763529Z"
    },
    "papermill": {
     "duration": 0.079108,
     "end_time": "2020-10-26T03:45:36.764188",
     "exception": false,
     "start_time": "2020-10-26T03:45:36.685080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_tabnet(num_col, output_dim):\n",
    "    model = StackedTabNetClassifier(\n",
    "        feature_columns=None,\n",
    "        num_classes=output_dim,\n",
    "        num_layers=2,\n",
    "        feature_dim=1024,\n",
    "        output_dim=1024,\n",
    "        num_features=num_col,\n",
    "        num_decision_steps=1,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=0,\n",
    "        batch_momentum=0.98,\n",
    "        virtual_batch_size=None,\n",
    "        norm_type=\"group\",\n",
    "        num_groups=-1,\n",
    "        multi_label=True,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.183115,
     "end_time": "2020-10-26T03:45:37.016206",
     "exception": false,
     "start_time": "2020-10-26T03:45:36.833091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:37.270760Z",
     "iopub.status.busy": "2020-10-26T03:45:37.269527Z",
     "iopub.status.idle": "2020-10-26T03:45:37.272717Z",
     "shell.execute_reply": "2020-10-26T03:45:37.271819Z"
    },
    "papermill": {
     "duration": 0.117005,
     "end_time": "2020-10-26T03:45:37.272872",
     "exception": false,
     "start_time": "2020-10-26T03:45:37.155867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [\"SimpleNN\", \"ResNet\", \"SplitNN\", \"TabNet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:37.487158Z",
     "iopub.status.busy": "2020-10-26T03:45:37.486047Z",
     "iopub.status.idle": "2020-10-26T03:45:37.488848Z",
     "shell.execute_reply": "2020-10-26T03:45:37.488041Z"
    },
    "papermill": {
     "duration": 0.114538,
     "end_time": "2020-10-26T03:45:37.488998",
     "exception": false,
     "start_time": "2020-10-26T03:45:37.374460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_STARTS = len(models) * 2\n",
    "N_SPLITS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:37.699727Z",
     "iopub.status.busy": "2020-10-26T03:45:37.698680Z",
     "iopub.status.idle": "2020-10-26T03:45:37.701497Z",
     "shell.execute_reply": "2020-10-26T03:45:37.700597Z"
    },
    "papermill": {
     "duration": 0.112908,
     "end_time": "2020-10-26T03:45:37.701679",
     "exception": false,
     "start_time": "2020-10-26T03:45:37.588771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_train_models = [\"SimpleNN\", \"ResNet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:37.873605Z",
     "iopub.status.busy": "2020-10-26T03:45:37.863103Z",
     "iopub.status.idle": "2020-10-26T03:45:37.900285Z",
     "shell.execute_reply": "2020-10-26T03:45:37.899778Z"
    },
    "papermill": {
     "duration": 0.120831,
     "end_time": "2020-10-26T03:45:37.900402",
     "exception": false,
     "start_time": "2020-10-26T03:45:37.779571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def learning(target, N_STARTS, N_SPLITS, do_predict=False, do_transfer_learning=False):\n",
    "    oof = {}\n",
    "    predictions = {}\n",
    "\n",
    "    for seed in range(N_STARTS):\n",
    "        model_name = models[seed % len(models)]\n",
    "\n",
    "        if not do_predict and model_name not in pre_train_models:\n",
    "            continue\n",
    "\n",
    "        seed_result = target.copy()\n",
    "        seed_result.loc[:, target.columns] = 0\n",
    "        prediction = ss.copy()\n",
    "        prediction.loc[:, ss.columns] = 0\n",
    "\n",
    "        # for SplitNN\n",
    "        split_cols = []\n",
    "        for _ in range(3):  # len(hidden_units) - 1\n",
    "            split_cols.append(np.random.choice(range(len(train.columns)), int(np.ceil(0.8 * len(train.columns)))))\n",
    "\n",
    "        if do_predict:\n",
    "            kfold_seed = random_seed + seed\n",
    "        else:\n",
    "            kfold_seed = seed\n",
    "\n",
    "        fix_seed(kfold_seed)\n",
    "\n",
    "        for n, (tr, te) in enumerate(\n",
    "            MultilabelStratifiedKFold(n_splits=N_SPLITS, random_state=kfold_seed, shuffle=True).split(target, target)\n",
    "        ):\n",
    "            start_time = time()\n",
    "\n",
    "            # Build Model\n",
    "            if model_name == \"SimpleNN\":\n",
    "                model = create_model_simple_nn(len(train.columns), len(target.columns))\n",
    "\n",
    "                if do_transfer_learning:\n",
    "                    model_base = create_model_simple_nn(len(train.columns), len(non_target_df.columns))\n",
    "\n",
    "            elif model_name == \"ResNet\":\n",
    "                model = create_model_resnet(len(train.columns), len(train_pca.columns), len(target.columns))\n",
    "\n",
    "                if do_transfer_learning:\n",
    "                    model_base = create_model_resnet(\n",
    "                        len(train.columns), len(train_pca.columns), len(non_target_df.columns)\n",
    "                    )\n",
    "\n",
    "            elif model_name == \"SplitNN\":\n",
    "                model = create_model_split_nn(len(train.columns), len(target.columns))\n",
    "\n",
    "                # if do_transfer_learning:\n",
    "                #    model_base = create_model_split_nn(len(train.columns), len(non_target_df.columns))\n",
    "\n",
    "            elif model_name == \"NODE\":\n",
    "                model = create_model_node(len(target.columns))\n",
    "\n",
    "                # if do_transfer_learning:\n",
    "                #    model = create_model_node(len(non_target_df.columns))\n",
    "\n",
    "            elif model_name == \"KernelRidge\":\n",
    "                model = KernelRidge(alpha=80, kernel=\"rbf\")\n",
    "\n",
    "                # if do_transfer_learning:\n",
    "                #    model = create_model_node(len(non_target_df.columns))\n",
    "\n",
    "            elif model_name == \"TabNet\":\n",
    "                model = create_model_tabnet(len(train.columns), len(target.columns))\n",
    "\n",
    "                # if do_transfer_learning:\n",
    "                #    model_base = create_model_tabnet(len(train.columns), len(non_target_df.columns))\n",
    "\n",
    "            elif model_name == \"SVM\":\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                raise \"Model name is invalid.\"\n",
    "\n",
    "            # Build Data Sets\n",
    "            if model_name == \"SplitNN\":\n",
    "                x_tr = [\n",
    "                    train.values[tr][:, split_cols[0]],\n",
    "                    train.values[tr][:, split_cols[1]],\n",
    "                    train.values[tr][:, split_cols[2]],\n",
    "                ]\n",
    "                x_val = [\n",
    "                    train.values[te][:, split_cols[0]],\n",
    "                    train.values[te][:, split_cols[1]],\n",
    "                    train.values[te][:, split_cols[2]],\n",
    "                ]\n",
    "                y_tr, y_val = target.astype(float).values[tr], target.astype(float).values[te]\n",
    "                x_tt = [test.values[:, split_cols[0]], test.values[:, split_cols[1]], test.values[:, split_cols[2]]]\n",
    "\n",
    "            elif model_name == \"ResNet\":\n",
    "                x_tr = [\n",
    "                    train.values[tr],\n",
    "                    train_pca.values[tr],\n",
    "                ]\n",
    "                x_val = [\n",
    "                    train.values[te],\n",
    "                    train_pca.values[te],\n",
    "                ]\n",
    "                y_tr, y_val = target.astype(float).values[tr], target.astype(float).values[te]\n",
    "                x_tt = [test.values, test_pca.values]\n",
    "\n",
    "            else:\n",
    "                x_tr, x_val = train.values[tr], train.values[te]\n",
    "                y_tr, y_val = target.astype(float).values[tr], target.astype(float).values[te]\n",
    "                x_tt = test.values\n",
    "\n",
    "            if model_name == \"KernelRidge\":\n",
    "                model.fit(x_tr, y_tr)\n",
    "            else:\n",
    "                model.compile(\n",
    "                    optimizer=tfa.optimizers.AdamW(lr=1e-3, weight_decay=1e-5, clipvalue=756),\n",
    "                    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001),\n",
    "                    metrics=logloss,\n",
    "                )\n",
    "\n",
    "                checkpoint_path = f\"{model_name}_repeat:{seed}_fold:{n}.hdf5\"\n",
    "\n",
    "                if do_transfer_learning and model_name not in [\"SplitNN\", \"NODE\", \"TabNet\"]:\n",
    "                    model_base.load_weights(checkpoint_path)\n",
    "                    for layer in range(len(model_base.layers[:-1])):\n",
    "                        model.layers[layer].set_weights(model_base.layers[layer].get_weights())\n",
    "\n",
    "                cb_checkpt = ModelCheckpoint(\n",
    "                    checkpoint_path,\n",
    "                    monitor=\"val_loss\",\n",
    "                    verbose=0,\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=True,\n",
    "                    mode=\"min\",\n",
    "                )\n",
    "                reduce_lr_loss = ReduceLROnPlateau(\n",
    "                    monitor=\"val_loss\", factor=0.1, patience=3, verbose=0, min_delta=1e-4, mode=\"min\"\n",
    "                )\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    patience=10,\n",
    "                    mode=\"min\",\n",
    "                    verbose=0,\n",
    "                    min_delta=1e-4,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "                model.fit(\n",
    "                    x_tr,\n",
    "                    y_tr,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                    callbacks=[cb_checkpt, reduce_lr_loss, early_stopping],\n",
    "                    verbose=0,\n",
    "                )\n",
    "\n",
    "            val_predict = model.predict(x_val)\n",
    "            fold_score = metric(target.loc[te].values, val_predict)\n",
    "            seed_result.loc[te, target.columns] += val_predict\n",
    "\n",
    "            if do_predict:\n",
    "                test_predict = model.predict(x_tt)\n",
    "                prediction.loc[:, target.columns] += test_predict / N_SPLITS\n",
    "\n",
    "            print(\n",
    "                f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] {model_name}: Seed {seed}, Fold {n}:\",\n",
    "                fold_score,\n",
    "            )\n",
    "\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            x = gc.collect()\n",
    "\n",
    "        oof[f\"{model_name}_{seed}\"] = seed_result\n",
    "        predictions[f\"{model_name}_{seed}\"] = prediction\n",
    "\n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T03:45:38.070895Z",
     "iopub.status.busy": "2020-10-26T03:45:38.069441Z",
     "iopub.status.idle": "2020-10-26T04:17:35.371519Z",
     "shell.execute_reply": "2020-10-26T04:17:35.372116Z"
    },
    "papermill": {
     "duration": 1917.395494,
     "end_time": "2020-10-26T04:17:35.372260",
     "exception": false,
     "start_time": "2020-10-26T03:45:37.976766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:41] SimpleNN: Seed 0, Fold 0: 0.004676131914872915\n",
      "[00:29] SimpleNN: Seed 0, Fold 1: 0.004661057657454437\n",
      "[00:24] SimpleNN: Seed 0, Fold 2: 0.004947709202041774\n",
      "[00:27] SimpleNN: Seed 0, Fold 3: 0.004739203231494746\n",
      "[00:28] SimpleNN: Seed 0, Fold 4: 0.004783587983030469\n",
      "[00:32] SimpleNN: Seed 0, Fold 5: 0.004686414759062373\n",
      "[00:27] SimpleNN: Seed 0, Fold 6: 0.004798288155700176\n",
      "[00:26] SimpleNN: Seed 0, Fold 7: 0.004788073284113613\n",
      "[00:25] SimpleNN: Seed 0, Fold 8: 0.005002871480948044\n",
      "[00:26] SimpleNN: Seed 0, Fold 9: 0.004907458770287938\n",
      "[01:28] ResNet: Seed 1, Fold 0: 0.004298438438336321\n",
      "[01:07] ResNet: Seed 1, Fold 1: 0.004455299081705721\n",
      "[01:01] ResNet: Seed 1, Fold 2: 0.004657634425864797\n",
      "[01:10] ResNet: Seed 1, Fold 3: 0.004551766278164715\n",
      "[00:54] ResNet: Seed 1, Fold 4: 0.004856607842250875\n",
      "[01:06] ResNet: Seed 1, Fold 5: 0.004507388156995587\n",
      "[01:01] ResNet: Seed 1, Fold 6: 0.00461062968873303\n",
      "[01:07] ResNet: Seed 1, Fold 7: 0.004506150929375031\n",
      "[01:05] ResNet: Seed 1, Fold 8: 0.004416840726378003\n",
      "[01:03] ResNet: Seed 1, Fold 9: 0.004572987138528546\n",
      "[00:28] SimpleNN: Seed 4, Fold 0: 0.0048864515713398235\n",
      "[00:27] SimpleNN: Seed 4, Fold 1: 0.004786603724990597\n",
      "[00:24] SimpleNN: Seed 4, Fold 2: 0.004958619238032741\n",
      "[00:30] SimpleNN: Seed 4, Fold 3: 0.004698455317900297\n",
      "[00:27] SimpleNN: Seed 4, Fold 4: 0.004788104407902149\n",
      "[00:26] SimpleNN: Seed 4, Fold 5: 0.0048083685239907826\n",
      "[00:28] SimpleNN: Seed 4, Fold 6: 0.004742730551044464\n",
      "[00:27] SimpleNN: Seed 4, Fold 7: 0.004727443336168359\n",
      "[00:27] SimpleNN: Seed 4, Fold 8: 0.004757515920428161\n",
      "[00:27] SimpleNN: Seed 4, Fold 9: 0.0049217835087819115\n",
      "[01:08] ResNet: Seed 5, Fold 0: 0.004564742837243787\n",
      "[01:06] ResNet: Seed 5, Fold 1: 0.0045625548544651435\n",
      "[01:08] ResNet: Seed 5, Fold 2: 0.004426917109217279\n",
      "[01:06] ResNet: Seed 5, Fold 3: 0.00455685997244845\n",
      "[01:06] ResNet: Seed 5, Fold 4: 0.004519353626464751\n",
      "[01:06] ResNet: Seed 5, Fold 5: 0.004449001516381704\n",
      "[01:02] ResNet: Seed 5, Fold 6: 0.004649754157378795\n",
      "[01:06] ResNet: Seed 5, Fold 7: 0.004528037185308121\n",
      "[00:58] ResNet: Seed 5, Fold 8: 0.004626159420634352\n",
      "[01:10] ResNet: Seed 5, Fold 9: 0.004393411269662059\n"
     ]
    }
   ],
   "source": [
    "# Pre train with non-scored labels\n",
    "_, _ = learning(non_target_df, N_STARTS, N_SPLITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T04:17:35.545327Z",
     "iopub.status.busy": "2020-10-26T04:17:35.544250Z",
     "iopub.status.idle": "2020-10-26T05:33:33.605657Z",
     "shell.execute_reply": "2020-10-26T05:33:33.605034Z"
    },
    "papermill": {
     "duration": 4558.148604,
     "end_time": "2020-10-26T05:33:33.605786",
     "exception": false,
     "start_time": "2020-10-26T04:17:35.457182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:31] SimpleNN: Seed 0, Fold 0: 0.016614941320104177\n",
      "[00:38] SimpleNN: Seed 0, Fold 1: 0.016290037565404324\n",
      "[00:32] SimpleNN: Seed 0, Fold 2: 0.01602512828853614\n",
      "[00:33] SimpleNN: Seed 0, Fold 3: 0.01607985101818241\n",
      "[00:30] SimpleNN: Seed 0, Fold 4: 0.01624032387332757\n",
      "[00:32] SimpleNN: Seed 0, Fold 5: 0.01662345121278815\n",
      "[00:29] SimpleNN: Seed 0, Fold 6: 0.01642606116766449\n",
      "[00:38] SimpleNN: Seed 0, Fold 7: 0.01614625580443336\n",
      "[00:32] SimpleNN: Seed 0, Fold 8: 0.01607162276065331\n",
      "[00:32] SimpleNN: Seed 0, Fold 9: 0.015935527331508294\n",
      "[01:02] ResNet: Seed 1, Fold 0: 0.016603945777303435\n",
      "[01:08] ResNet: Seed 1, Fold 1: 0.01628404764894823\n",
      "[01:02] ResNet: Seed 1, Fold 2: 0.016652518448971212\n",
      "[01:10] ResNet: Seed 1, Fold 3: 0.016058892051248313\n",
      "[01:07] ResNet: Seed 1, Fold 4: 0.015941748615778253\n",
      "[01:04] ResNet: Seed 1, Fold 5: 0.016181627873650163\n",
      "[01:06] ResNet: Seed 1, Fold 6: 0.01625641028592689\n",
      "[01:08] ResNet: Seed 1, Fold 7: 0.01629803293879659\n",
      "[01:07] ResNet: Seed 1, Fold 8: 0.01627067463784043\n",
      "[01:06] ResNet: Seed 1, Fold 9: 0.01623676778622705\n",
      "[01:35] SplitNN: Seed 2, Fold 0: 0.016598509009601057\n",
      "[01:25] SplitNN: Seed 2, Fold 1: 0.016832213759550102\n",
      "[01:33] SplitNN: Seed 2, Fold 2: 0.016741542091194\n",
      "[01:31] SplitNN: Seed 2, Fold 3: 0.01646463245246623\n",
      "[01:34] SplitNN: Seed 2, Fold 4: 0.016463165447474854\n",
      "[01:30] SplitNN: Seed 2, Fold 5: 0.016832198269177444\n",
      "[01:27] SplitNN: Seed 2, Fold 6: 0.016478343435070965\n",
      "[01:27] SplitNN: Seed 2, Fold 7: 0.016271586399541002\n",
      "[01:27] SplitNN: Seed 2, Fold 8: 0.01659184553084422\n",
      "[01:25] SplitNN: Seed 2, Fold 9: 0.016687282051339715\n",
      "[00:38] TabNet: Seed 3, Fold 0: 0.0171450183912752\n",
      "[00:32] TabNet: Seed 3, Fold 1: 0.016827300278842282\n",
      "[00:34] TabNet: Seed 3, Fold 2: 0.016588565719538345\n",
      "[00:33] TabNet: Seed 3, Fold 3: 0.016973505067089452\n",
      "[00:31] TabNet: Seed 3, Fold 4: 0.01734319469147709\n",
      "[00:33] TabNet: Seed 3, Fold 5: 0.01717744414697221\n",
      "[00:32] TabNet: Seed 3, Fold 6: 0.017266455297382247\n",
      "[00:35] TabNet: Seed 3, Fold 7: 0.016808333207907745\n",
      "[00:35] TabNet: Seed 3, Fold 8: 0.01718409147550062\n",
      "[00:33] TabNet: Seed 3, Fold 9: 0.016808359246523685\n",
      "[00:33] SimpleNN: Seed 4, Fold 0: 0.016292066994072894\n",
      "[00:37] SimpleNN: Seed 4, Fold 1: 0.01626114921377033\n",
      "[00:34] SimpleNN: Seed 4, Fold 2: 0.01657713399461651\n",
      "[00:33] SimpleNN: Seed 4, Fold 3: 0.016099442249106696\n",
      "[00:32] SimpleNN: Seed 4, Fold 4: 0.016053192921664405\n",
      "[00:32] SimpleNN: Seed 4, Fold 5: 0.016327887974312123\n",
      "[00:32] SimpleNN: Seed 4, Fold 6: 0.016329289309783078\n",
      "[00:34] SimpleNN: Seed 4, Fold 7: 0.016052897157552513\n",
      "[00:33] SimpleNN: Seed 4, Fold 8: 0.016381209879220506\n",
      "[00:30] SimpleNN: Seed 4, Fold 9: 0.016169458644684116\n",
      "[01:09] ResNet: Seed 5, Fold 0: 0.01615099407643243\n",
      "[01:06] ResNet: Seed 5, Fold 1: 0.01643883826910976\n",
      "[01:08] ResNet: Seed 5, Fold 2: 0.016319519075882874\n",
      "[01:04] ResNet: Seed 5, Fold 3: 0.016172473864819233\n",
      "[01:09] ResNet: Seed 5, Fold 4: 0.01647308967476609\n",
      "[01:03] ResNet: Seed 5, Fold 5: 0.016479289098006427\n",
      "[01:01] ResNet: Seed 5, Fold 6: 0.01667925787934458\n",
      "[01:10] ResNet: Seed 5, Fold 7: 0.01618551756902119\n",
      "[01:09] ResNet: Seed 5, Fold 8: 0.0160093502486933\n",
      "[01:09] ResNet: Seed 5, Fold 9: 0.01620548663964677\n",
      "[01:35] SplitNN: Seed 6, Fold 0: 0.01651957488640677\n",
      "[01:29] SplitNN: Seed 6, Fold 1: 0.016306355126912146\n",
      "[01:29] SplitNN: Seed 6, Fold 2: 0.016598217720404748\n",
      "[01:27] SplitNN: Seed 6, Fold 3: 0.016757219650839896\n",
      "[01:36] SplitNN: Seed 6, Fold 4: 0.0163998452334505\n",
      "[01:33] SplitNN: Seed 6, Fold 5: 0.016751846077280714\n",
      "[01:37] SplitNN: Seed 6, Fold 6: 0.016426581064778335\n",
      "[01:32] SplitNN: Seed 6, Fold 7: 0.016508610240168746\n",
      "[01:30] SplitNN: Seed 6, Fold 8: 0.016408331044288665\n",
      "[01:34] SplitNN: Seed 6, Fold 9: 0.01655426591500434\n",
      "[00:32] TabNet: Seed 7, Fold 0: 0.017493948792516884\n",
      "[00:33] TabNet: Seed 7, Fold 1: 0.016805509367314515\n",
      "[00:32] TabNet: Seed 7, Fold 2: 0.016739121271283446\n",
      "[00:35] TabNet: Seed 7, Fold 3: 0.01669292518120544\n",
      "[00:31] TabNet: Seed 7, Fold 4: 0.017632602103913556\n",
      "[00:33] TabNet: Seed 7, Fold 5: 0.017085346725819704\n",
      "[00:33] TabNet: Seed 7, Fold 6: 0.016820269483601095\n",
      "[00:32] TabNet: Seed 7, Fold 7: 0.01697140852993347\n",
      "[00:33] TabNet: Seed 7, Fold 8: 0.016865769345819487\n",
      "[00:33] TabNet: Seed 7, Fold 9: 0.016954729404144664\n"
     ]
    }
   ],
   "source": [
    "oof, predictions = learning(target_df, N_STARTS, N_SPLITS, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.10975,
     "end_time": "2020-10-26T05:33:33.824767",
     "exception": false,
     "start_time": "2020-10-26T05:33:33.715017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Learning - by columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T05:33:34.081633Z",
     "iopub.status.busy": "2020-10-26T05:33:34.065119Z",
     "iopub.status.idle": "2020-10-26T05:33:34.084615Z",
     "shell.execute_reply": "2020-10-26T05:33:34.084039Z"
    },
    "papermill": {
     "duration": 0.147113,
     "end_time": "2020-10-26T05:33:34.084769",
     "exception": false,
     "start_time": "2020-10-26T05:33:33.937656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in oof.keys():\n",
    "    if \"SVM\" not in key:\n",
    "        continue\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    x_new = oof[key].values\n",
    "    x_tt_new = predictions[key].values\n",
    "\n",
    "    seed_result = target_df.copy()\n",
    "    seed_result.loc[:, target_df.columns] = 0\n",
    "    prediction = ss.copy()\n",
    "    prediction.loc[:, ss.columns] = 0\n",
    "\n",
    "    for col in range(target_df.shape[1]):\n",
    "        target = target_df.values[:, col]\n",
    "\n",
    "        if target.sum() >= N_SPLITS:\n",
    "            kfold_seed = random_seed + N_STARTS + int(key.rsplit(\"_\", 1)[-1]) * 300 + col\n",
    "            skf = StratifiedKFold(n_splits=N_SPLITS, random_state=kfold_seed, shuffle=True)\n",
    "\n",
    "            for n, (tr, te) in enumerate(skf.split(target, target)):\n",
    "                x_tr, x_val = x_new[tr, col].reshape(-1, 1), x_new[te, col].reshape(-1, 1)\n",
    "                y_tr, y_val = target[tr], target[te]\n",
    "                x_tt = x_tt_new[:, col].reshape(-1, 1)\n",
    "\n",
    "                model = SVC(C=40, cache_size=2000)\n",
    "                model.fit(x_tr, y_tr)\n",
    "\n",
    "                val_predict = model.decision_function(x_val)\n",
    "                seed_result.loc[te, target_df.columns[col]] += val_predict\n",
    "\n",
    "                test_predict = model.decision_function(x_tt)\n",
    "                prediction.loc[:, target_df.columns[col]] += test_predict / N_SPLITS\n",
    "        else:\n",
    "            seed_result.loc[:, target_df.columns[col]] += x_new[:, col]\n",
    "            prediction.loc[:, target_df.columns[col]] += x_tt_new[:, col]\n",
    "\n",
    "    seed_score = metric(target_df.values, seed_result.values)\n",
    "    print(\n",
    "        f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] {key}: \",\n",
    "        seed_score,\n",
    "    )\n",
    "\n",
    "    oof[key] = seed_result\n",
    "    predictions[key] = prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.111929,
     "end_time": "2020-10-26T05:33:34.312117",
     "exception": false,
     "start_time": "2020-10-26T05:33:34.200188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Learning - Platt Scaling\n",
    "\n",
    "https://www.kaggle.com/gogo827jz/kernel-logistic-regression-one-for-206-targets?scriptVersionId=43366198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T05:33:34.553236Z",
     "iopub.status.busy": "2020-10-26T05:33:34.546536Z",
     "iopub.status.idle": "2020-10-26T05:33:34.556988Z",
     "shell.execute_reply": "2020-10-26T05:33:34.556225Z"
    },
    "papermill": {
     "duration": 0.134604,
     "end_time": "2020-10-26T05:33:34.557107",
     "exception": false,
     "start_time": "2020-10-26T05:33:34.422503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in oof.keys():\n",
    "    if all(col not in key for col in [\"KernelRidge\", \"SVM\"]):\n",
    "        continue\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    x_new = oof[key].values\n",
    "    x_tt_new = predictions[key].values\n",
    "\n",
    "    seed_result = target_df.copy()\n",
    "    seed_result.loc[:, target_df.columns] = 0\n",
    "    prediction = ss.copy()\n",
    "    prediction.loc[:, ss.columns] = 0\n",
    "\n",
    "    for col in range(target_df.shape[1]):\n",
    "        target = target_df.values[:, col]\n",
    "\n",
    "        if target.sum() >= N_SPLITS:\n",
    "            kfold_seed = random_seed + (N_STARTS + 1) * 300 + int(key.rsplit(\"_\", 1)[-1]) * 300 + col\n",
    "            skf = StratifiedKFold(n_splits=N_SPLITS, random_state=kfold_seed, shuffle=True)\n",
    "\n",
    "            for n, (tr, te) in enumerate(skf.split(target, target)):\n",
    "                x_tr, x_val = x_new[tr, col].reshape(-1, 1), x_new[te, col].reshape(-1, 1)\n",
    "                y_tr, y_val = target[tr], target[te]\n",
    "                x_tt = x_tt_new[:, col].reshape(-1, 1)\n",
    "\n",
    "                model = LogisticRegression(penalty=\"none\", max_iter=1000)\n",
    "                model.fit(x_tr, y_tr)\n",
    "\n",
    "                val_predict = model.predict_proba(x_val)\n",
    "                seed_result.loc[te, target_df.columns[col]] += val_predict[:, 1]\n",
    "\n",
    "                test_predict = model.predict_proba(x_tt)\n",
    "                prediction.loc[:, target_df.columns[col]] += test_predict[:, 1] / N_SPLITS\n",
    "        else:\n",
    "            seed_result.loc[:, target_df.columns[col]] += x_new[:, col]\n",
    "            prediction.loc[:, target_df.columns[col]] += x_tt_new[:, col]\n",
    "\n",
    "    seed_score = metric(target_df.values, seed_result.values)\n",
    "    print(\n",
    "        f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] {key}: \",\n",
    "        seed_score,\n",
    "    )\n",
    "\n",
    "    oof[key] = seed_result\n",
    "    predictions[key] = prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.112833,
     "end_time": "2020-10-26T05:33:34.782157",
     "exception": false,
     "start_time": "2020-10-26T05:33:34.669324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T05:33:35.022444Z",
     "iopub.status.busy": "2020-10-26T05:33:35.020790Z",
     "iopub.status.idle": "2020-10-26T05:33:35.991258Z",
     "shell.execute_reply": "2020-10-26T05:33:35.991793Z"
    },
    "papermill": {
     "duration": 1.09882,
     "end_time": "2020-10-26T05:33:35.991955",
     "exception": false,
     "start_time": "2020-10-26T05:33:34.893135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial blend CV: 0.01589195511394151\n",
      "Optimized blend CV: 0.01589195511394151\n",
      "Optimized weights: [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]\n",
      "Check the sum of all weights: 1.0\n"
     ]
    }
   ],
   "source": [
    "initial_weights = [1.0 / N_STARTS for _ in range(N_STARTS)] + [1.0]\n",
    "\n",
    "# https://www.kaggle.com/gogo827jz/optimise-blending-weights-with-bonus-0#Bonus-(Lagrange-Multiplier)\n",
    "\n",
    "\n",
    "def lagrange_func(params):\n",
    "    # weights, _lambda = params\n",
    "    blend_ = blend(target_df.values.shape, params[:-1], oof)\n",
    "    return metric(target_df.values, blend_) - params[-1] * (sum(params[:-1]) - 1)\n",
    "\n",
    "\n",
    "grad_l = grad(lagrange_func)\n",
    "\n",
    "\n",
    "def lagrange_obj(params):\n",
    "    # weights, _lambda = params\n",
    "    d = grad_l(params).tolist()\n",
    "    return d[:-1] + [sum(params[:-1]) - 1]\n",
    "\n",
    "\n",
    "blend_ = blend(target_df.values.shape, initial_weights[:-1], oof)\n",
    "print(f\"Initial blend CV: {metric(target_df.values, blend_)}\")\n",
    "\n",
    "optimize = False\n",
    "if optimize:\n",
    "    optimized_weights = fsolve(lagrange_obj, initial_weights)\n",
    "else:\n",
    "    optimized_weights = initial_weights\n",
    "\n",
    "blend_ = blend(target_df.values.shape, optimized_weights[:-1], oof)\n",
    "print(f\"Optimized blend CV: {metric(target_df.values, blend_)}\")\n",
    "\n",
    "print(f\"Optimized weights: {optimized_weights[:-1]}\")\n",
    "print(f\"Check the sum of all weights: {sum(optimized_weights[:-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.120768,
     "end_time": "2020-10-26T05:33:36.230876",
     "exception": false,
     "start_time": "2020-10-26T05:33:36.110108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T05:33:36.478183Z",
     "iopub.status.busy": "2020-10-26T05:33:36.477030Z",
     "iopub.status.idle": "2020-10-26T05:33:36.811063Z",
     "shell.execute_reply": "2020-10-26T05:33:36.811724Z"
    },
    "papermill": {
     "duration": 0.461325,
     "end_time": "2020-10-26T05:33:36.811881",
     "exception": false,
     "start_time": "2020-10-26T05:33:36.350556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weighted blend\n",
    "submit_df.loc[:, target_df.columns] = blend(ss.shape, optimized_weights[:-1], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T05:33:37.293438Z",
     "iopub.status.busy": "2020-10-26T05:33:37.284577Z",
     "iopub.status.idle": "2020-10-26T05:33:37.352827Z",
     "shell.execute_reply": "2020-10-26T05:33:37.354141Z"
    },
    "papermill": {
     "duration": 0.266593,
     "end_time": "2020-10-26T05:33:37.354342",
     "exception": false,
     "start_time": "2020-10-26T05:33:37.087749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clipping\n",
    "submit_df.loc[:, target_df.columns] = submit_df.loc[:, target_df.columns].clip(1e-7, 1 - 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T05:33:37.672069Z",
     "iopub.status.busy": "2020-10-26T05:33:37.671035Z",
     "iopub.status.idle": "2020-10-26T05:33:37.709758Z",
     "shell.execute_reply": "2020-10-26T05:33:37.709264Z"
    },
    "papermill": {
     "duration": 0.186957,
     "end_time": "2020-10-26T05:33:37.709881",
     "exception": false,
     "start_time": "2020-10-26T05:33:37.522924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit_df.loc[test_df[\"cp_type\"] == \"ctl_vehicle\", target_df.columns] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.136299,
     "end_time": "2020-10-26T05:33:37.957177",
     "exception": false,
     "start_time": "2020-10-26T05:33:37.820878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T05:33:38.188843Z",
     "iopub.status.busy": "2020-10-26T05:33:38.188195Z",
     "iopub.status.idle": "2020-10-26T05:33:41.223396Z",
     "shell.execute_reply": "2020-10-26T05:33:41.222250Z"
    },
    "papermill": {
     "duration": 3.151557,
     "end_time": "2020-10-26T05:33:41.223523",
     "exception": false,
     "start_time": "2020-10-26T05:33:38.071966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 6534.308962,
   "end_time": "2020-10-26T05:33:42.971090",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-26T03:44:48.662128",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
