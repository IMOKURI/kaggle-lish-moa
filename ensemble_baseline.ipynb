{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "ensemble-baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "F0fdkwGB7OS3",
        "papermill": {
          "duration": 0.041247,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:02:58.310522",
          "end_time": "2020-11-02T00:02:58.351769"
        }
      },
      "source": [
        "# Strategy\n",
        "\n",
        "- Preprocessing\n",
        "    - Include ctrl_vehicle\n",
        "    - RankGauss\n",
        "    - PCA + Existing Features\n",
        "    - KMeans\n",
        "    - Basic stats\n",
        "- Model\n",
        "    - Multi head ResNet (tensorflow)\n",
        "    - TabNet (pytorch)\n",
        "- Training\n",
        "    - Pre-train with non-scored target.\n",
        "    - Train with public test pseudo label\n",
        "    - Optimizer: Adam/AdamW with weight_decay\n",
        "    - Loss: BCE with Label smoothing + Logits\n",
        "- Prediction\n",
        "    - Ensemble above with average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBc7l0UMiW_i"
      },
      "source": [
        "# Change Log\n",
        "\n",
        "- v65\n",
        "    - Remove clipping.\n",
        "    - Disable Variance Encoding.\n",
        "- v66\n",
        "    - Add AUC.\n",
        "    - CV only with original training data.\n",
        "- v67\n",
        "    - Add `train_drug.csv` .\n",
        "    - Add Drug and MultiLabel Stratification.\n",
        "- v68\n",
        "    - Remove public test pseudo label.\n",
        "    - Enable pseudo labeling.\n",
        "    - Disable pre-training with non-scored target.\n",
        "- V69\n",
        "    - Disable pseudo labeling.\n",
        "    - Re-enable pre-training with non-scored target.\n",
        "    - Re-add public test pseudo label.\n",
        "    - Add correlation.\n",
        "    - Update label smoothing parameter.\n",
        "- v70 - **LB: 0.01840**\n",
        "    - Amend num of seed.\n",
        "- v71\n",
        "    - Update model parameters.\n",
        "        - ResNet network\n",
        "        - TabNet dimension\n",
        "- v72\n",
        "    - Add KMeans and basic stats.\n",
        "    - Add NODE model.\n",
        "- v73\n",
        "    - Update split condition of group multilabel stratified kfold.\n",
        "    - Update NODE parameters.\n",
        "- v74\n",
        "    - Disable pre-train with non-scored target due to execution time reduction.\n",
        "- v75\n",
        "    - Fold 5 to 7.\n",
        "- v76\n",
        "    - Remove ResNet for execution time reduction.\n",
        "- v77\n",
        "    - Use 3 models. [\"ResNet\", \"TabNet\", \"NODE\"]\n",
        "    - Enable pre-train for ResNet.\n",
        "- v78 - **LB: 0.01841**\n",
        "    - Reset fold eash seeds.\n",
        "- v79\n",
        "    - Add simple NN model again.\n",
        "    - Fold 7 to 5.\n",
        "- v80\n",
        "    - Remove simple NN and NODE model.\n",
        "    - Increase num of seed x2 to x3.\n",
        "- v81\n",
        "    - Use ctrl_vehicle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQVXKcKnx3VV"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ1uTTKS7OS4"
      },
      "source": [
        "## for Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6l6s1ka7OS5",
        "trusted": true
      },
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u2KbV-T7OS8",
        "trusted": true
      },
      "source": [
        "COMPETE = \"lish-moa\"\n",
        "DATASETS = [\n",
        "    \"imokuri/pytorchtabnet\",\n",
        "    \"imokuri/moablendblendblend\",\n",
        "    \"imokuri/adabelief010\",\n",
        "    \"tolgadincer/autograd\",\n",
        "    \"yasufuminakama/iterative-stratification\",\n",
        "    \"rahulsd91/moapublictest\",\n",
        "]\n",
        "PACKAGES = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCI1aEn_7OTA",
        "trusted": true
      },
      "source": [
        "if IN_COLAB:\n",
        "    !pip install -q -U git+https://github.com/IMOKURI/kaggle_on_google_colab.git\n",
        "\n",
        "    from kaggle_on_google_colab import setup\n",
        "    kaggle = setup.Setup()\n",
        "    kaggle.dirs(COMPETE)\n",
        "\n",
        "    !kaggle competitions download -p /content/zip {COMPETE}\n",
        "    for line in setup.exec_get_lines(cmd=f\"kaggle competitions files --csv {COMPETE} | egrep -v \\\"Warning: Looks like you're using an outdated API Version|name,size,creationDate\\\" | cut -d , -f 1\"):\n",
        "        !unzip -q -n /content/zip/{line.decode().strip()}.zip -d /content/{COMPETE}/input/{COMPETE}\n",
        "\n",
        "    for dataset in DATASETS:\n",
        "        dataset_name = dataset.split(\"/\")[-1]\n",
        "\n",
        "        !kaggle datasets download -p /content/zip {dataset}\n",
        "        !unzip -q -n /content/zip/{dataset_name}.zip -d /content/{COMPETE}/input/{dataset_name}\n",
        "\n",
        "    for package_ in PACKAGES:\n",
        "        !pip install {package_}\n",
        "\n",
        "    !pip install -U tensorflow-addons\n",
        "    !mv /content/zip/train_drug.csv /content/{COMPETE}/input/{COMPETE}/\n",
        "\n",
        "    %cd /content/{COMPETE}/output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "zfsIxBee7OTC",
        "papermill": {
          "duration": 0.037487,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:02:58.390119",
          "end_time": "2020-11-02T00:02:58.427606"
        }
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "ZSt3tVRO7OTD",
        "trusted": false,
        "papermill": {
          "duration": 0.046893,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:02:58.465372",
          "end_time": "2020-11-02T00:02:58.512265"
        }
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Di-6b3xS7OTF",
        "trusted": false,
        "papermill": {
          "duration": 2.871801,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:02:58.550511",
          "end_time": "2020-11-02T00:03:01.422312"
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "sys.path.append(\"../input/autograd\")\n",
        "import autograd.numpy as np\n",
        "from autograd import grad\n",
        "\n",
        "sys.path.append(\"../input/pytorchtabnet\")\n",
        "from pytorch_tabnet.metrics import Metric\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "#sys.path.append(\"../input/adabelief010\")\n",
        "#from AdaBelief import AdaBelief\n",
        "#from AdaBelief_tf import AdaBeliefOptimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "PZIdM0rJ7OTH",
        "trusted": false,
        "papermill": {
          "duration": 6.004229,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:01.490703",
          "end_time": "2020-11-02T00:03:07.494932"
        }
      },
      "source": [
        "import datetime\n",
        "import gc\n",
        "import itertools\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from time import time\n",
        "from typing import Optional\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as L\n",
        "import tensorflow_addons as tfa\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from scipy.optimize import fsolve, minimize\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow_probability import distributions as tfp_distributions\n",
        "from tensorflow_probability import stats as tfp_stats\n",
        "from torch import nn\n",
        "from torch.nn.modules.loss import _WeightedLoss\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau as torch_ReduceLROnPlateau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "vA1aD2yd7OTJ",
        "trusted": false,
        "papermill": {
          "duration": 0.044992,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:07.535861",
          "end_time": "2020-11-02T00:03:07.580853"
        }
      },
      "source": [
        "# import numpy as np\n",
        "# import optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "nrm75bEQ7OTL",
        "trusted": false,
        "papermill": {
          "duration": 0.051965,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:07.619035",
          "end_time": "2020-11-02T00:03:07.671"
        }
      },
      "source": [
        "MIXED_PRECISION = False\n",
        "XLA_ACCELERATE = True\n",
        "\n",
        "if MIXED_PRECISION:\n",
        "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "\n",
        "    if tpu:\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_bfloat16\")\n",
        "    else:\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
        "    mixed_precision.set_policy(policy)\n",
        "    print(\"Mixed precision enabled\")\n",
        "\n",
        "if XLA_ACCELERATE:\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "    print(\"Accelerated Linear Algebra enabled\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "8aMe4rbK7OTN",
        "trusted": false,
        "papermill": {
          "duration": 0.048041,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:07.711873",
          "end_time": "2020-11-02T00:03:07.759914"
        }
      },
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ff-lKRvA7OTP",
        "papermill": {
          "duration": 0.043281,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:07.801312",
          "end_time": "2020-11-02T00:03:07.844593"
        }
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "vYlmC0dI7OTP",
        "trusted": false,
        "papermill": {
          "duration": 0.405788,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:07.884323",
          "end_time": "2020-11-02T00:03:08.290111"
        }
      },
      "source": [
        "def fix_seed(seed=2020):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "random_seed = 22\n",
        "fix_seed(random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyamWSs0M93B"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Ysy4T3ZM7OTR",
        "trusted": false,
        "papermill": {
          "duration": 0.048342,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:08.330615",
          "end_time": "2020-11-02T00:03:08.378957"
        }
      },
      "source": [
        "# Evaluation Metric with sigmoid applied and clipping\n",
        "\n",
        "## for tensorflow\n",
        "def logloss(y_true, y_pred):\n",
        "    logits = 1 / (1 + K.exp(-y_pred))\n",
        "    aux = (1 - y_true) * K.log(1 - logits + 1e-15) + y_true * K.log(logits + 1e-15)\n",
        "    return K.mean(-aux)\n",
        "\n",
        "## for pytorch\n",
        "class LogitsLogLoss(Metric):\n",
        "    def __init__(self):\n",
        "        self._name = \"logits_ll\"\n",
        "        self._maximize = False\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        logits = 1 / (1 + np.exp(-y_pred))\n",
        "        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n",
        "        return np.mean(-aux)\n",
        "\n",
        "## for overall\n",
        "## [Fast Numpy Log Loss] https://www.kaggle.com/gogo827jz/optimise-blending-weights-4-5x-faster-log-loss\n",
        "def metric(y_true, y_pred):\n",
        "    loss = 0\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        loss += -np.mean(y_true[:, i] * np.log(y_pred[:, i] + 1e-15) + (1 - y_true[:, i]) * np.log(1 - y_pred[:, i] + 1e-15))\n",
        "    return loss / y_pred.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAAH9Q1VNc9m"
      },
      "source": [
        "## Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtIMMq6Y7OTY",
        "trusted": false
      },
      "source": [
        "# https://www.kaggle.com/felipebihaiek/torch-continued-from-auxiliary-targets-smoothing\n",
        "class SmoothBCEwLogits(_WeightedLoss):\n",
        "    def __init__(self, weight=None, reduction=\"mean\", smoothing=0.0):\n",
        "        super().__init__(weight=weight, reduction=reduction)\n",
        "        self.smoothing = smoothing\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    @staticmethod\n",
        "    def _smooth(targets: torch.Tensor, n_labels: int, smoothing=0.0):\n",
        "        assert 0 <= smoothing < 1\n",
        "        with torch.no_grad():\n",
        "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
        "        return targets\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1), self.smoothing)\n",
        "        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)\n",
        "\n",
        "        if self.reduction == \"sum\":\n",
        "            loss = loss.sum()\n",
        "        elif self.reduction == \"mean\":\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HEx7QqGNqZ8"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "fgO6t6pg7OTV",
        "trusted": false,
        "papermill": {
          "duration": 0.05356,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:08.509901",
          "end_time": "2020-11-02T00:03:08.563461"
        }
      },
      "source": [
        "# Blend oof predictions\n",
        "def blend(size, weights, oof):\n",
        "    blend_ = np.zeros(size)\n",
        "    for i, key in enumerate(oof.keys()):\n",
        "        blend_ += weights[i] * oof[key].values[:blend_.shape[0], :blend_.shape[1]]\n",
        "    return blend_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcsCWnZeovLR"
      },
      "source": [
        "def cross_validation(size, weight, y_true, oof):\n",
        "    x = size[0]\n",
        "    blend_ = blend(y_true[:x].shape, weight, oof)\n",
        "\n",
        "    aucs = []\n",
        "    for task_id in range(blend_.shape[1]):\n",
        "        aucs.append(roc_auc_score(y_true=y_true[:x, task_id], y_score=blend_[:, task_id]))\n",
        "        \n",
        "    CV = metric(y_true[:x], blend_)\n",
        "    AUC = np.mean(aucs)\n",
        "    print(f\"Blended CV: {CV}, AUC : {AUC}\")\n",
        "\n",
        "    return CV, AUC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "iotU3PZB7OTa",
        "papermill": {
          "duration": 0.041079,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:08.703851",
          "end_time": "2020-11-02T00:03:08.74493"
        }
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "p03erQnA7OTa",
        "trusted": false,
        "papermill": {
          "duration": 7.69805,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:08.788115",
          "end_time": "2020-11-02T00:03:16.486165"
        }
      },
      "source": [
        "train_df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n",
        "test_df = pd.read_csv(\"../input/lish-moa/test_features.csv\")\n",
        "target_df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\n",
        "non_target_df = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\n",
        "submit_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n",
        "drug_df = pd.read_csv('../input/lish-moa/train_drug.csv')\n",
        "\n",
        "pub_test_df = pd.read_csv(\"../input/moapublictest/test_features.csv\")\n",
        "pub_submit_df = pd.read_csv(\"../input/moablendblendblend/submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "hYN5WkgA7OTc",
        "trusted": false,
        "papermill": {
          "duration": 0.130368,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:16.529819",
          "end_time": "2020-11-02T00:03:16.660187"
        }
      },
      "source": [
        "train = train_df.copy()\n",
        "test = test_df.copy()\n",
        "target = target_df.copy()\n",
        "non_target = non_target_df.copy()\n",
        "ss = submit_df.copy()\n",
        "drug = drug_df.copy()\n",
        "\n",
        "pub_test = pub_test_df.copy()\n",
        "pub_ss = pub_submit_df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_HmJN8tnpob"
      },
      "source": [
        "## Use public test data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "1ETxnt8B7OTe",
        "trusted": false,
        "papermill": {
          "duration": 0.134832,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:16.701536",
          "end_time": "2020-11-02T00:03:16.836368"
        }
      },
      "source": [
        "# Merge public test data (and pseudo label) into train data\n",
        "train = pd.concat([train, pub_test]).reset_index(drop=True)\n",
        "target = pd.concat([target, pub_ss]).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Cn_3SXzg7OTg",
        "trusted": false,
        "papermill": {
          "duration": 0.087015,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:16.877809",
          "end_time": "2020-11-02T00:03:16.964824"
        }
      },
      "source": [
        "target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "7LA6ekI07OTi",
        "papermill": {
          "duration": 0.042495,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.010293",
          "end_time": "2020-11-02T00:03:17.052788"
        }
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "3DM9pkDt7OTj",
        "trusted": false,
        "papermill": {
          "duration": 0.064235,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.09555",
          "end_time": "2020-11-02T00:03:17.159785"
        }
      },
      "source": [
        "train.loc[:, \"cp_dose\"] = train.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})\n",
        "test.loc[:, \"cp_dose\"] = test.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "jx46CYog7OTk",
        "trusted": false,
        "papermill": {
          "duration": 0.056203,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.202413",
          "end_time": "2020-11-02T00:03:17.258616"
        }
      },
      "source": [
        "train.loc[:, \"cp_time\"] = train.loc[:, \"cp_time\"].map({24: 0, 48: 1, 72: 2})\n",
        "test.loc[:, \"cp_time\"] = test.loc[:, \"cp_time\"].map({24: 0, 48: 1, 72: 2})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "aQWxpszg7OTm",
        "papermill": {
          "duration": 0.042914,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.300561",
          "end_time": "2020-11-02T00:03:17.343475"
        }
      },
      "source": [
        "## Remove ctrl_vehicle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "0HYbBuVW7OTm",
        "trusted": false,
        "papermill": {
          "duration": 0.283079,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.385995",
          "end_time": "2020-11-02T00:03:17.669074"
        }
      },
      "source": [
        "USE_CTRL_VEHICLE = True\n",
        "\n",
        "if USE_CTRL_VEHICLE:\n",
        "    train.loc[:, \"cp_type\"] = train.loc[:, \"cp_type\"].map({\"ctl_vehicle\": 0, \"trt_cp\": 1})\n",
        "    test.loc[:, \"cp_type\"] = test.loc[:, \"cp_type\"].map({\"ctl_vehicle\": 0, \"trt_cp\": 1})\n",
        "\n",
        "else:\n",
        "    target = target.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
        "    non_target = non_target.loc[train[: train_df.shape[0]][\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
        "\n",
        "    train = train.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
        "\n",
        "    train = train.drop(\"cp_type\", axis=1)\n",
        "    test = test.drop(\"cp_type\", axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCzLe0NasfYe"
      },
      "source": [
        "## Merge drug_id into training data\n",
        "\n",
        "https://www.kaggle.com/c/lish-moa/discussion/195195"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLso2JtqsdQi"
      },
      "source": [
        "target_drug = pd.DataFrame(target.loc[:, \"sig_id\"]).merge(drug, on='sig_id', how='left')\n",
        "non_target_drug = pd.DataFrame(non_target.loc[:, \"sig_id\"]).merge(drug, on='sig_id', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtxsR6XoeM0T"
      },
      "source": [
        "target_drug = target_drug.fillna(\"xxxxxxxxx\")\n",
        "non_target_drug = non_target_drug.fillna(\"xxxxxxxxx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4w0OboIswhr"
      },
      "source": [
        "target_drug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvkK0m0xsoTz"
      },
      "source": [
        "## Remove sig_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "5KBPZw3p7OTq",
        "trusted": false,
        "papermill": {
          "duration": 0.054757,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.872423",
          "end_time": "2020-11-02T00:03:17.92718"
        }
      },
      "source": [
        "del train[\"sig_id\"]\n",
        "del target[\"sig_id\"]\n",
        "del non_target[\"sig_id\"]\n",
        "del test[\"sig_id\"]\n",
        "del ss[\"sig_id\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "GCSu_X1_7OTs",
        "trusted": false,
        "papermill": {
          "duration": 0.083712,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.970028",
          "end_time": "2020-11-02T00:03:18.05374"
        }
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "JZaiV9v_7OTv",
        "trusted": false,
        "papermill": {
          "duration": 0.058698,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:18.099745",
          "end_time": "2020-11-02T00:03:18.158443"
        }
      },
      "source": [
        "print(train.shape)\n",
        "print(target.shape)\n",
        "print(non_target.shape)\n",
        "\n",
        "print(test.shape)\n",
        "print(ss.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "x6rjhO-b7OTx",
        "papermill": {
          "duration": 0.044814,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:18.206614",
          "end_time": "2020-11-02T00:03:18.251428"
        }
      },
      "source": [
        "## Rank Gauss\n",
        "\n",
        "https://www.kaggle.com/nayuts/moa-pytorch-nn-pca-rankgauss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "jUsSMkTK7OTx",
        "trusted": false,
        "papermill": {
          "duration": 9.414623,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:18.29612",
          "end_time": "2020-11-02T00:03:27.710743"
        }
      },
      "source": [
        "g_cols = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
        "c_cols = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
        "\n",
        "for col in g_cols + c_cols:\n",
        "    transformer = QuantileTransformer(n_quantiles=100, random_state=random_seed, output_distribution=\"normal\")\n",
        "\n",
        "    vec_len = len(train[col].values)\n",
        "    vec_len_test = len(test[col].values)\n",
        "\n",
        "    raw_vec = train[col].values.reshape(vec_len, 1)\n",
        "    transformer.fit(raw_vec)\n",
        "\n",
        "    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
        "    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "VY4_EigR7OT0",
        "trusted": false,
        "papermill": {
          "duration": 0.08184,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:27.771441",
          "end_time": "2020-11-02T00:03:27.853281"
        }
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "DHiSL9lh7OT2",
        "papermill": {
          "duration": 0.04586,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:27.900605",
          "end_time": "2020-11-02T00:03:27.946465"
        }
      },
      "source": [
        "## PCA features (+ Existing features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "vSiyBS5s7OT2",
        "trusted": false,
        "papermill": {
          "duration": 2.738443,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:27.992409",
          "end_time": "2020-11-02T00:03:30.730852"
        }
      },
      "source": [
        "# g-\n",
        "n_comp = 50\n",
        "\n",
        "data = pd.concat([pd.DataFrame(train[g_cols]), pd.DataFrame(test[g_cols])])\n",
        "data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[g_cols])\n",
        "train2 = data2[: train.shape[0]]\n",
        "test2 = data2[-test.shape[0] :]\n",
        "\n",
        "train2 = pd.DataFrame(train2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
        "test2 = pd.DataFrame(test2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
        "\n",
        "train = pd.concat((train, train2), axis=1)\n",
        "test = pd.concat((test, test2), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "4m1Xajv77OT4",
        "trusted": false,
        "papermill": {
          "duration": 0.483929,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:30.777031",
          "end_time": "2020-11-02T00:03:31.26096"
        }
      },
      "source": [
        "# c-\n",
        "n_comp = 15\n",
        "\n",
        "data = pd.concat([pd.DataFrame(train[c_cols]), pd.DataFrame(test[c_cols])])\n",
        "data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[c_cols])\n",
        "train2 = data2[: train.shape[0]]\n",
        "test2 = data2[-test.shape[0] :]\n",
        "\n",
        "train2 = pd.DataFrame(train2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
        "test2 = pd.DataFrame(test2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
        "\n",
        "train = pd.concat((train, train2), axis=1)\n",
        "test = pd.concat((test, test2), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "2vVAG1aL7OT6",
        "trusted": false,
        "papermill": {
          "duration": 0.084273,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:31.308289",
          "end_time": "2020-11-02T00:03:31.392562"
        }
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "GIHl5F3x7OT7",
        "trusted": false,
        "papermill": {
          "duration": 0.324282,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:31.445162",
          "end_time": "2020-11-02T00:03:31.769444"
        }
      },
      "source": [
        "train_pca = train.copy()\n",
        "test_pca = test.copy()\n",
        "\n",
        "train_pca.drop(g_cols, axis=1, inplace=True)\n",
        "test_pca.drop(g_cols, axis=1, inplace=True)\n",
        "\n",
        "train_pca.drop(c_cols, axis=1, inplace=True)\n",
        "test_pca.drop(c_cols, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "nJyx4dSR7OT9",
        "trusted": false,
        "papermill": {
          "duration": 0.087344,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:31.838133",
          "end_time": "2020-11-02T00:03:31.925477"
        }
      },
      "source": [
        "train_pca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ujMOcMFG7OUD",
        "papermill": {
          "duration": 0.04964,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:31.976171",
          "end_time": "2020-11-02T00:03:32.025811"
        }
      },
      "source": [
        "## feature Selection using Variance Encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "OR4sYLWT7OUE",
        "trusted": false,
        "papermill": {
          "duration": 0.637637,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:32.072952",
          "end_time": "2020-11-02T00:03:32.710589"
        }
      },
      "source": [
        "# https://www.kaggle.com/c/lish-moa/discussion/194973#1067941\n",
        "if False:\n",
        "\n",
        "    var_threshold = 0.5\n",
        "\n",
        "    data = train.append(test)\n",
        "    ve_columns = (data.iloc[:, 2:].var() >= var_threshold).values\n",
        "    ve_data = data.iloc[:, 2:].loc[:, ve_columns]\n",
        "\n",
        "    ve_train = ve_data[: train.shape[0]]\n",
        "    ve_test = ve_data[-test.shape[0] :]\n",
        "\n",
        "\n",
        "    train = pd.DataFrame(train[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
        "    train = pd.concat([train, ve_train], axis=1)\n",
        "\n",
        "\n",
        "    test = pd.DataFrame(test[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
        "    test = pd.concat([test, ve_test], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "lvb-RhDK7OUG",
        "trusted": false,
        "papermill": {
          "duration": 0.09962,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:32.764308",
          "end_time": "2020-11-02T00:03:32.863928"
        }
      },
      "source": [
        "# train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekFLgTbpKfA4"
      },
      "source": [
        "## KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA5PfEXjKg5O"
      },
      "source": [
        "%%time\n",
        "\n",
        "features_g = [col for col in train.columns if col.startswith(\"g-\")]\n",
        "features_c = [col for col in train.columns if col.startswith(\"c-\")]\n",
        "\n",
        "def fe_cluster(train_, test_, n_clusters_g = 35, n_clusters_c = 5):\n",
        "\n",
        "    def create_cluster(tr, te, features, kind = 'g', n_clusters = n_clusters_g):\n",
        "        tmp_train_ = tr[features].copy()\n",
        "        tmp_test_ = te[features].copy()\n",
        "        data = pd.concat([tmp_train_, tmp_test_], axis = 0)\n",
        "\n",
        "        kmeans = KMeans(n_clusters = n_clusters, random_state = random_seed).fit(data)\n",
        "        \n",
        "        tr[f'clusters_{kind}'] = kmeans.labels_[:tr.shape[0]]\n",
        "        te[f'clusters_{kind}'] = kmeans.labels_[-te.shape[0]:]\n",
        "        tr = pd.get_dummies(tr, columns = [f'clusters_{kind}'])\n",
        "        te = pd.get_dummies(te, columns = [f'clusters_{kind}'])\n",
        "        return tr, te\n",
        "    \n",
        "    train_, test_ = create_cluster(train_, test_, features_g, kind = 'g', n_clusters = n_clusters_g)\n",
        "    train_, test_ = create_cluster(train_, test_, features_c, kind = 'c', n_clusters = n_clusters_c)\n",
        "    return train_, test_\n",
        "\n",
        "train, test = fe_cluster(train, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ7Cetl7KjfY"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQeImT1CKmF1"
      },
      "source": [
        "## Basic stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-6Zi3IiKoFF"
      },
      "source": [
        "for stats in [\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]:\n",
        "    train[\"g_\" + stats] = getattr(train[features_g], stats)(axis = 1)\n",
        "    train[\"c_\" + stats] = getattr(train[features_c], stats)(axis = 1)\n",
        "    train[\"gc_\" + stats] = getattr(train[features_g + features_c], stats)(axis = 1)\n",
        "    \n",
        "    test[\"g_\" + stats] = getattr(test[features_g], stats)(axis = 1)\n",
        "    test[\"c_\" + stats] = getattr(test[features_c], stats)(axis = 1)\n",
        "    test[\"gc_\" + stats] = getattr(test[features_g + features_c], stats)(axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi0-UnkaKqaw"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cazlr344-Vx-"
      },
      "source": [
        "# Model - Simple NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j59Uk0H9-YdL"
      },
      "source": [
        "def create_model_simple_nn(num_col, output_dim):\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            L.Input(num_col),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.4),\n",
        "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"elu\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.3),\n",
        "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"swish\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.3),\n",
        "            tfa.layers.WeightNormalization(L.Dense(128, activation=\"selu\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dense(output_dim),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "nUBSHNuL7OUI",
        "papermill": {
          "duration": 0.055601,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:32.921976",
          "end_time": "2020-11-02T00:03:32.977577"
        }
      },
      "source": [
        "# Model - Multi input ResNet\n",
        "\n",
        "https://www.kaggle.com/rahulsd91/moa-multi-input-resnet-model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "w4ySRylL7OUI",
        "trusted": false,
        "papermill": {
          "duration": 0.069892,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.031525",
          "end_time": "2020-11-02T00:03:33.101417"
        }
      },
      "source": [
        "def create_model_resnet(n_features, n_features_2, n_labels):\n",
        "    input_1 = L.Input(shape=(n_features,), name=\"Input1\")\n",
        "    input_2 = L.Input(shape=(n_features_2,), name=\"Input2\")\n",
        "\n",
        "    head_1 = tf.keras.Sequential(\n",
        "        [\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"selu\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(1024, activation=\"swish\")),\n",
        "        ],\n",
        "        name=\"Head1\",\n",
        "    )\n",
        "\n",
        "    input_3 = head_1(input_1)\n",
        "    input_3_concat = L.Concatenate()([input_2, input_3])\n",
        "\n",
        "    head_2 = tf.keras.Sequential(\n",
        "        [\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"swish\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(512, activation=\"relu\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(1024, activation=\"relu\")),\n",
        "        ],\n",
        "        name=\"Head2\",\n",
        "    )\n",
        "\n",
        "    input_4 = head_2(input_3_concat)\n",
        "    input_4_avg = L.Average()([input_3, input_4])\n",
        "\n",
        "    head_3 = tf.keras.Sequential(\n",
        "        [\n",
        "            L.BatchNormalization(),\n",
        "            tfa.layers.WeightNormalization(L.Dense(128, activation=\"relu\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(128, activation=\"swish\")),\n",
        "            L.BatchNormalization(),\n",
        "            # L.Dense(n_labels, activation=\"sigmoid\"),\n",
        "            L.Dense(n_labels),  # from_logits=True\n",
        "        ],\n",
        "        name=\"Head3\",\n",
        "    )\n",
        "\n",
        "    output = head_3(input_4_avg)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "fiC-Q8MQ7OUK",
        "papermill": {
          "duration": 0.048086,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.151294",
          "end_time": "2020-11-02T00:03:33.19938"
        }
      },
      "source": [
        "# Model - TabNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "o3z6PRzw7OUK",
        "trusted": false,
        "papermill": {
          "duration": 0.060762,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.248415",
          "end_time": "2020-11-02T00:03:33.309177"
        }
      },
      "source": [
        "def create_model_tabnet(seed):\n",
        "    tabnet_params = dict(\n",
        "        n_d=32,\n",
        "        n_a=32,\n",
        "        n_steps=1,\n",
        "        n_independent=1,\n",
        "        n_shared=1,\n",
        "        gamma=1.3,\n",
        "        lambda_sparse=0,\n",
        "        optimizer_fn=optim.Adam,\n",
        "        optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
        "        #optimizer_fn=AdaBelief,\n",
        "        #optimizer_params=dict(lr=2e-2, weight_decay=1e-5, weight_decouple=False),\n",
        "        mask_type=\"entmax\",\n",
        "        scheduler_params=dict(mode=\"min\", patience=5, min_lr=1e-5, threshold=1e-5, factor=0.1),\n",
        "        scheduler_fn=torch_ReduceLROnPlateau,\n",
        "        seed=seed,\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    model = TabNetRegressor(**tabnet_params)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USek84BqSPyY"
      },
      "source": [
        "# Model - NODE\n",
        "\n",
        "Neural Oblivious Decision Ensembles\n",
        "\n",
        "https://www.kaggle.com/gogo827jz/moa-neural-oblivious-decision-ensembles-tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTf4tJOFSZzH"
      },
      "source": [
        "@tf.function\n",
        "def sparsemoid(inputs: tf.Tensor):\n",
        "    return tf.clip_by_value(0.5 * inputs + 0.5, 0.0, 1.0)\n",
        "\n",
        "@tf.function\n",
        "def identity(x: tf.Tensor):\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAUXcDCnXM8d"
      },
      "source": [
        "class ODST(L.Layer):\n",
        "    def __init__(self, n_trees: int = 3, depth: int = 4, units: int = 1, threshold_init_beta: float = 1.0):\n",
        "        super(ODST, self).__init__()\n",
        "        self.initialized = False\n",
        "        self.n_trees = n_trees\n",
        "        self.depth = depth\n",
        "        self.units = units\n",
        "        self.threshold_init_beta = threshold_init_beta\n",
        "\n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        feature_selection_logits_init = tf.zeros_initializer()\n",
        "        self.feature_selection_logits = tf.Variable(\n",
        "            initial_value=feature_selection_logits_init(\n",
        "                shape=(input_shape[-1], self.n_trees, self.depth), dtype=\"float32\"\n",
        "            ),\n",
        "            trainable=True,\n",
        "            name=\"feature_selection_logits\",\n",
        "        )\n",
        "\n",
        "        feature_thresholds_init = tf.zeros_initializer()\n",
        "        self.feature_thresholds = tf.Variable(\n",
        "            initial_value=feature_thresholds_init(shape=(self.n_trees, self.depth), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "            name=\"feature_thresholds\",\n",
        "        )\n",
        "\n",
        "        log_temperatures_init = tf.ones_initializer()\n",
        "        self.log_temperatures = tf.Variable(\n",
        "            initial_value=log_temperatures_init(shape=(self.n_trees, self.depth), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "            name=\"log_temperatures\",\n",
        "        )\n",
        "\n",
        "        indices = K.arange(0, 2 ** self.depth, 1)\n",
        "        offsets = 2 ** K.arange(0, self.depth, 1)\n",
        "        bin_codes = tf.reshape(indices, (1, -1)) // tf.reshape(offsets, (-1, 1)) % 2\n",
        "        bin_codes_1hot = tf.stack([bin_codes, 1 - bin_codes], axis=-1)\n",
        "        self.bin_codes_1hot = tf.Variable(\n",
        "            initial_value=tf.cast(bin_codes_1hot, \"float32\"), trainable=False, name=\"bin_codes_1hot\"\n",
        "        )\n",
        "\n",
        "        response_init = tf.ones_initializer()\n",
        "        self.response = tf.Variable(\n",
        "            initial_value=response_init(shape=(self.n_trees, self.units, 2 ** self.depth), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "            name=\"response\",\n",
        "        )\n",
        "\n",
        "    def initialize(self, inputs):\n",
        "        feature_values = self.feature_values(inputs)\n",
        "\n",
        "        # intialize feature_thresholds\n",
        "        percentiles_q = 100 * tfp_distributions.Beta(self.threshold_init_beta, self.threshold_init_beta).sample(\n",
        "            [self.n_trees * self.depth]\n",
        "        )\n",
        "        flattened_feature_values = tf.map_fn(K.flatten, feature_values)\n",
        "        init_feature_thresholds = tf.linalg.diag_part(\n",
        "            tfp_stats.percentile(flattened_feature_values, percentiles_q, axis=0)\n",
        "        )\n",
        "\n",
        "        self.feature_thresholds.assign(tf.reshape(init_feature_thresholds, self.feature_thresholds.shape))\n",
        "\n",
        "        # intialize log_temperatures\n",
        "        self.log_temperatures.assign(\n",
        "            tfp_stats.percentile(tf.math.abs(feature_values - self.feature_thresholds), 50, axis=0)\n",
        "        )\n",
        "\n",
        "    def feature_values(self, inputs: tf.Tensor, training: bool = None):\n",
        "        feature_selectors = tfa.activations.sparsemax(self.feature_selection_logits)\n",
        "        # ^--[in_features, n_trees, depth]\n",
        "\n",
        "        feature_values = tf.einsum(\"bi,ind->bnd\", inputs, feature_selectors)\n",
        "        # ^--[batch_size, n_trees, depth]\n",
        "\n",
        "        return feature_values\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, training: bool = None):\n",
        "        if not self.initialized:\n",
        "            self.initialize(inputs)\n",
        "            self.initialized = True\n",
        "\n",
        "        feature_values = self.feature_values(inputs)\n",
        "\n",
        "        threshold_logits_a = (feature_values - self.feature_thresholds) * tf.math.exp(-self.log_temperatures)\n",
        "\n",
        "        threshold_logits_b = tf.stack([-threshold_logits_a, threshold_logits_a], axis=-1)\n",
        "        # ^--[batch_size, n_trees, depth, 2]\n",
        "\n",
        "        bins = sparsemoid(threshold_logits_b)\n",
        "        # ^--[batch_size, n_trees, depth, 2], approximately binary\n",
        "\n",
        "        bin_matches = tf.einsum(\"btds,dcs->btdc\", bins, self.bin_codes_1hot)\n",
        "        # ^--[batch_size, n_trees, depth, 2 ** depth]\n",
        "\n",
        "        response_weights = tf.math.reduce_prod(bin_matches, axis=-2)\n",
        "        # ^-- [batch_size, n_trees, 2 ** depth]\n",
        "\n",
        "        response = tf.einsum(\"bnd,ncd->bnc\", response_weights, self.response)\n",
        "        # ^-- [batch_size, n_trees, units]\n",
        "\n",
        "        return tf.reduce_sum(response, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSs_xRt6XTBL"
      },
      "source": [
        "class NODE(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        units: int = 1,\n",
        "        n_layers: int = 1,\n",
        "        output_dim=1,\n",
        "        dropout_rate=0.1,\n",
        "        link: tf.function = tf.identity,\n",
        "        n_trees: int = 3,\n",
        "        depth: int = 4,\n",
        "        threshold_init_beta: float = 1.0,\n",
        "        feature_column: Optional[L.DenseFeatures] = None,\n",
        "    ):\n",
        "        super(NODE, self).__init__()\n",
        "        self.units = units\n",
        "        self.n_layers = n_layers\n",
        "        self.n_trees = n_trees\n",
        "        self.depth = depth\n",
        "        self.units = units\n",
        "        self.threshold_init_beta = threshold_init_beta\n",
        "        self.feature_column = feature_column\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if feature_column is None:\n",
        "            self.feature = L.Lambda(identity)\n",
        "        else:\n",
        "            self.feature = feature_column\n",
        "\n",
        "        self.bn = [L.BatchNormalization() for _ in range(n_layers + 1)]\n",
        "        self.dropout = [L.Dropout(self.dropout_rate) for _ in range(n_layers + 1)]\n",
        "        self.ensemble = [\n",
        "            ODST(n_trees=n_trees, depth=depth, units=units, threshold_init_beta=threshold_init_beta)\n",
        "            for _ in range(n_layers)\n",
        "        ]\n",
        "\n",
        "        self.last_layer = L.Dense(self.output_dim)\n",
        "\n",
        "        self.link = link\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        X_a = self.feature(inputs)\n",
        "        X_b = self.bn[0](X_a, training=training)\n",
        "        X_c = self.dropout[0](X_b, training=training)\n",
        "\n",
        "        X = defaultdict(dict)\n",
        "        X[0][0] = X_c\n",
        "        for i, tree in enumerate(self.ensemble):\n",
        "            X[i][1] = tf.concat([X[i][0], tree(X[i][0])], axis=1)\n",
        "            X[i][2] = self.bn[i + 1](X[i][1], training=training)\n",
        "            X[i + 1][0] = self.dropout[i + 1](X[i][2], training=training)\n",
        "\n",
        "        return self.link(self.last_layer(X[i + 1][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMjljmU8XYiq"
      },
      "source": [
        "def create_model_node(output_dim):\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            NODE(\n",
        "                n_layers=2,\n",
        "                units=128,\n",
        "                output_dim=128,\n",
        "                dropout_rate=0.2,\n",
        "                depth=3,\n",
        "                n_trees=2,\n",
        "            ),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(128, activation=\"elu\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(128, activation=\"swish\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dense(output_dim),  # from_logits=True\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "zzOOGJtq7OUL",
        "papermill": {
          "duration": 0.047969,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.359075",
          "end_time": "2020-11-02T00:03:33.407044"
        }
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "vUVBZRkJ7OUP",
        "trusted": false,
        "papermill": {
          "duration": 0.055731,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.562741",
          "end_time": "2020-11-02T00:03:33.618472"
        }
      },
      "source": [
        "models = [\"ResNet\", \"TabNet\"]\n",
        "N_STARTS = len(models) * 3\n",
        "N_SPLITS = 5\n",
        "\n",
        "if IN_COLAB:\n",
        "    models = [\"SimpleNN\", \"ResNet\", \"TabNet\", \"NODE\"]\n",
        "    N_STARTS = len(models) * 1\n",
        "    N_SPLITS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "pW34j0Mc7OUR",
        "trusted": false,
        "papermill": {
          "duration": 0.055319,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.668112",
          "end_time": "2020-11-02T00:03:33.723431"
        }
      },
      "source": [
        "pre_train_models = [\"ResNet\", \"SimpleNN\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "QUyOmuMg7OUU",
        "trusted": false,
        "papermill": {
          "duration": 0.089391,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.791723",
          "end_time": "2020-11-02T00:03:33.881114"
        }
      },
      "source": [
        "SAVE_MODEL = True\n",
        "\n",
        "def learning(train_, train_pca_, target_, drug_, N_STARTS=6, N_SPLITS=5, do_predict=False, transfer_learning_base=None, pseudo_labeling=False):\n",
        "    oof = {}\n",
        "    predictions = {}\n",
        "\n",
        "    for seed in range(N_STARTS):\n",
        "        model_name = models[seed % len(models)]\n",
        "\n",
        "        if not do_predict and model_name not in pre_train_models:\n",
        "            continue\n",
        "\n",
        "        seed_result = pd.DataFrame(np.zeros(target_.shape))\n",
        "        prediction = pd.DataFrame(np.zeros(ss.shape))\n",
        "\n",
        "        if pseudo_labeling:\n",
        "            kfold_seed = random_seed * 10 + seed\n",
        "        elif do_predict:\n",
        "            kfold_seed = random_seed + seed\n",
        "        else:\n",
        "            kfold_seed = seed\n",
        "\n",
        "        fix_seed(kfold_seed)\n",
        "\n",
        "        if \"fold\" in drug_.columns:\n",
        "            drug_.drop([\"fold\"], axis=1, inplace=True)\n",
        "\n",
        "        # LOCATE DRUGS\n",
        "        vc = drug_.drug_id.value_counts()\n",
        "        vc1 = vc.loc[(vc==6)|(vc==12)|(vc==18)].index.sort_values()\n",
        "        vc2 = vc.loc[(vc!=6)&(vc!=12)&(vc!=18)].index.sort_values()\n",
        "\n",
        "        dct1 = {}\n",
        "        dct2 = {}\n",
        "\n",
        "        # STRATIFY DRUGS 18X OR LESS\n",
        "        skf = MultilabelStratifiedKFold(n_splits=N_SPLITS, random_state=kfold_seed, shuffle=True)\n",
        "        tmp = pd.concat([drug_, target_], axis=1).groupby('drug_id').mean().loc[vc1]\n",
        "        for fold, (idxT, idxV) in enumerate(skf.split(tmp, tmp)):\n",
        "            dd = {k:fold for k in tmp.index[idxV].values}\n",
        "            dct1.update(dd)\n",
        "\n",
        "        # STRATIFY DRUGS MORE THAN 18X\n",
        "        skf = MultilabelStratifiedKFold(n_splits=N_SPLITS, random_state=kfold_seed, shuffle=True)\n",
        "        tmp = drug_.loc[drug_.drug_id.isin(vc2)].reset_index(drop=True)\n",
        "        for fold, (idxT, idxV) in enumerate(skf.split(tmp, tmp)):\n",
        "            dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
        "            dct2.update(dd)\n",
        "\n",
        "        # ASSIGN FOLDS\n",
        "        drug_['fold'] = drug_.drug_id.map(dct1)\n",
        "        drug_.loc[drug_.fold.isna(), 'fold'] = drug_.loc[drug_.fold.isna(), 'sig_id'].map(dct2)\n",
        "        drug_.fold = drug_.fold.astype('int8')\n",
        "\n",
        "        for n in range(N_SPLITS):\n",
        "            tr = drug_[drug_[\"fold\"] != n].index\n",
        "            te = drug_[drug_[\"fold\"] == n].index\n",
        "\n",
        "            start_time = time()\n",
        "\n",
        "            # Build Model\n",
        "            if model_name == \"ResNet\":\n",
        "                model = create_model_resnet(len(train_.columns), len(train_pca_.columns), len(target_.columns))\n",
        "\n",
        "                if transfer_learning_base is not None:\n",
        "                    model_base = create_model_resnet(\n",
        "                        len(train_.columns), len(train_pca_.columns), len(transfer_learning_base.columns)\n",
        "                    )\n",
        "\n",
        "            elif model_name == \"SimpleNN\":\n",
        "                model = create_model_simple_nn(len(train_.columns), len(target_.columns))\n",
        "\n",
        "                if transfer_learning_base is not None:\n",
        "                    model_base = create_model_simple_nn(\n",
        "                        len(train_.columns), len(transfer_learning_base.columns)\n",
        "                    )\n",
        "\n",
        "            elif model_name == \"TabNet\":\n",
        "                model = create_model_tabnet(kfold_seed)\n",
        "\n",
        "            elif model_name == \"NODE\":\n",
        "                model = create_model_node(len(target_.columns))\n",
        "\n",
        "                if transfer_learning_base is not None:\n",
        "                    model_base = create_model_node(\n",
        "                        len(transfer_learning_base.columns)\n",
        "                    )\n",
        "\n",
        "            else:\n",
        "                raise \"Model name is invalid.\"\n",
        "\n",
        "            # Build Data Sets\n",
        "            if model_name == \"ResNet\":\n",
        "                x_tr = [\n",
        "                    train_.values[tr],\n",
        "                    train_pca_.values[tr],\n",
        "                ]\n",
        "                x_val = [\n",
        "                    train_.values[te],\n",
        "                    train_pca_.values[te],\n",
        "                ]\n",
        "                y_tr, y_val = target_.astype(float).values[tr], target_.astype(float).values[te]\n",
        "                x_tt = [test.values, test_pca.values]\n",
        "\n",
        "            else:\n",
        "                x_tr, x_val = train_.values[tr], train_.values[te]\n",
        "                y_tr, y_val = target_.astype(float).values[tr], target_.astype(float).values[te]\n",
        "                x_tt = test.values\n",
        "\n",
        "            if model_name == \"TabNet\":\n",
        "                checkpoint_path = f\"{model_name}_repeat:{seed}_fold:{n}\"\n",
        "\n",
        "                if transfer_learning_base is not None and model_name in pre_train_models:\n",
        "                    model.load_model(checkpoint_path + \".zip\")\n",
        "\n",
        "                model.fit(\n",
        "                    X_train=x_tr,\n",
        "                    y_train=y_tr,\n",
        "                    eval_set=[(x_val, y_val)],\n",
        "                    eval_name=[\"val\"],\n",
        "                    eval_metric=[\"logits_ll\"],\n",
        "                    max_epochs=200,\n",
        "                    patience=10,\n",
        "                    batch_size=1024,\n",
        "                    virtual_batch_size=32,\n",
        "                    num_workers=1,\n",
        "                    drop_last=False,\n",
        "                    #loss_fn=F.binary_cross_entropy_with_logits,\n",
        "                    loss_fn=SmoothBCEwLogits(smoothing=1e-6),\n",
        "                )\n",
        "\n",
        "                if SAVE_MODEL:\n",
        "                    try:\n",
        "                        os.remove(checkpoint_path)\n",
        "                    except OSError:\n",
        "                        pass\n",
        "                    model.save_model(checkpoint_path)\n",
        "\n",
        "            else:\n",
        "                model.compile(\n",
        "                    optimizer=tfa.optimizers.AdamW(lr=1e-3, weight_decay=1e-5),\n",
        "                    #optimizer=AdaBeliefOptimizer(lr=1e-3, weight_decay=1e-5),\n",
        "                    #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=1e-6),\n",
        "                    metrics=logloss,\n",
        "                )\n",
        "\n",
        "                checkpoint_path = f\"{model_name}_repeat:{seed}_fold:{n}.hdf5\"\n",
        "\n",
        "                if transfer_learning_base is not None and model_name in pre_train_models:\n",
        "                    model_base.load_weights(checkpoint_path)\n",
        "                    for layer in range(len(model_base.layers[:-1])):\n",
        "                        model.layers[layer].set_weights(model_base.layers[layer].get_weights())\n",
        "\n",
        "                if SAVE_MODEL:\n",
        "                    cb_checkpt = ModelCheckpoint(\n",
        "                        checkpoint_path,\n",
        "                        monitor=\"val_loss\",\n",
        "                        verbose=0,\n",
        "                        save_best_only=True,\n",
        "                        save_weights_only=True,\n",
        "                        mode=\"min\",\n",
        "                    )\n",
        "                reduce_lr_loss = ReduceLROnPlateau(\n",
        "                    monitor=\"val_loss\", factor=0.1, patience=5, verbose=0, min_delta=1e-5, min_lr=1e-5, mode=\"min\"\n",
        "                )\n",
        "                early_stopping = EarlyStopping(\n",
        "                    monitor=\"val_loss\",\n",
        "                    patience=10,\n",
        "                    mode=\"min\",\n",
        "                    verbose=0,\n",
        "                    min_delta=1e-5,\n",
        "                    restore_best_weights=True,\n",
        "                )\n",
        "                if SAVE_MODEL:\n",
        "                    callbacks = [cb_checkpt, reduce_lr_loss, early_stopping]\n",
        "                else:\n",
        "                    callbacks = [reduce_lr_loss, early_stopping]\n",
        "                model.fit(\n",
        "                    x_tr,\n",
        "                    y_tr,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    epochs=200,\n",
        "                    batch_size=128,\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=0,\n",
        "                )\n",
        "\n",
        "            val_predict = model.predict(x_val)\n",
        "            val_predict = 1 / (1 + np.exp(-val_predict))\n",
        "            seed_result.loc[te, :] += val_predict\n",
        "\n",
        "            if do_predict:\n",
        "                test_predict = model.predict(x_tt)\n",
        "                test_predict = 1 / (1 + np.exp(-test_predict))\n",
        "                prediction += test_predict / N_SPLITS\n",
        "\n",
        "            if model_name == \"TabNet\":\n",
        "                fold_score = np.min(model.history[\"val_logits_ll\"])\n",
        "            else:\n",
        "                fold_score = metric(target_.loc[te].values, val_predict)\n",
        "\n",
        "            print(\n",
        "                f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] {model_name}: Seed {seed}, Fold {n}:\",\n",
        "                fold_score,\n",
        "            )\n",
        "\n",
        "            K.clear_session()\n",
        "            del model\n",
        "            x = gc.collect()\n",
        "\n",
        "        oof[f\"{model_name}_{seed}\"] = seed_result\n",
        "        predictions[f\"{model_name}_{seed}\"] = prediction\n",
        "\n",
        "    return oof, predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "QEPrZIyP7OUV",
        "trusted": false,
        "papermill": {
          "duration": 1526.52396,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.929627",
          "end_time": "2020-11-02T00:29:00.453587"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "PRE_TRAIN = True\n",
        "\n",
        "if PRE_TRAIN:\n",
        "    _, _ = learning(\n",
        "        train[: non_target.shape[0]],\n",
        "        train_pca[: non_target.shape[0]],\n",
        "        non_target,\n",
        "        non_target_drug,\n",
        "        N_STARTS,\n",
        "        N_SPLITS,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "PA4xk_Tn7OUX",
        "trusted": false,
        "papermill": {
          "duration": 3311.470931,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:29:00.508861",
          "end_time": "2020-11-02T01:24:11.979792"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "oof, predictions = learning(\n",
        "    train,\n",
        "    train_pca,\n",
        "    target,\n",
        "    target_drug,\n",
        "    N_STARTS,\n",
        "    N_SPLITS,\n",
        "    do_predict=True,\n",
        "    transfer_learning_base=non_target,\n",
        "    pseudo_labeling=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "iy2jGG1j7OUY",
        "papermill": {
          "duration": 0.073609,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:12.054101",
          "end_time": "2020-11-02T01:24:12.12771"
        }
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "lx66CzK_7OUa",
        "trusted": false,
        "papermill": {
          "duration": 0.553717,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:12.20192",
          "end_time": "2020-11-02T01:24:12.755637"
        }
      },
      "source": [
        "initial_weights = [1.0 / N_STARTS for _ in range(N_STARTS)] + [1.0]\n",
        "y_true = target.values[:non_target.shape[0]]\n",
        "\n",
        "print(f\"===== OOF CV =====\")\n",
        "for key, val in oof.items():\n",
        "    print(f\"OOF Key: {key}, CV: {metric(y_true, val.values[:y_true.shape[0]])}\")\n",
        "\n",
        "oof_by_model = {\n",
        "    model: {k: v for k, v in oof.items() if k.startswith(model)}\n",
        "    for model in models\n",
        "}\n",
        "for model, oof_ in oof_by_model.items():\n",
        "    print(f\"\\n===== Model {model} CV =====\")\n",
        "    cross_validation(y_true.shape, initial_weights[:-1], y_true, oof_)\n",
        "\n",
        "print(f\"\\n===== Overall CV =====\")\n",
        "cross_validation(y_true.shape, initial_weights[:-1], y_true, oof)\n",
        "\n",
        "optimize = False\n",
        "\n",
        "if optimize:\n",
        "    # https://www.kaggle.com/gogo827jz/optimise-blending-weights-with-bonus-0#Bonus-(Lagrange-Multiplier)\n",
        "\n",
        "    def lagrange_func(params):\n",
        "        # weights, _lambda = params\n",
        "        blend_ = blend(y_true.shape, params[:-1], oof)\n",
        "        return metric(y_true, blend_) - params[-1] * (sum(params[:-1]) - 1)\n",
        "\n",
        "    grad_l = grad(lagrange_func)\n",
        "\n",
        "    def lagrange_obj(params):\n",
        "        # weights, _lambda = params\n",
        "        d = grad_l(params).tolist()\n",
        "        return d[:-1] + [sum(params[:-1]) - 1]\n",
        "\n",
        "    optimized_weights = fsolve(lagrange_obj, initial_weights)\n",
        "    cross_validation(y_true.shape, optimized_weights[:-1], y_true, oof)\n",
        "\n",
        "    print(f\"Optimized weights: {optimized_weights[:-1]}\")\n",
        "    print(f\"Check the sum of all weights: {sum(optimized_weights[:-1])}\")\n",
        "\n",
        "else:\n",
        "    optimized_weights = initial_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkapdEQ8pDfg"
      },
      "source": [
        "predictions_by_model = {\n",
        "    model: {k: v for k, v in predictions.items() if k.startswith(model)}\n",
        "    for model in models\n",
        "}\n",
        "\n",
        "blend_by_model = {\n",
        "    model: pd.DataFrame(blend(ss.shape, initial_weights[:-1], predictions_by_model[model]))\n",
        "    for model in models\n",
        "}\n",
        "\n",
        "if IN_COLAB:\n",
        "    pub_test_pseudo_label = pub_ss.drop(\"sig_id\", axis=1)\n",
        "    pub_test_pseudo_label.columns = range(206)\n",
        "    blend_by_model[\"pub_test\"] = pub_test_pseudo_label\n",
        "\n",
        "for a, b in itertools.combinations(blend_by_model.keys(), 2):\n",
        "    corr = blend_by_model[a].corrwith(blend_by_model[b], axis=1)\n",
        "    print(f\"Prediction correlation between {a} and {b}: {corr.mean()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "lfXBBv717OUb",
        "papermill": {
          "duration": 0.074835,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:12.831038",
          "end_time": "2020-11-02T01:24:12.905873"
        }
      },
      "source": [
        "# Pseudo Label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMaOyL_VxjAw"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "j4Gs-Mxq7OUc",
        "trusted": false,
        "papermill": {
          "duration": 0.085132,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:12.981005",
          "end_time": "2020-11-02T01:24:13.066137"
        }
      },
      "source": [
        "PESEUDO_LABELING = False\n",
        "\n",
        "if PESEUDO_LABELING:\n",
        "    # Blend Predictions\n",
        "    pseudo_label_df = submit_df.copy()\n",
        "    pseudo_label_df.loc[:, target.columns] = blend(ss.shape, optimized_weights[:-1], predictions)\n",
        "\n",
        "    # Preprocess Pseudo Label\n",
        "    pseudo_label_df = pseudo_label_df.loc[test_df[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
        "\n",
        "    pseudo_label_drug = pd.DataFrame(pseudo_label_df.loc[:, \"sig_id\"]).merge(drug, on='sig_id', how='left')\n",
        "    pseudo_label_drug = pseudo_label_drug.fillna(\"yyyyyyyyy\")\n",
        "\n",
        "    target_drug = target_drug.drop([\"fold\"], axis=1)\n",
        "\n",
        "    del pseudo_label_df[\"sig_id\"]\n",
        "\n",
        "    print(train.shape)\n",
        "    print(test.shape)\n",
        "\n",
        "    print(train_pca.shape)\n",
        "    print(test_pca.shape)\n",
        "\n",
        "    print(target.shape)\n",
        "    print(pseudo_label_df.shape)\n",
        "\n",
        "    print(target_drug.shape)\n",
        "    print(pseudo_label_drug.shape)\n",
        "\n",
        "    pseudo_label_df    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Cs7pF-_xlDN"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "x4wfWXGK7OUg",
        "trusted": false,
        "papermill": {
          "duration": 0.084166,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:13.451735",
          "end_time": "2020-11-02T01:24:13.535901"
        }
      },
      "source": [
        "if PESEUDO_LABELING:\n",
        "    oof, predictions = learning(\n",
        "        pd.concat([train, test], ignore_index=True),\n",
        "        pd.concat([train_pca, test_pca], ignore_index=True),\n",
        "        pd.concat([target, pseudo_label_df], ignore_index=True),\n",
        "        pd.concat([target_drug, pseudo_label_drug], ignore_index=True),\n",
        "        N_STARTS,\n",
        "        N_SPLITS,\n",
        "        do_predict=True,\n",
        "        transfer_learning_base=target,\n",
        "        pseudo_labeling=True,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "gTl48pEn7OUj",
        "papermill": {
          "duration": 0.074435,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:13.610402",
          "end_time": "2020-11-02T01:24:13.684837"
        }
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "XcpPCne97OUk",
        "trusted": false,
        "papermill": {
          "duration": 0.083685,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:13.759273",
          "end_time": "2020-11-02T01:24:13.842958"
        }
      },
      "source": [
        "if PESEUDO_LABELING:\n",
        "    for key, val in oof.items():\n",
        "        print(f\"OOF Key: {key}, CV: {metric(y_true, val.values[:y_true.shape[0]])}\")\n",
        "        \n",
        "    cross_validation(y_true.shape, initial_weights[:-1], y_true, oof)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "zghrQJ2L7OUl",
        "papermill": {
          "duration": 0.075326,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:13.91903",
          "end_time": "2020-11-02T01:24:13.994356"
        }
      },
      "source": [
        "# Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "6R71_Ar-7OUl",
        "trusted": false,
        "papermill": {
          "duration": 0.516377,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:14.069203",
          "end_time": "2020-11-02T01:24:14.58558"
        }
      },
      "source": [
        "# Weighted blend\n",
        "submit_df.loc[:, target.columns] = blend(ss.shape, optimized_weights[:-1], predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "bei97A9O7OUn",
        "trusted": false,
        "papermill": {
          "duration": 0.183091,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:14.697182",
          "end_time": "2020-11-02T01:24:14.880273"
        }
      },
      "source": [
        "# Clipping\n",
        "# submit_df.loc[:, target.columns] = submit_df.loc[:, target.columns].clip(1e-7, 1 - 1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "JHpr_90W7OUp",
        "trusted": false,
        "papermill": {
          "duration": 0.124214,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:14.982501",
          "end_time": "2020-11-02T01:24:15.106715"
        }
      },
      "source": [
        "submit_df.loc[test_df[\"cp_type\"] == \"ctl_vehicle\", target.columns] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "7mHXnoKn7OUr",
        "papermill": {
          "duration": 0.075513,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:15.182599",
          "end_time": "2020-11-02T01:24:15.258112"
        }
      },
      "source": [
        "# Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "7pdd1k1E7OUr",
        "trusted": false,
        "papermill": {
          "duration": 2.21436,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:15.333588",
          "end_time": "2020-11-02T01:24:17.547948"
        }
      },
      "source": [
        "submit_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}