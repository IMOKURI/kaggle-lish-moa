{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "ensemble-baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.041247,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:02:58.310522",
          "end_time": "2020-11-02T00:02:58.351769"
        },
        "id": "F0fdkwGB7OS3",
        "tags": []
      },
      "source": [
        "# Strategy\n",
        "\n",
        "- Preprocessing\n",
        "    - Remove ctrl_vehicle\n",
        "    - RankGauss\n",
        "    - PCA + Existing Features\n",
        "    - KMeans\n",
        "    - Basic stats\n",
        "- Model\n",
        "    - Multi head ResNet (tensorflow)\n",
        "    - TabNet (pytorch)\n",
        "- Training\n",
        "    - Pre-train with non-scored target\n",
        "    - Train with public test pseudo label\n",
        "    - Add swap noise\n",
        "    - Optimizer: Adam/AdamW with weight_decay\n",
        "    - Loss: BCE with Label smoothing + Logits\n",
        "- Prediction\n",
        "    - Ensemble above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZdDKrmgsQLA"
      },
      "source": [
        "# Score\n",
        "\n",
        "| Version | LB         | CV                      | AUC                   | CV Strategy | Fold | Seed | Execution Time | Blend Weight            |\n",
        "| ---     | ---        | ---                     | ---                   | ---         | ---  | ---  | ---            | ---                     |\n",
        "| v65     | 0.01823    | 0.01617542475216824     | -                     | old         | 5    | 3    | 4697s          | -                       |\n",
        "| v80     | 0.01837    | 0.01667020593849        | 0.6648896787429924    | new         | 5    | 3    | 4421s          | -                       |\n",
        "| v82     | 0.01831    | 0.015602756672807215    | 0.8186556909524331    | old         | 5    | 3    | 4997s          | -                       |\n",
        "| v84     | 0.01835    | 0.016074252564811344    | 0.7612621209944287    | ensemble    | 5    | 2    | 6314s          | -                       |\n",
        "| v85     | 0.01828    | 0.015434994072972176    | 0.8286985624571163    | old         | 5    | 3    | 5117s          | -                       |\n",
        "| v86     | 0.01830    | 0.015259595245052899    | 0.8452802832071501    | old         | 5    | 3    | 5009s          | -                       |\n",
        "| v87     | 0.01828    | 0.015175610900380308    | 0.8423746866733632    | old         | 5    | 3    | 5431s          | -                       |\n",
        "| v88     | 0.01833    | 0.016577553117110334    | 0.6818396112329141    | new         | 5    | 3    | 4514s          | -                       |\n",
        "| v89     | 0.01829    | 0.015641367689461786    | 0.8047873468589011    | ensemble    | 5    | 2    | 6780s          | -                       |\n",
        "| v95     | 0.01839    | 0.01663198621996857     | 0.672225012937386     | new         | 5    | 3    | 4252s          | -                       |\n",
        "| v99     | 0.01830    | 0.015189184341729493    | 0.8420906362134304    | old         | 5    | 3    | 5773s          | -                       |\n",
        "| v100    | 0.01833    | 0.01656720665455032     | 0.6840103620313757    | new         | 5    | 3    | 4680s          | [0.51164704 0.48835296] |\n",
        "| v101    | 0.01830    | 0.015183626489439578    | 0.843703992539187     | old         | 5    | 3    | 5878s          | [0.52 0.48]             |\n",
        "| v102    | -          | 0.015135838689740356    | 0.8451226146134456    | old         | 5    | 3    | 3734s          | [0.52 0.48]             |\n",
        "| v103    |            | 0.016536242921597825    | 0.6838286812341022    | new         | 7    | 3    | 5144s          | [0.54292787 0.45707213] |\n",
        "| v104    | 0.01828    | 0.015071106262833596    | 0.8482520004180092    | old         | 7    | 3    | 5960s          | [0.54 0.46]             |\n",
        "| v105    | 0.01836    | 0.01677800119352758     | 0.662181721922453     | new         | 7    | 3    | 4310s          | [0.46308043 0.53691957] |\n",
        "| v108    | 0.01839    | 0.01524083343963236     | 0.8350044030974552    | old         | 7    | 3    | 5183s          | -                       |\n",
        "| v110    | 0.01842    | 0.01537866718238164     | 0.8258931115993253    | old         | 7    | 3    | 4684s          | -                       |\n",
        "| v111    | 0.01852    | 0.01688012902109876     | 0.6367487641979563    | new         | 7    | 3    | 3900s          | [0.5614442 0.4385558]   |\n",
        "| v112    |            | 0.01684836227455031     | 0.6423886816790423    | new         | 7    | 4    | 5085s          | [0.57265942 0.42734058] |\n",
        "| v113    |            |                         |                       | new         | 7    | 3    |     s          |                         |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBc7l0UMiW_i"
      },
      "source": [
        "# Change Log\n",
        "\n",
        "- v65\n",
        "    - Remove clipping.\n",
        "    - Disable Variance Encoding.\n",
        "- v80\n",
        "    - Remove simple NN and NODE model.\n",
        "    - Increase num of seed x2 to x3.\n",
        "- v81\n",
        "    - Use ctrl_vehicle.\n",
        "- v82\n",
        "    - Remove ctrl_vehicle.\n",
        "    - Add flag to disable cv_with_drug_id. Disable for now.\n",
        "        - It may be good options that\n",
        "            - is enabled for parameter tuning.\n",
        "            - is disabled for training.\n",
        "- v83\n",
        "    - Change input of ResNet network.\n",
        "- v84\n",
        "    - Revert v83.\n",
        "    - Ensemble old and new cv predictions.\n",
        "- v85\n",
        "    - Separate model by CV.\n",
        "    - Use old cv oriented model.\n",
        "- v86\n",
        "    - Update public test label to v65.\n",
        "- v87\n",
        "    - Remove unused code.\n",
        "        - Model(SimpleNN, NODE)\n",
        "        - Pseudo labeling\n",
        "        - Variable(CV_WITH_DRUG_ID)\n",
        "        - AdaBelief\n",
        "        - Weighted ensemble\n",
        "        - Clipping\n",
        "    - Hyper parameter tuning for ResNet to use 5 layers.\n",
        "- v90\n",
        "    - Hyper parameter tuning for New CV on ResNet.\n",
        "    - Blend weight optimization.\n",
        "- v95\n",
        "    - Fix final weights.\n",
        "- v96\n",
        "    - Restore v87 model.\n",
        "    - 7 kfold.\n",
        "- v97\n",
        "    - Add swap noise.\n",
        "- v100\n",
        "    - Enable blend optimization.\n",
        "- v101\n",
        "    - Fixed blend weights.\n",
        "- v102\n",
        "    - Load pre-train model with non-scored target.\n",
        "- v105\n",
        "    - Add TabNet pre-train code. (WIP)\n",
        "    - Use more strict drug group kfold.\n",
        "- v106\n",
        "    - Disable public test pseudo labeling.\n",
        "    - Update parameters for preprocessing.\n",
        "    - Set categorical features as parameters for TabNet.\n",
        "- v107\n",
        "    - Re-enable public test pseudo labeling.\n",
        "- v108\n",
        "    - Restore v105 parameters.\n",
        "    - Disable public test pseudo labeling.\n",
        "- v109\n",
        "    - Pre-train with target and non-target.\n",
        "- v110\n",
        "    - Enable pre-train for TabNet.\n",
        "- v113\n",
        "    - Also use Simple NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQVXKcKnx3VV"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaR8v1Yr23Ml"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6l6s1ka7OS5",
        "trusted": true
      },
      "source": [
        "import sys\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aiL-BRW3Hfd",
        "trusted": true
      },
      "source": [
        "USE_PUBLIC_TEST_PSEUDO_LABEL = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqJFpiv74iX-",
        "trusted": true
      },
      "source": [
        "# \"in-notebook\", \"load-others\", \"no\"\n",
        "PRE_TRAIN_MODEL = \"load-others\"\n",
        "\n",
        "PRE_TRAIN_MODEL_DIR_SIMPLE_NN = \"../input/pre-train-simple-nn\"\n",
        "PRE_TRAIN_MODEL_DIR_RESNET = \"../input/pretrain-with-non-scored-target-baseline\"\n",
        "PRE_TRAIN_MODEL_DIR_TABNET = \"../input/pretrain-tabnet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-zw-Hv822uk",
        "trusted": true
      },
      "source": [
        "HYPER_PARAMETER_TUNING = False\n",
        "\n",
        "if HYPER_PARAMETER_TUNING:\n",
        "    TUNING_RESNET = True\n",
        "    TUNING_TABNET = False\n",
        "\n",
        "    USE_PUBLIC_TEST_PSEUDO_LABEL = False\n",
        "    PRE_TRAIN_MODEL = \"load-others\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ1uTTKS7OS4"
      },
      "source": [
        "## for Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u2KbV-T7OS8",
        "trusted": true
      },
      "source": [
        "COMPETE = \"lish-moa\"\n",
        "DATASETS = [\n",
        "    \"imokuri/moapublictestpredictions\",\n",
        "    \"optimo/pytorchtabnetpretraining\",\n",
        "    \"rahulsd91/moapublictest\",\n",
        "    \"tolgadincer/autograd\",\n",
        "    \"yasufuminakama/iterative-stratification\",\n",
        "]\n",
        "KERNEL_OUTPUTS = [\n",
        "    \"imokuri/pre-train-simple-nn\",\n",
        "    \"imokuri/pretrain-with-non-scored-target-baseline\",\n",
        "    \"imokuri/pretrain-tabnet\"\n",
        "]\n",
        "PACKAGES = [\"optuna\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCI1aEn_7OTA",
        "trusted": true
      },
      "source": [
        "if IN_COLAB:\n",
        "    !python2 -m pip uninstall kaggle -y\n",
        "    !python3 -m pip uninstall kaggle -y\n",
        "    !python3 -m pip install -U -q kaggle\n",
        "\n",
        "    !pip install -q -U git+https://github.com/IMOKURI/kaggle_on_google_colab.git\n",
        "\n",
        "    from kaggle_on_google_colab import setup\n",
        "    kaggle = setup.Setup()\n",
        "    kaggle.dirs(COMPETE)\n",
        "\n",
        "    !kaggle competitions download -p /content/zip {COMPETE}\n",
        "    !unzip -q -n /content/zip/{COMPETE}.zip -d /content/{COMPETE}/input/{COMPETE}\n",
        "    #for line in setup.exec_get_lines(cmd=f\"kaggle competitions files --csv {COMPETE} | egrep -v \\\"Warning: Looks like you're using an outdated API Version|name,size,creationDate\\\" | cut -d , -f 1\"):\n",
        "    #    !unzip -q -n /content/zip/{line.decode().strip()}.zip -d /content/{COMPETE}/input/{COMPETE}\n",
        "\n",
        "    for dataset in DATASETS:\n",
        "        dataset_name = dataset.split(\"/\")[-1]\n",
        "\n",
        "        !kaggle datasets download -p /content/zip {dataset}\n",
        "        !unzip -q -n /content/zip/{dataset_name}.zip -d /content/{COMPETE}/input/{dataset_name}\n",
        "\n",
        "    for kernel in KERNEL_OUTPUTS:\n",
        "        kernel_name = kernel.split(\"/\")[-1]\n",
        "\n",
        "        !kaggle kernels output -p /content/{COMPETE}/input/{kernel_name} {kernel}\n",
        "\n",
        "    for package_ in PACKAGES:\n",
        "        !pip install -q {package_}\n",
        "\n",
        "    !pip install -q -U tensorflow-addons\n",
        "    #!mv /content/zip/train_drug.csv /content/{COMPETE}/input/{COMPETE}/\n",
        "\n",
        "    %cd /content/{COMPETE}/output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.037487,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:02:58.390119",
          "end_time": "2020-11-02T00:02:58.427606"
        },
        "id": "zfsIxBee7OTC",
        "tags": []
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.046893,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:02:58.465372",
          "end_time": "2020-11-02T00:02:58.512265"
        },
        "id": "ZSt3tVRO7OTD",
        "trusted": true,
        "tags": []
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 2.871801,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:02:58.550511",
          "end_time": "2020-11-02T00:03:01.422312"
        },
        "id": "Di-6b3xS7OTF",
        "trusted": true,
        "tags": []
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "sys.path.append(\"../input/autograd\")\n",
        "import autograd.numpy as np\n",
        "from autograd import grad\n",
        "\n",
        "#sys.path.append(\"../input/pytorchtabnet\")\n",
        "!pip install -q ../input/pytorchtabnetpretraining/pytorch_tabnet-2.0.1-py3-none-any.whl\n",
        "from pytorch_tabnet.metrics import Metric\n",
        "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 6.004229,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:01.490703",
          "end_time": "2020-11-02T00:03:07.494932"
        },
        "id": "PZIdM0rJ7OTH",
        "trusted": true,
        "tags": []
      },
      "source": [
        "import datetime\n",
        "import gc\n",
        "import json\n",
        "import io\n",
        "import itertools\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "from time import time\n",
        "\n",
        "# import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as L\n",
        "import tensorflow.keras.regularizers as R\n",
        "import tensorflow_addons as tfa\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from scipy.optimize import fsolve, minimize\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from torch import nn\n",
        "from torch.nn.modules.loss import _WeightedLoss\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau as torch_ReduceLROnPlateau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBCQf4N63tLr",
        "trusted": true
      },
      "source": [
        "if HYPER_PARAMETER_TUNING:\n",
        "    import optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn3NWKbmPWRj",
        "trusted": true
      },
      "source": [
        "if IN_COLAB:\n",
        "    from IPython.display import SVG, display_svg\n",
        "    from tensorflow.keras.utils import model_to_dot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.051965,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:07.619035",
          "end_time": "2020-11-02T00:03:07.671"
        },
        "tags": [],
        "id": "nrm75bEQ7OTL",
        "trusted": true
      },
      "source": [
        "MIXED_PRECISION = False\n",
        "XLA_ACCELERATE = True\n",
        "\n",
        "if MIXED_PRECISION:\n",
        "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "\n",
        "    if tpu:\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_bfloat16\")\n",
        "    else:\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
        "    mixed_precision.set_policy(policy)\n",
        "    print(\"Mixed precision enabled\")\n",
        "\n",
        "if XLA_ACCELERATE:\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "    print(\"Accelerated Linear Algebra enabled\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.048041,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:07.711873",
          "end_time": "2020-11-02T00:03:07.759914"
        },
        "id": "8aMe4rbK7OTN",
        "trusted": true,
        "tags": []
      },
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.043281,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:07.801312",
          "end_time": "2020-11-02T00:03:07.844593"
        },
        "id": "ff-lKRvA7OTP",
        "tags": []
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.405788,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:07.884323",
          "end_time": "2020-11-02T00:03:08.290111"
        },
        "id": "vYlmC0dI7OTP",
        "trusted": true,
        "tags": []
      },
      "source": [
        "def fix_seed(seed=2020):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "random_seed = 22\n",
        "fix_seed(random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRRBictUC72K",
        "trusted": true
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
        "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS_yBdE7tdSR"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWfvin6dtein",
        "trusted": true
      },
      "source": [
        "# https://www.kaggle.com/markpeng/deepinsight-efficientnet-b3-noisystudent/comments#1075013\n",
        "\n",
        "def add_swap_noise(index, X, train_):\n",
        "    swap_prob=0.15\n",
        "    swap_portion=0.1\n",
        "\n",
        "    for i in range(len(index)):\n",
        "        if np.random.rand() < swap_prob:\n",
        "            swap_index = np.random.randint(train_.shape[0], size=1)[0]\n",
        "            # Select only gene expression and cell viability features\n",
        "            swap_features = np.random.choice(\n",
        "                np.array(range(2, train_.shape[1])),\n",
        "                size=int(train_.shape[1] * swap_portion),\n",
        "                replace=False\n",
        "            )\n",
        "            X[i, swap_features] = train_[swap_index, swap_features]\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyamWSs0M93B"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.048342,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:08.330615",
          "end_time": "2020-11-02T00:03:08.378957"
        },
        "id": "Ysy4T3ZM7OTR",
        "trusted": true,
        "tags": []
      },
      "source": [
        "# Evaluation Metric with sigmoid applied and clipping\n",
        "\n",
        "## for tensorflow\n",
        "def logloss(y_true, y_pred):\n",
        "    logits = 1 / (1 + K.exp(-y_pred))\n",
        "    aux = (1 - y_true) * K.log(1 - logits + 1e-15) + y_true * K.log(logits + 1e-15)\n",
        "    return K.mean(-aux)\n",
        "\n",
        "\n",
        "## for pytorch\n",
        "class LogitsLogLoss(Metric):\n",
        "    def __init__(self):\n",
        "        self._name = \"logits_ll\"\n",
        "        self._maximize = False\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        logits = 1 / (1 + np.exp(-y_pred))\n",
        "        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n",
        "        return np.mean(-aux)\n",
        "\n",
        "\n",
        "## for overall\n",
        "## [Fast Numpy Log Loss] https://www.kaggle.com/gogo827jz/optimise-blending-weights-4-5x-faster-log-loss\n",
        "def metric(y_true, y_pred):\n",
        "    loss = 0\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        loss += -np.mean(\n",
        "            y_true[:, i] * np.log(y_pred[:, i] + 1e-15) + (1 - y_true[:, i]) * np.log(1 - y_pred[:, i] + 1e-15)\n",
        "        )\n",
        "    return loss / y_pred.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAAH9Q1VNc9m"
      },
      "source": [
        "## Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtIMMq6Y7OTY",
        "trusted": true
      },
      "source": [
        "# https://www.kaggle.com/felipebihaiek/torch-continued-from-auxiliary-targets-smoothing\n",
        "class SmoothBCEwLogits(_WeightedLoss):\n",
        "    def __init__(self, weight=None, reduction=\"mean\", smoothing=0.0):\n",
        "        super().__init__(weight=weight, reduction=reduction)\n",
        "        self.smoothing = smoothing\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    @staticmethod\n",
        "    def _smooth(targets: torch.Tensor, n_labels: int, smoothing=0.0):\n",
        "        assert 0 <= smoothing < 1\n",
        "        with torch.no_grad():\n",
        "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
        "        return targets\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1), self.smoothing)\n",
        "        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)\n",
        "\n",
        "        if self.reduction == \"sum\":\n",
        "            loss = loss.sum()\n",
        "        elif self.reduction == \"mean\":\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HEx7QqGNqZ8"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.05356,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:08.509901",
          "end_time": "2020-11-02T00:03:08.563461"
        },
        "id": "fgO6t6pg7OTV",
        "trusted": true,
        "tags": []
      },
      "source": [
        "# Blend oof predictions\n",
        "def blend(size, weights, oof):\n",
        "    blend_ = np.zeros(size)\n",
        "    for i, key in enumerate(oof.keys()):\n",
        "        blend_ += weights[i] * oof[key].values[: blend_.shape[0], : blend_.shape[1]]\n",
        "    return blend_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcsCWnZeovLR",
        "trusted": true
      },
      "source": [
        "def cross_validation(size, weight, y_true, oof):\n",
        "    x = size[0]\n",
        "    blend_ = blend(y_true[:x].shape, weight, oof)\n",
        "\n",
        "    aucs = []\n",
        "    for task_id in range(blend_.shape[1]):\n",
        "        aucs.append(roc_auc_score(y_true=y_true[:x, task_id], y_score=blend_[:, task_id]))\n",
        "\n",
        "    CV = metric(y_true[:x], blend_)\n",
        "    AUC = np.mean(aucs)\n",
        "    print(f\"Blended CV: {CV}, AUC : {AUC}\")\n",
        "\n",
        "    return CV, AUC, pd.DataFrame(blend_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.041079,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:08.703851",
          "end_time": "2020-11-02T00:03:08.74493"
        },
        "id": "iotU3PZB7OTa",
        "tags": []
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 7.69805,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:08.788115",
          "end_time": "2020-11-02T00:03:16.486165"
        },
        "id": "p03erQnA7OTa",
        "trusted": true,
        "tags": []
      },
      "source": [
        "train_df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n",
        "test_df = pd.read_csv(\"../input/lish-moa/test_features.csv\")\n",
        "target_df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\n",
        "non_target_df = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\n",
        "submit_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n",
        "drug_df = pd.read_csv(\"../input/lish-moa/train_drug.csv\")\n",
        "\n",
        "pub_test_df = pd.read_csv(\"../input/moapublictest/test_features.csv\")\n",
        "# pub_submit_df = pd.read_csv(\"../input/moapublictestpredictions/submission-blendblendblend.csv\")\n",
        "pub_submit_df = pd.read_csv(\"../input/moapublictestpredictions/submission-v65-old_best_lb.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.130368,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:16.529819",
          "end_time": "2020-11-02T00:03:16.660187"
        },
        "id": "hYN5WkgA7OTc",
        "trusted": true,
        "tags": []
      },
      "source": [
        "train = train_df.copy()\n",
        "test = test_df.copy()\n",
        "target = target_df.copy()\n",
        "non_target = non_target_df.copy()\n",
        "ss = submit_df.copy()\n",
        "drug = drug_df.copy()\n",
        "\n",
        "pub_test = pub_test_df.copy()\n",
        "pub_ss = pub_submit_df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_HmJN8tnpob"
      },
      "source": [
        "## Use public test data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.134832,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:16.701536",
          "end_time": "2020-11-02T00:03:16.836368"
        },
        "id": "1ETxnt8B7OTe",
        "trusted": true,
        "tags": []
      },
      "source": [
        "# Merge public test data (and pseudo label) into train data\n",
        "if USE_PUBLIC_TEST_PSEUDO_LABEL:\n",
        "    train = pd.concat([train, pub_test]).reset_index(drop=True)\n",
        "    target = pd.concat([target, pub_ss]).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.087015,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:16.877809",
          "end_time": "2020-11-02T00:03:16.964824"
        },
        "tags": [],
        "id": "Cn_3SXzg7OTg",
        "trusted": true
      },
      "source": [
        "target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.042495,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.010293",
          "end_time": "2020-11-02T00:03:17.052788"
        },
        "id": "7LA6ekI07OTi",
        "tags": []
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.064235,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.09555",
          "end_time": "2020-11-02T00:03:17.159785"
        },
        "id": "3DM9pkDt7OTj",
        "trusted": true,
        "tags": []
      },
      "source": [
        "train.loc[:, \"cp_dose\"] = train.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})\n",
        "test.loc[:, \"cp_dose\"] = test.loc[:, \"cp_dose\"].map({\"D1\": 0, \"D2\": 1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.056203,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.202413",
          "end_time": "2020-11-02T00:03:17.258616"
        },
        "id": "jx46CYog7OTk",
        "trusted": true,
        "tags": []
      },
      "source": [
        "train.loc[:, \"cp_time\"] = train.loc[:, \"cp_time\"].map({24: 0, 48: 1, 72: 2})\n",
        "test.loc[:, \"cp_time\"] = test.loc[:, \"cp_time\"].map({24: 0, 48: 1, 72: 2})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.042914,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.300561",
          "end_time": "2020-11-02T00:03:17.343475"
        },
        "id": "aQWxpszg7OTm",
        "tags": []
      },
      "source": [
        "## Remove ctrl_vehicle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.283079,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.385995",
          "end_time": "2020-11-02T00:03:17.669074"
        },
        "id": "0HYbBuVW7OTm",
        "trusted": true,
        "tags": []
      },
      "source": [
        "USE_CTRL_VEHICLE = False\n",
        "\n",
        "if USE_CTRL_VEHICLE:\n",
        "    train.loc[:, \"cp_type\"] = train.loc[:, \"cp_type\"].map({\"ctl_vehicle\": 0, \"trt_cp\": 1})\n",
        "    test.loc[:, \"cp_type\"] = test.loc[:, \"cp_type\"].map({\"ctl_vehicle\": 0, \"trt_cp\": 1})\n",
        "\n",
        "else:\n",
        "    target = target.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
        "    non_target = non_target.loc[train[: train_df.shape[0]][\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
        "\n",
        "    train = train.loc[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
        "\n",
        "    train = train.drop(\"cp_type\", axis=1)\n",
        "    test = test.drop(\"cp_type\", axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCzLe0NasfYe"
      },
      "source": [
        "## Merge drug_id into training data\n",
        "\n",
        "https://www.kaggle.com/c/lish-moa/discussion/195195"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLso2JtqsdQi",
        "trusted": true
      },
      "source": [
        "target_drug = pd.DataFrame(target.loc[:, \"sig_id\"]).merge(drug, on=\"sig_id\", how=\"left\")\n",
        "non_target_drug = pd.DataFrame(non_target.loc[:, \"sig_id\"]).merge(drug, on=\"sig_id\", how=\"left\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtxsR6XoeM0T",
        "trusted": true
      },
      "source": [
        "target_drug = target_drug.fillna(\"xxxxxxxxx\")\n",
        "non_target_drug = non_target_drug.fillna(\"xxxxxxxxx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4w0OboIswhr",
        "trusted": true
      },
      "source": [
        "target_drug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvkK0m0xsoTz"
      },
      "source": [
        "## Remove sig_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.054757,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.872423",
          "end_time": "2020-11-02T00:03:17.92718"
        },
        "id": "5KBPZw3p7OTq",
        "trusted": true,
        "tags": []
      },
      "source": [
        "del train[\"sig_id\"]\n",
        "del target[\"sig_id\"]\n",
        "del non_target[\"sig_id\"]\n",
        "del test[\"sig_id\"]\n",
        "del ss[\"sig_id\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.083712,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:17.970028",
          "end_time": "2020-11-02T00:03:18.05374"
        },
        "tags": [],
        "id": "GCSu_X1_7OTs",
        "trusted": true
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.058698,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:18.099745",
          "end_time": "2020-11-02T00:03:18.158443"
        },
        "tags": [],
        "id": "JZaiV9v_7OTv",
        "trusted": true
      },
      "source": [
        "print(train.shape)\n",
        "print(target.shape)\n",
        "print(non_target.shape)\n",
        "\n",
        "print(test.shape)\n",
        "print(ss.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.044814,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:18.206614",
          "end_time": "2020-11-02T00:03:18.251428"
        },
        "id": "x6rjhO-b7OTx",
        "tags": []
      },
      "source": [
        "## Rank Gauss\n",
        "\n",
        "https://www.kaggle.com/nayuts/moa-pytorch-nn-pca-rankgauss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 9.414623,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:18.29612",
          "end_time": "2020-11-02T00:03:27.710743"
        },
        "id": "jUsSMkTK7OTx",
        "trusted": true,
        "tags": []
      },
      "source": [
        "g_cols = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
        "c_cols = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
        "\n",
        "for col in g_cols + c_cols:\n",
        "    transformer = QuantileTransformer(n_quantiles=100, random_state=random_seed, output_distribution=\"normal\")\n",
        "\n",
        "    vec_len = len(train[col].values)\n",
        "    vec_len_test = len(test[col].values)\n",
        "\n",
        "    raw_vec = train[col].values.reshape(vec_len, 1)\n",
        "    transformer.fit(raw_vec)\n",
        "\n",
        "    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
        "    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.08184,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:27.771441",
          "end_time": "2020-11-02T00:03:27.853281"
        },
        "tags": [],
        "id": "VY4_EigR7OT0",
        "trusted": true
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.04586,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:27.900605",
          "end_time": "2020-11-02T00:03:27.946465"
        },
        "id": "DHiSL9lh7OT2",
        "tags": []
      },
      "source": [
        "## PCA features (+ Existing features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 2.738443,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:27.992409",
          "end_time": "2020-11-02T00:03:30.730852"
        },
        "id": "vSiyBS5s7OT2",
        "trusted": true,
        "tags": []
      },
      "source": [
        "# g-\n",
        "n_comp = 50\n",
        "\n",
        "data = pd.concat([pd.DataFrame(train[g_cols]), pd.DataFrame(test[g_cols])])\n",
        "data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[g_cols])\n",
        "train2 = data2[: train.shape[0]]\n",
        "test2 = data2[-test.shape[0] :]\n",
        "\n",
        "train2 = pd.DataFrame(train2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
        "test2 = pd.DataFrame(test2, columns=[f\"pca_G-{i}\" for i in range(n_comp)])\n",
        "\n",
        "train = pd.concat((train, train2), axis=1)\n",
        "test = pd.concat((test, test2), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.483929,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:30.777031",
          "end_time": "2020-11-02T00:03:31.26096"
        },
        "id": "4m1Xajv77OT4",
        "trusted": true,
        "tags": []
      },
      "source": [
        "# c-\n",
        "n_comp = 15\n",
        "\n",
        "data = pd.concat([pd.DataFrame(train[c_cols]), pd.DataFrame(test[c_cols])])\n",
        "data2 = PCA(n_components=n_comp, random_state=random_seed).fit_transform(data[c_cols])\n",
        "train2 = data2[: train.shape[0]]\n",
        "test2 = data2[-test.shape[0] :]\n",
        "\n",
        "train2 = pd.DataFrame(train2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
        "test2 = pd.DataFrame(test2, columns=[f\"pca_C-{i}\" for i in range(n_comp)])\n",
        "\n",
        "train = pd.concat((train, train2), axis=1)\n",
        "test = pd.concat((test, test2), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.084273,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:31.308289",
          "end_time": "2020-11-02T00:03:31.392562"
        },
        "tags": [],
        "id": "2vVAG1aL7OT6",
        "trusted": true
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.324282,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:31.445162",
          "end_time": "2020-11-02T00:03:31.769444"
        },
        "id": "GIHl5F3x7OT7",
        "trusted": true,
        "tags": []
      },
      "source": [
        "train_pca = train.copy()\n",
        "test_pca = test.copy()\n",
        "\n",
        "train_pca.drop(g_cols, axis=1, inplace=True)\n",
        "test_pca.drop(g_cols, axis=1, inplace=True)\n",
        "\n",
        "train_pca.drop(c_cols, axis=1, inplace=True)\n",
        "test_pca.drop(c_cols, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.087344,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:31.838133",
          "end_time": "2020-11-02T00:03:31.925477"
        },
        "tags": [],
        "id": "nJyx4dSR7OT9",
        "trusted": true
      },
      "source": [
        "train_pca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.04964,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:31.976171",
          "end_time": "2020-11-02T00:03:32.025811"
        },
        "id": "ujMOcMFG7OUD",
        "tags": []
      },
      "source": [
        "## feature Selection using Variance Encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.637637,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:32.072952",
          "end_time": "2020-11-02T00:03:32.710589"
        },
        "id": "OR4sYLWT7OUE",
        "trusted": true,
        "tags": []
      },
      "source": [
        "# https://www.kaggle.com/c/lish-moa/discussion/194973#1067941\n",
        "if False:\n",
        "\n",
        "    var_threshold = 0.5\n",
        "\n",
        "    data = train.append(test)\n",
        "    ve_columns = (data.iloc[:, 2:].var() >= var_threshold).values\n",
        "    ve_data = data.iloc[:, 2:].loc[:, ve_columns]\n",
        "\n",
        "    ve_train = ve_data[: train.shape[0]]\n",
        "    ve_test = ve_data[-test.shape[0] :]\n",
        "\n",
        "    train = pd.DataFrame(train[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
        "    train = pd.concat([train, ve_train], axis=1)\n",
        "\n",
        "    test = pd.DataFrame(test[[\"cp_time\", \"cp_dose\"]].values.reshape(-1, 2), columns=[\"cp_time\", \"cp_dose\"])\n",
        "    test = pd.concat([test, ve_test], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.09962,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:32.764308",
          "end_time": "2020-11-02T00:03:32.863928"
        },
        "id": "lvb-RhDK7OUG",
        "trusted": true,
        "tags": []
      },
      "source": [
        "# train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekFLgTbpKfA4"
      },
      "source": [
        "## KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA5PfEXjKg5O",
        "trusted": true
      },
      "source": [
        "%%time\n",
        "\n",
        "features_g = [col for col in train.columns if col.startswith(\"g-\")]\n",
        "features_c = [col for col in train.columns if col.startswith(\"c-\")]\n",
        "\n",
        "\n",
        "def fe_cluster(train_, test_, n_clusters_g=35, n_clusters_c=5):\n",
        "    def create_cluster(tr, te, features, kind=\"g\", n_clusters=n_clusters_g):\n",
        "        tmp_train_ = tr[features].copy()\n",
        "        tmp_test_ = te[features].copy()\n",
        "        data = pd.concat([tmp_train_, tmp_test_], axis=0)\n",
        "\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed).fit(data)\n",
        "\n",
        "        tr[f\"clusters_{kind}\"] = kmeans.labels_[: tr.shape[0]]\n",
        "        te[f\"clusters_{kind}\"] = kmeans.labels_[-te.shape[0] :]\n",
        "        tr = pd.get_dummies(tr, columns=[f\"clusters_{kind}\"])\n",
        "        te = pd.get_dummies(te, columns=[f\"clusters_{kind}\"])\n",
        "        return tr, te\n",
        "\n",
        "    train_, test_ = create_cluster(train_, test_, features_g, kind=\"g\", n_clusters=n_clusters_g)\n",
        "    train_, test_ = create_cluster(train_, test_, features_c, kind=\"c\", n_clusters=n_clusters_c)\n",
        "    return train_, test_\n",
        "\n",
        "\n",
        "train, test = fe_cluster(train, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ7Cetl7KjfY",
        "trusted": true
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQeImT1CKmF1"
      },
      "source": [
        "## Basic stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-6Zi3IiKoFF",
        "trusted": true
      },
      "source": [
        "for stats in [\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]:\n",
        "    train[\"g_\" + stats] = getattr(train[features_g], stats)(axis=1)\n",
        "    train[\"c_\" + stats] = getattr(train[features_c], stats)(axis=1)\n",
        "    train[\"gc_\" + stats] = getattr(train[features_g + features_c], stats)(axis=1)\n",
        "\n",
        "    test[\"g_\" + stats] = getattr(test[features_g], stats)(axis=1)\n",
        "    test[\"c_\" + stats] = getattr(test[features_c], stats)(axis=1)\n",
        "    test[\"gc_\" + stats] = getattr(test[features_g + features_c], stats)(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi0-UnkaKqaw",
        "trusted": true
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEU0TZF1vTgF"
      },
      "source": [
        "## Pick up categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0XGL1xtvXQp"
      },
      "source": [
        "if False:\n",
        "    features_cat = [col for col in train.columns if col.startswith(\"cp_\") or col.startswith(\"clusters_\")]\n",
        "    features_cat_index = [train.columns.get_loc(col) for col in features_cat]\n",
        "\n",
        "    train_cat = train[features_cat].values.astype(\"int8\")\n",
        "    test_cat = test[features_cat].values.astype(\"int8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnDew1q2w5rb"
      },
      "source": [
        "#features_cat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM7P7qCtCqal"
      },
      "source": [
        "## Reduce mem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwUNWDd6DH9x",
        "trusted": true
      },
      "source": [
        "train = reduce_mem_usage(train)\n",
        "test = reduce_mem_usage(test)\n",
        "\n",
        "train_pca = reduce_mem_usage(train_pca)\n",
        "test_pca = reduce_mem_usage(test_pca)\n",
        "\n",
        "target = reduce_mem_usage(target)\n",
        "non_target = reduce_mem_usage(non_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VXN_zaO4B5T"
      },
      "source": [
        "# Model - Simple NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toj58xHR4Em6"
      },
      "source": [
        "def create_simple_nn(num_col, output_dim):\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            L.Input(num_col),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dense(1500),\n",
        "            L.LeakyReLU(),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.5),\n",
        "            L.Dense(1250),\n",
        "            L.LeakyReLU(),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.35),\n",
        "            L.Dense(1000),\n",
        "            L.LeakyReLU(),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.3),\n",
        "            L.Dense(750),\n",
        "            L.LeakyReLU(),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.25),\n",
        "            L.Dense(output_dim),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.055601,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:32.921976",
          "end_time": "2020-11-02T00:03:32.977577"
        },
        "id": "nUBSHNuL7OUI",
        "tags": []
      },
      "source": [
        "# Model - Multi input ResNet\n",
        "\n",
        "https://www.kaggle.com/rahulsd91/moa-multi-input-resnet-model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.069892,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.031525",
          "end_time": "2020-11-02T00:03:33.101417"
        },
        "id": "w4ySRylL7OUI",
        "trusted": true,
        "tags": []
      },
      "source": [
        "def create_model_resnet(n_features, n_features_2, n_labels):\n",
        "    input_1 = L.Input(shape=(n_features,), name=\"Input1\")\n",
        "    input_2 = L.Input(shape=(n_features_2,), name=\"Input2\")\n",
        "\n",
        "    block_1 = tf.keras.Sequential(\n",
        "        [\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(128, activation=\"elu\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(512, activation=\"selu\")),\n",
        "        ],\n",
        "        name=\"Block1\",\n",
        "    )\n",
        "\n",
        "    output_1 = block_1(input_1)\n",
        "    connection_1 = L.Concatenate(name=\"Connection1\")([input_2, output_1])\n",
        "\n",
        "    block_2 = tf.keras.Sequential(\n",
        "        [\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"swish\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"elu\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(256, activation=\"relu\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(1024, activation=\"relu\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(512, activation=\"relu\")),\n",
        "        ],\n",
        "        name=\"Block2\",\n",
        "    )\n",
        "\n",
        "    output_2 = block_2(connection_1)\n",
        "    connection_2 = L.Average(name=\"Connection2\")([output_1, output_2])\n",
        "\n",
        "    block_3 = tf.keras.Sequential(\n",
        "        [\n",
        "            L.BatchNormalization(),\n",
        "            tfa.layers.WeightNormalization(L.Dense(128, activation=\"selu\")),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(1024, activation=\"elu\")),\n",
        "            L.BatchNormalization(),\n",
        "        ],\n",
        "        name=\"Block3\",\n",
        "    )\n",
        "\n",
        "    output_3 = block_3(connection_2)\n",
        "\n",
        "    # output = L.Dense(n_labels, activation=\"sigmoid\", name=\"Output\")(output_3)\n",
        "    output = L.Dense(n_labels, name=\"Output\")(output_3)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8ouPy9-qMSz",
        "trusted": true
      },
      "source": [
        "if IN_COLAB:\n",
        "    model_test = create_model_resnet(len(train.columns), len(train_pca.columns), len(target.columns))\n",
        "    model_test.summary()\n",
        "    display_svg(SVG(model_to_dot(model_test, show_shapes=True, dpi=72).create(prog=\"dot\", format=\"svg\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCGqI-8fyv7S",
        "trusted": true
      },
      "source": [
        "def create_model_resnet_tuning(n_features, n_features_2, n_labels, params):\n",
        "    n_hidden_layers = params[\"n_layers\"]\n",
        "    units = params[\"units\"]\n",
        "    activations = params[\"activations\"]\n",
        "\n",
        "    input_1 = L.Input(shape=(n_features,), name=\"Input1\")\n",
        "    input_2 = L.Input(shape=(n_features_2,), name=\"Input2\")\n",
        "\n",
        "    block_1 = tf.keras.Sequential(\n",
        "        [\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(units[-3], activation=activations[-4])),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(units[n_hidden_layers - 1], activation=activations[-3])),\n",
        "        ],\n",
        "        name=\"Block1\",\n",
        "    )\n",
        "\n",
        "    output_1 = block_1(input_1)\n",
        "    connection_1 = L.Concatenate(name=\"Connection1\")([input_2, output_1])\n",
        "\n",
        "    layers_2 = []\n",
        "    for i in range(n_hidden_layers):\n",
        "        layers_2 += [\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(units[i], activation=activations[i])),\n",
        "        ]\n",
        "    block_2 = tf.keras.Sequential(layers_2, name=\"Block2\")\n",
        "\n",
        "    output_2 = block_2(connection_1)\n",
        "    connection_2 = L.Average(name=\"Connection2\")([output_1, output_2])\n",
        "\n",
        "    block_3 = tf.keras.Sequential(\n",
        "        [\n",
        "            L.BatchNormalization(),\n",
        "            tfa.layers.WeightNormalization(L.Dense(units[-2], activation=activations[-2])),\n",
        "            L.BatchNormalization(),\n",
        "            L.Dropout(0.2),\n",
        "            tfa.layers.WeightNormalization(L.Dense(units[-1], activation=activations[-1])),\n",
        "            L.BatchNormalization(),\n",
        "        ],\n",
        "        name=\"Block3\",\n",
        "    )\n",
        "\n",
        "    output_3 = block_3(connection_2)\n",
        "\n",
        "    # output = L.Dense(n_labels, activation=\"sigmoid\", name=\"Output\")(output_3)\n",
        "    output = L.Dense(n_labels, name=\"Output\")(output_3)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.048086,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.151294",
          "end_time": "2020-11-02T00:03:33.19938"
        },
        "id": "fiC-Q8MQ7OUK",
        "tags": []
      },
      "source": [
        "# Model - TabNet\n",
        "\n",
        "- [Pre-train](https://www.kaggle.com/optimo/selfsupervisedtabnet/notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.060762,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.248415",
          "end_time": "2020-11-02T00:03:33.309177"
        },
        "id": "o3z6PRzw7OUK",
        "trusted": true,
        "tags": []
      },
      "source": [
        "def create_model_tabnet(seed, pre_train=False):\n",
        "    tabnet_params = dict(\n",
        "        n_d=32,\n",
        "        n_a=32,\n",
        "        n_steps=1,\n",
        "        n_independent=1,  # 2 is better CV than 1, but need more time\n",
        "        n_shared=1,  # same above\n",
        "        gamma=1.3,\n",
        "        lambda_sparse=0,\n",
        "        #cat_dims=[len(np.unique(train_cat[:, i])) for i in range(train_cat.shape[1])],\n",
        "        #cat_emb_dim=[1] * train_cat.shape[1],\n",
        "        #cat_idxs=features_cat_index,\n",
        "        optimizer_fn=optim.Adam,\n",
        "        optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
        "        mask_type=\"entmax\",\n",
        "        scheduler_params=dict(mode=\"min\", patience=5, min_lr=1e-5, threshold=1e-5, factor=0.1),\n",
        "        scheduler_fn=torch_ReduceLROnPlateau,\n",
        "        seed=seed,\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    if pre_train:\n",
        "        model = TabNetPretrainer(**tabnet_params)\n",
        "    else:\n",
        "        model = TabNetRegressor(**tabnet_params)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJr0rKKtTOBR",
        "trusted": true
      },
      "source": [
        "def create_model_tabnet_tuning(seed, params=None):\n",
        "    tabnet_params = dict(\n",
        "        n_d=params[\"n_d\"],\n",
        "        n_a=params[\"n_a\"],\n",
        "        n_steps=1,\n",
        "        n_independent=1,\n",
        "        n_shared=1,\n",
        "        gamma=1.3,\n",
        "        lambda_sparse=0,\n",
        "        optimizer_fn=optim.Adam,\n",
        "        optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
        "        mask_type=\"entmax\",\n",
        "        scheduler_params=dict(mode=\"min\", patience=5, min_lr=1e-5, threshold=1e-5, factor=0.1),\n",
        "        scheduler_fn=torch_ReduceLROnPlateau,\n",
        "        seed=seed,\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    model = TabNetRegressor(**tabnet_params)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.047969,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.359075",
          "end_time": "2020-11-02T00:03:33.407044"
        },
        "id": "zzOOGJtq7OUL",
        "tags": []
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.055731,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.562741",
          "end_time": "2020-11-02T00:03:33.618472"
        },
        "id": "vUVBZRkJ7OUP",
        "trusted": true,
        "tags": []
      },
      "source": [
        "models = [\n",
        "    {\"model_name\": \"SimpleNN\", \"cv\": \"with_drug_id\"},\n",
        "    #{\"model_name\": \"SimpleNN\", \"cv\": \"without_drug_id\"},\n",
        "    {\"model_name\": \"ResNet\", \"cv\": \"with_drug_id\"},\n",
        "    #{\"model_name\": \"ResNet\", \"cv\": \"without_drug_id\"},\n",
        "    {\"model_name\": \"TabNet\", \"cv\": \"with_drug_id\"},\n",
        "    #{\"model_name\": \"TabNet\", \"cv\": \"without_drug_id\"},\n",
        "]\n",
        "N_SEED = 3\n",
        "N_STARTS = len(models) * N_SEED\n",
        "N_SPLITS = 7\n",
        "pre_train_models = [\"ResNet\", \"SimpleNN\", \"TabNet\"]  # , \"TabNet\"\n",
        "\n",
        "if IN_COLAB:\n",
        "    models = [\n",
        "        {\"model_name\": \"SimpleNN\", \"cv\": \"with_drug_id\"},\n",
        "        #{\"model_name\": \"SimpleNN\", \"cv\": \"without_drug_id\"},\n",
        "        {\"model_name\": \"ResNet\", \"cv\": \"with_drug_id\"},\n",
        "        #{\"model_name\": \"ResNet\", \"cv\": \"without_drug_id\"},\n",
        "        {\"model_name\": \"TabNet\", \"cv\": \"with_drug_id\"},\n",
        "        #{\"model_name\": \"TabNet\", \"cv\": \"without_drug_id\"},\n",
        "    ]\n",
        "    N_SEED = 1\n",
        "    N_STARTS = len(models) * N_SEED\n",
        "    N_SPLITS = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.089391,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.791723",
          "end_time": "2020-11-02T00:03:33.881114"
        },
        "id": "QUyOmuMg7OUU",
        "trusted": true,
        "tags": []
      },
      "source": [
        "def learning(\n",
        "    train_,\n",
        "    train_pca_,\n",
        "    target_,\n",
        "    drug_,\n",
        "    test_,\n",
        "    test_pca_,\n",
        "    N_STARTS=6,\n",
        "    N_SPLITS=5,\n",
        "    train_flags=[\"normal\"],\n",
        "    transfer_learning_base=None,\n",
        "    params=None,\n",
        "):\n",
        "    oof = {}\n",
        "    predictions = {}\n",
        "\n",
        "    for seed in range(N_STARTS):\n",
        "        model_name = models[seed % len(models)][\"model_name\"]\n",
        "        cv = models[seed % len(models)][\"cv\"]\n",
        "\n",
        "        if (\"pre_train\" in train_flags and model_name not in pre_train_models):\n",
        "            continue\n",
        "            \n",
        "        seed_result = pd.DataFrame(np.zeros(target_.shape))\n",
        "        prediction = pd.DataFrame(np.zeros(ss.shape))\n",
        "\n",
        "        if \"pre_train\" in train_flags:\n",
        "            kfold_seed = random_seed + seed\n",
        "        else:\n",
        "            kfold_seed = seed\n",
        "\n",
        "        fix_seed(kfold_seed)\n",
        "\n",
        "        if \"fold\" in drug_.columns:\n",
        "            drug_.drop([\"fold\"], axis=1, inplace=True)\n",
        "\n",
        "        # LOCATE DRUGS\n",
        "        vc = drug_.drug_id.value_counts()\n",
        "        #vc1 = vc.loc[(vc == 6) | (vc == 12) | (vc == 18)].index.sort_values()\n",
        "        #vc2 = vc.loc[(vc != 6) & (vc != 12) & (vc != 18)].index.sort_values()\n",
        "        vc1 = vc.loc[vc <= 19].index.sort_values()\n",
        "        vc2 = vc.loc[vc > 19].index.sort_values()\n",
        "        \n",
        "        dct1 = {}\n",
        "        dct2 = {}\n",
        "\n",
        "        # STRATIFY DRUGS 18X OR LESS\n",
        "        skf = MultilabelStratifiedKFold(n_splits=N_SPLITS, random_state=kfold_seed, shuffle=True)\n",
        "        tmp = pd.concat([drug_, target_], axis=1).groupby(\"drug_id\").mean().loc[vc1]\n",
        "        for fold, (idxT, idxV) in enumerate(skf.split(tmp, tmp)):\n",
        "            dd = {k: fold for k in tmp.index[idxV].values}\n",
        "            dct1.update(dd)\n",
        "\n",
        "        # STRATIFY DRUGS MORE THAN 18X\n",
        "        skf = MultilabelStratifiedKFold(n_splits=N_SPLITS, random_state=kfold_seed, shuffle=True)\n",
        "        tmp = drug_.loc[drug_.drug_id.isin(vc2)].reset_index(drop=True)\n",
        "        for fold, (idxT, idxV) in enumerate(skf.split(tmp, tmp)):\n",
        "            dd = {k: fold for k in tmp.sig_id[idxV].values}\n",
        "            dct2.update(dd)\n",
        "\n",
        "        # ASSIGN FOLDS\n",
        "        drug_[\"fold\"] = drug_.drug_id.map(dct1)\n",
        "        drug_.loc[drug_.fold.isna(), \"fold\"] = drug_.loc[drug_.fold.isna(), \"sig_id\"].map(dct2)\n",
        "        drug_.fold = drug_.fold.astype(\"int8\")\n",
        "\n",
        "        for n, (tr, te) in enumerate(\n",
        "            MultilabelStratifiedKFold(n_splits=N_SPLITS, random_state=kfold_seed, shuffle=True).split(target_, target_)\n",
        "        ):\n",
        "            if cv == \"with_drug_id\":\n",
        "                tr = drug_[drug_[\"fold\"] != n].index\n",
        "                te = drug_[drug_[\"fold\"] == n].index\n",
        "\n",
        "            start_time = time()\n",
        "\n",
        "            # Build Data Sets\n",
        "            if model_name == \"ResNet\":\n",
        "                x_tr = [\n",
        "                    add_swap_noise(tr, train_.values[tr], train_.values[tr]),\n",
        "                    train_pca_.values[tr],\n",
        "                ]\n",
        "                x_val = [\n",
        "                    train_.values[te],\n",
        "                    train_pca_.values[te],\n",
        "                ]\n",
        "                y_tr, y_val = target_.astype(float).values[tr], target_.astype(float).values[te]\n",
        "                x_tt = [test_.values, test_pca_.values]\n",
        "\n",
        "            else:\n",
        "                x_tr, x_val = add_swap_noise(tr, train_.values[tr], train_.values[tr]), train_.values[te]\n",
        "                y_tr, y_val = target_.astype(float).values[tr], target_.astype(float).values[te]\n",
        "                x_tt = test_.values\n",
        "\n",
        "            # Build Model\n",
        "            if model_name == \"ResNet\":\n",
        "                if \"hyperparameter_tuning\" in train_flags:\n",
        "                    model = create_model_resnet_tuning(\n",
        "                        len(train_.columns), len(train_pca_.columns), len(target_.columns), params[model_name]\n",
        "                    )\n",
        "                    \n",
        "                    if \"fine_tuning\" in train_flags:\n",
        "                        model_base = create_model_resnet_tuning(\n",
        "                            len(train_.columns),\n",
        "                            len(train_pca_.columns),\n",
        "                            len(transfer_learning_base.columns),\n",
        "                            params[model_name],\n",
        "                        )\n",
        "                    \n",
        "                else:\n",
        "                    model = create_model_resnet(len(train_.columns), len(train_pca_.columns), len(target_.columns))\n",
        "\n",
        "                    if \"fine_tuning\" in train_flags:\n",
        "                        model_base = create_model_resnet(\n",
        "                            len(train_.columns), len(train_pca_.columns), len(transfer_learning_base.columns)\n",
        "                        )\n",
        "\n",
        "            elif model_name == \"SimpleNN\":\n",
        "                if \"hyperparameter_tuning\" in train_flags:\n",
        "                    # TODO\n",
        "                    pass\n",
        "\n",
        "                else:\n",
        "                    model = create_simple_nn(len(train_.columns), len(target_.columns))\n",
        "\n",
        "                    if \"fine_tuning\" in train_flags:\n",
        "                        model_base = create_simple_nn(len(train_.columns), len(transfer_learning_base.columns))\n",
        "\n",
        "            elif model_name == \"TabNet\":\n",
        "                if \"hyperparameter_tuning\" in train_flags:\n",
        "                    model = create_model_tabnet_tuning(kfold_seed, params[model_name])\n",
        "                    \n",
        "                elif \"pre_train\" in train_flags:\n",
        "                    #model = create_model_tabnet(kfold_seed, pre_train=True)\n",
        "                    model = create_model_tabnet(kfold_seed, pre_train=False)\n",
        "\n",
        "                else:\n",
        "                    model = create_model_tabnet(kfold_seed)\n",
        "                    \n",
        "                    if any(flag in train_flags for flag in [\"fine_tuning\", \"self_supervise\"]):\n",
        "                        model_base = create_model_tabnet(kfold_seed, pre_train=True)\n",
        "                        \n",
        "            else:\n",
        "                raise \"Model name is invalid.\"\n",
        "\n",
        "            if model_name == \"TabNet\":\n",
        "                checkpoint_path = f\"{model_name}_repeat:{seed // len(models)}_fold:{n}\"\n",
        "                \n",
        "                if PRE_TRAIN_MODEL == \"load-others\":\n",
        "                    checkpoint_path = os.path.join(\n",
        "                        PRE_TRAIN_MODEL_DIR_TABNET,\n",
        "                        checkpoint_path\n",
        "                    )\n",
        "                    \n",
        "                if \"fine_tuning\" in train_flags and os.path.exists(checkpoint_path):\n",
        "                    model_base.load_model(checkpoint_path + \".zip\")\n",
        "\n",
        "                    model.fit(\n",
        "                        X_train=x_tr,\n",
        "                        y_train=y_tr,\n",
        "                        eval_set=[(x_val, y_val)],\n",
        "                        eval_name=[\"val\"],\n",
        "                        eval_metric=[\"logits_ll\"],\n",
        "                        max_epochs=200,\n",
        "                        patience=10,\n",
        "                        batch_size=1024,\n",
        "                        virtual_batch_size=32,\n",
        "                        num_workers=1,\n",
        "                        drop_last=True,\n",
        "                        # loss_fn=F.binary_cross_entropy_with_logits,\n",
        "                        loss_fn=SmoothBCEwLogits(smoothing=1e-6),\n",
        "                        from_unsupervised=model_base,\n",
        "                    )\n",
        "                    \n",
        "                #elif \"pre_train\" in train_flags:\n",
        "                #    model.fit(\n",
        "                #        X_train=test_.values,\n",
        "                #        eval_set=[train_.values],\n",
        "                #        # eval_name=[\"val\"],\n",
        "                #        max_epochs=200,\n",
        "                #        patience=10,\n",
        "                #        batch_size=1024,\n",
        "                #        virtual_batch_size=32,\n",
        "                #        num_workers=1,\n",
        "                #        drop_last=True,\n",
        "                #        # loss_fn=F.binary_cross_entropy_with_logits,\n",
        "                #        # loss_fn=SmoothBCEwLogits(smoothing=1e-6),\n",
        "                #        pretraining_ratio=0.5,\n",
        "                #    )\n",
        "                    \n",
        "                else:\n",
        "                    model.fit(\n",
        "                        X_train=x_tr,\n",
        "                        y_train=y_tr,\n",
        "                        eval_set=[(x_val, y_val)],\n",
        "                        eval_name=[\"val\"],\n",
        "                        eval_metric=[\"logits_ll\"],\n",
        "                        max_epochs=200,\n",
        "                        patience=10,\n",
        "                        batch_size=1024,\n",
        "                        virtual_batch_size=32,\n",
        "                        num_workers=1,\n",
        "                        drop_last=True,\n",
        "                        # loss_fn=F.binary_cross_entropy_with_logits,\n",
        "                        loss_fn=SmoothBCEwLogits(smoothing=1e-6),\n",
        "                    )\n",
        "                    \n",
        "                if \"pre_train\" in train_flags:\n",
        "                    try:\n",
        "                        os.remove(checkpoint_path)\n",
        "                    except OSError:\n",
        "                        pass\n",
        "                    model.save_model(checkpoint_path)\n",
        "                    continue\n",
        "\n",
        "            else:\n",
        "                model.compile(\n",
        "                    optimizer=tfa.optimizers.AdamW(lr=1e-3, weight_decay=1e-5),\n",
        "                    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=1e-6),\n",
        "                    metrics=logloss,\n",
        "                )\n",
        "\n",
        "                checkpoint_path = f\"{model_name}_repeat:{seed // len(models)}_fold:{n}.hdf5\"\n",
        "\n",
        "                if PRE_TRAIN_MODEL == \"load-others\":\n",
        "                    if model_name == \"ResNet\":\n",
        "                        checkpoint_path = os.path.join(\n",
        "                            PRE_TRAIN_MODEL_DIR_RESNET,\n",
        "                            checkpoint_path\n",
        "                        )\n",
        "                    elif model_name == \"SimpleNN\":\n",
        "                        checkpoint_path = os.path.join(\n",
        "                            PRE_TRAIN_MODEL_DIR_SIMPLE_NN,\n",
        "                            checkpoint_path\n",
        "                        )\n",
        "\n",
        "                if \"fine_tuning\" in train_flags and os.path.exists(checkpoint_path):\n",
        "                    model_base.load_weights(checkpoint_path)\n",
        "                    for layer in range(len(model_base.layers[:-1])):\n",
        "                        model.layers[layer].set_weights(model_base.layers[layer].get_weights())\n",
        "\n",
        "                if \"pre_train\" in train_flags:\n",
        "                    cb_checkpt = ModelCheckpoint(\n",
        "                        checkpoint_path,\n",
        "                        monitor=\"val_loss\",\n",
        "                        verbose=0,\n",
        "                        save_best_only=True,\n",
        "                        save_weights_only=True,\n",
        "                        mode=\"min\",\n",
        "                    )\n",
        "                reduce_lr_loss = ReduceLROnPlateau(\n",
        "                    monitor=\"val_loss\", factor=0.1, patience=5, verbose=0, min_delta=1e-5, min_lr=1e-5, mode=\"min\"\n",
        "                )\n",
        "                early_stopping = EarlyStopping(\n",
        "                    monitor=\"val_loss\",\n",
        "                    patience=10,\n",
        "                    mode=\"min\",\n",
        "                    verbose=0,\n",
        "                    min_delta=1e-5,\n",
        "                    restore_best_weights=True,\n",
        "                )\n",
        "                if \"pre_train\" in train_flags:\n",
        "                    callbacks = [cb_checkpt, reduce_lr_loss, early_stopping]\n",
        "                else:\n",
        "                    callbacks = [reduce_lr_loss, early_stopping]\n",
        "                model.fit(\n",
        "                    x_tr,\n",
        "                    y_tr,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    epochs=200,\n",
        "                    batch_size=128,\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=0,\n",
        "                )\n",
        "\n",
        "            val_predict = model.predict(x_val)\n",
        "            val_predict = 1 / (1 + np.exp(-val_predict))\n",
        "            seed_result.loc[te, :] += val_predict\n",
        "\n",
        "            if any(flag in train_flags for flag in [\"normal\", \"fine_tuning\"]):\n",
        "                test_predict = model.predict(x_tt)\n",
        "                test_predict = 1 / (1 + np.exp(-test_predict))\n",
        "                prediction += test_predict / N_SPLITS\n",
        "\n",
        "            if model_name == \"TabNet\":\n",
        "                fold_score = np.min(model.history[\"val_logits_ll\"])\n",
        "            else:\n",
        "                fold_score = metric(target_.loc[te].values, val_predict)\n",
        "\n",
        "            print(\n",
        "                f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] {model_name} {cv}: Seed {seed}, Fold {n}:\",\n",
        "                fold_score,\n",
        "            )\n",
        "\n",
        "            K.clear_session()\n",
        "            del model\n",
        "            if \"model_base\" in globals():\n",
        "                del model_base\n",
        "            gc.collect()\n",
        "\n",
        "        oof[f\"{model_name}_{cv}_{seed}\"] = seed_result\n",
        "        predictions[f\"{model_name}_{cv}_{seed}\"] = prediction\n",
        "\n",
        "    return oof, predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa-XZKZr0OcL"
      },
      "source": [
        "## Hyper parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDdNTfxi0Q2Y",
        "trusted": true
      },
      "source": [
        "def objective(trial):\n",
        "    tuning_resnet = False\n",
        "    tuning_tabnet = True\n",
        "\n",
        "    params = {model[\"model_name\"]: None for model in models}\n",
        "\n",
        "    if TUNING_RESNET:\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 2, 5)\n",
        "        # n_layers = 5\n",
        "\n",
        "        units = []\n",
        "        for i in range(n_layers + 3):\n",
        "            u = trial.suggest_categorical(f\"units_{i}\", [128, 256, 512, 1024])\n",
        "            units.append(u)\n",
        "\n",
        "        activations = []\n",
        "        for i in range(n_layers + 4):\n",
        "            a = trial.suggest_categorical(f\"activations_{i}\", [\"relu\", \"elu\", \"selu\", \"swish\"])\n",
        "            activations.append(a)\n",
        "\n",
        "        params[\"ResNet\"] = {\n",
        "            \"n_layers\": n_layers,\n",
        "            \"units\": units,\n",
        "            \"activations\": activations,\n",
        "        }\n",
        "\n",
        "    if TUNING_TABNET:\n",
        "        n_d = trial.suggest_categorical(\"n_d\", [16, 24, 32, 48, 64, 96, 128, 160])\n",
        "        n_a = trial.suggest_categorical(\"n_a\", [16, 24, 32, 48, 64, 96, 128, 160])\n",
        "\n",
        "        params[\"TabNet\"] = {\n",
        "            \"n_d\": n_d,\n",
        "            \"n_a\": n_a,\n",
        "        }\n",
        "\n",
        "    # Training\n",
        "    if PRE_TRAIN_MODEL == \"in-notebook\":\n",
        "        _, _ = learning(\n",
        "            train[: non_target.shape[0]],\n",
        "            train_pca[: non_target.shape[0]],\n",
        "            non_target,\n",
        "            non_target_drug,\n",
        "            test,\n",
        "            test_pca,\n",
        "            N_STARTS,\n",
        "            N_SPLITS,\n",
        "            train_flags=[\"pre_train\", \"hyperparameter_tuning\"],\n",
        "            params=params,\n",
        "        )\n",
        "\n",
        "    oof, predictions = learning(\n",
        "        train,\n",
        "        train_pca,\n",
        "        target,\n",
        "        target_drug,\n",
        "        test,\n",
        "        test_pca,\n",
        "        N_STARTS,\n",
        "        N_SPLITS,\n",
        "        train_flags=[\"normal\", \"hyperparameter_tuning\"],  # \"normal\", \"fine_tuning\", \"self_supervise\"\n",
        "        transfer_learning_base=non_target,\n",
        "        params=params,\n",
        "    )\n",
        "\n",
        "    initial_weights = [1.0 / N_STARTS for _ in range(N_STARTS)]\n",
        "    y_true = target.values[: non_target.shape[0]]\n",
        "\n",
        "    cv, auc, _ = cross_validation(y_true.shape, initial_weights, y_true, oof)\n",
        "\n",
        "    return cv * 1000 / auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzqitb2E07g9",
        "trusted": true
      },
      "source": [
        "if HYPER_PARAMETER_TUNING:\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=20, gc_after_trial=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opMxHb9N6o7Y",
        "trusted": true
      },
      "source": [
        "if HYPER_PARAMETER_TUNING:\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  CV:  {}\".format(trial.value))\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "    print(optuna.importance.get_param_importances(study))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsSV0mNk6xHe",
        "trusted": true
      },
      "source": [
        "if HYPER_PARAMETER_TUNING:\n",
        "    raise \"Finished parameter tuning.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMBw29HH0TGT"
      },
      "source": [
        "## Normal training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 1526.52396,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:03:33.929627",
          "end_time": "2020-11-02T00:29:00.453587"
        },
        "tags": [],
        "id": "QEPrZIyP7OUV",
        "trusted": true
      },
      "source": [
        "%%time\n",
        "\n",
        "if PRE_TRAIN_MODEL == \"in-notebook\":\n",
        "    _, _ = learning(\n",
        "        train[: non_target.shape[0]],\n",
        "        train_pca[: non_target.shape[0]],\n",
        "        pd.concat([non_target, target], axis=1),\n",
        "        non_target_drug,\n",
        "        test,\n",
        "        test_pca,\n",
        "        N_STARTS,\n",
        "        N_SPLITS,\n",
        "        train_flags=[\"pre_train\"],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 3311.470931,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T00:29:00.508861",
          "end_time": "2020-11-02T01:24:11.979792"
        },
        "tags": [],
        "id": "PA4xk_Tn7OUX",
        "trusted": true
      },
      "source": [
        "%%time\n",
        "\n",
        "oof, predictions = learning(\n",
        "    train,\n",
        "    train_pca,\n",
        "    target,\n",
        "    target_drug,\n",
        "    test,\n",
        "    test_pca,\n",
        "    N_STARTS,\n",
        "    N_SPLITS,\n",
        "    train_flags=[\"fine_tuning\"],  # \"normal\", \"fine_tuning\", \"self_supervise\"\n",
        "    transfer_learning_base=pd.concat([non_target, target], axis=1),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.073609,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:12.054101",
          "end_time": "2020-11-02T01:24:12.12771"
        },
        "id": "iy2jGG1j7OUY",
        "tags": []
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.553717,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:12.20192",
          "end_time": "2020-11-02T01:24:12.755637"
        },
        "tags": [],
        "id": "lx66CzK_7OUa",
        "trusted": true
      },
      "source": [
        "oof_weights = [1.0 / N_STARTS for _ in range(N_STARTS)]\n",
        "model_weights = [1.0 / len(models) for _ in range(len(models))]\n",
        "seed_weights = [1.0 / N_SEED for _ in range(N_SEED)]\n",
        "\n",
        "y_true = target.values[: non_target.shape[0]]\n",
        "\n",
        "print(f\"===== OOF - CV =====\")\n",
        "for key, val in oof.items():\n",
        "    print(f\"OOF Key: {key}, CV: {metric(y_true, val.values[:y_true.shape[0]])}\")\n",
        "\n",
        "oof_by_model = {\n",
        "    f\"{model['model_name']}_{model['cv']}\": {\n",
        "        k: v for k, v in oof.items() if k.startswith(f\"{model['model_name']}_{model['cv']}\")\n",
        "    }\n",
        "    for model in models\n",
        "}\n",
        "\n",
        "blend_by_model = {}\n",
        "for model, oof_ in oof_by_model.items():\n",
        "    print(f\"\\n===== Model {model} - CV =====\")\n",
        "    _, _, blend_by_model[model] = cross_validation(y_true.shape, seed_weights, y_true, oof_)\n",
        "\n",
        "print(f\"\\n===== Overall - CV =====\")\n",
        "_ = cross_validation(y_true.shape, model_weights, y_true, blend_by_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWP8oBRmrKn7",
        "trusted": true
      },
      "source": [
        "optimize = \"lagrange\"\n",
        "\n",
        "if IN_COLAB:\n",
        "    optimize = \"lagrange\"  # \"lagrange\", \"fixed\", \"default\"\n",
        "\n",
        "if optimize == \"fixed\":\n",
        "    model_weights = [0.54, 0.46]\n",
        "    print(f\"Fixed weights: {model_weights}\")\n",
        "\n",
        "    cross_validation(y_true.shape, model_weights, y_true, blend_by_model)\n",
        "\n",
        "elif optimize == \"lagrange\":\n",
        "    # https://www.kaggle.com/gogo827jz/optimise-blending-weights-with-bonus-0#Bonus-(Lagrange-Multiplier)\n",
        "\n",
        "    def lagrange_func(params):\n",
        "        # weights, _lambda = params\n",
        "        blend_ = blend(y_true.shape, params[:-1], blend_by_model)\n",
        "        return metric(y_true, blend_) - params[-1] * (sum(params[:-1]) - 1)\n",
        "\n",
        "    grad_l = grad(lagrange_func)\n",
        "\n",
        "    def lagrange_obj(params):\n",
        "        # weights, _lambda = params\n",
        "        d = grad_l(params).tolist()\n",
        "        return d[:-1] + [sum(params[:-1]) - 1]\n",
        "\n",
        "    optimized_weights = fsolve(lagrange_obj, model_weights + [1.0])\n",
        "    cross_validation(y_true.shape, optimized_weights[:-1], y_true, blend_by_model)\n",
        "\n",
        "    print(f\"Optimized weights: {optimized_weights[:-1]}\")\n",
        "    print(f\"Check the sum of all weights: {sum(optimized_weights[:-1])}\")\n",
        "\n",
        "    model_weights = optimized_weights[:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkapdEQ8pDfg",
        "trusted": true
      },
      "source": [
        "predictions_by_model = {\n",
        "    f\"{model['model_name']}_{model['cv']}\": {\n",
        "        k: v for k, v in predictions.items() if k.startswith(f\"{model['model_name']}_{model['cv']}\")\n",
        "    }\n",
        "    for model in models\n",
        "}\n",
        "\n",
        "blend_by_model = {\n",
        "    f\"{model['model_name']}_{model['cv']}\": pd.DataFrame(\n",
        "        blend(ss.shape, seed_weights, predictions_by_model[f\"{model['model_name']}_{model['cv']}\"])\n",
        "    )\n",
        "    for model in models\n",
        "}\n",
        "\n",
        "for a, b in itertools.combinations(blend_by_model.keys(), 2):\n",
        "    corr = blend_by_model[a].corrwith(blend_by_model[b], axis=1)\n",
        "    print(f\"Prediction correlation between {a} and {b}: {corr.mean()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.075326,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:13.91903",
          "end_time": "2020-11-02T01:24:13.994356"
        },
        "id": "zghrQJ2L7OUl",
        "tags": []
      },
      "source": [
        "# Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.516377,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:14.069203",
          "end_time": "2020-11-02T01:24:14.58558"
        },
        "id": "6R71_Ar-7OUl",
        "trusted": true,
        "tags": []
      },
      "source": [
        "# Weighted blend\n",
        "submit_df.loc[:, target.columns] = blend(ss.shape, model_weights, blend_by_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.124214,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:14.982501",
          "end_time": "2020-11-02T01:24:15.106715"
        },
        "id": "JHpr_90W7OUp",
        "trusted": true,
        "tags": []
      },
      "source": [
        "submit_df.loc[test_df[\"cp_type\"] == \"ctl_vehicle\", target.columns] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.075513,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:15.182599",
          "end_time": "2020-11-02T01:24:15.258112"
        },
        "id": "7mHXnoKn7OUr",
        "tags": []
      },
      "source": [
        "# Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 2.21436,
          "status": "completed",
          "exception": false,
          "start_time": "2020-11-02T01:24:15.333588",
          "end_time": "2020-11-02T01:24:17.547948"
        },
        "id": "7pdd1k1E7OUr",
        "trusted": true,
        "tags": []
      },
      "source": [
        "submit_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}